%\def\Xactiveplot#1#2#3#4{}
%\def\Xactivesideplot#1#2#3#4{}

\title{The helical coordinate}
\author{Jon Claerbout}
\maketitle
\label{paper:hlx}

For many years it has been true that
our most powerful signal-analysis techniques
are in {\em  one}-dimensional space,
while our most important applications are in {\em  multi}-dimensional space.
The helical coordinate system makes a giant step
towards overcoming this difficulty.

\par
Many geophysical map estimation applications appear to be multidimensional,
but actually they are not.
To see the tip of the iceberg, consider this example:
On a two-dimensional cartesian mesh, the function
$
\begin{array}{|r|r|r|r|}
        \hline
        0 & 0 & 0 &0 \\
        \hline
        0 & 1 & 1 &0 \\
        \hline
        0 & 1 & 1 &0 \\
        \hline
        0 & 0 & 0 &0 \\
        \hline
\end{array}
$
\par\noindent
has the autocorrelation
$
\begin{array}{|r|r|r|} \hline
        1 & 2 & 1\\
        \hline
        2 & 4 & 2\\
        \hline
        1 & 2 & 1
        \\ \hline
\end{array}
$.

\par\noindent
Likewise, on a one-dimensional cartesian mesh,
\par\noindent
the function $
%\mathcal{B} =
\begin{array}{|r|r|r|r|r|r|r|r|r|r|r|r|r|} \hline
 1&1&0&0& \cdots& 0& 1&1
        \\ \hline
\end{array}
$
\par\noindent
has the autocorrelation
$ %\mathcal{R} =
\begin{array}{|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|} \hline
 1&2&1&0&\cdots&0&2&4&2&0&\cdots&1&2&1
        \\ \hline
\end{array}
$.


%\par\noindent
%the function $ \bold b = ( 1,1,0,0, \cdots, 0, 1,1) $
%\par\noindent
%has the autocorrelation
%$ \bold r = ( 1,2,1,0,\cdots,0,2,4,2,0,\cdots,1,2,1)$.

\par\noindent
Observe the numbers in the one-dimensional world are identical
with the numbers in the two-dimensional world.
This correspondence is no accident.


\section{FILTERING ON A HELIX}
\inputdir{helicon}
Figure \ref{fig:diamond} shows some two-dimensional shapes
that are convolved together.
The left panel shows an impulse response function,
the center shows some impulses,
and the right shows the superposition of responses.
\plot{diamond}{width=6in,height=2.0in}{
  Two-dimensional convolution
  as performed in one dimension
  by module
  \texttt{helicon} %\vpageref{lst:helicon}
}

\par
A surprising, indeed amazing, fact is that
Figure \ref{fig:diamond} was not computed with a two-dimensional
convolution program.
It was computed with a one-dimensional computer program.
It could have been done with anybody's one-dimensional convolution program,
either in the time domain or in the fourier domain.
This magical trick is done with the helical coordinate system.

\par
A basic idea of filtering, be it in one dimension, two dimensions, or more,
is that you have some filter coefficients and some sampled data;
you pass the filter over the data; 
at each location you find an output by crossmultiplying
the filter coefficients times the underlying data and summing the terms.

\par
The helical coordinate system is much simpler than you might imagine.
Ordinarily, a plane of data is thought of as a collection of columns,
side by side.
Instead, imagine the columns stored end-to-end,
and then coiled around a cylinder.
This is the helix.
Fortran programmers will recognize that fortran's way
of storing 2-D arrays in one-dimensional memory
is exactly what we need for this helical mapping.
Seismologists sometimes use the word ``supertrace''
to describe a collection of seismograms stored ``end-to-end''.

\inputdir{.}

Figure \ref{fig:sergey-helix} shows a helical mesh for 2-D data on a cylinder.
Darkened squares depict a 2-D filter
shaped like the Laplacian operator $\partial_{xx}+\partial_{yy}$.
The input data, the filter, and the output data are all on helical meshes
all of which could be unrolled into linear strips.
A compact 2-D filter like a Laplacian,
on a helix is a sparse 1-D filter with long empty gaps.

%\activeplot{sergey-helix}{width=6.0in,bb=210 315 630 545}{} {
%\activeplot{sergey-helix}{width=6.0in,bb=186 134 657 380}{} {
\plot{sergey-helix}{width=6.0in,bb=210 155 630 390}{
  Filtering on a helix.
  The same filter coefficients overlay the same data values
  if the 2-D coils are unwound into 1-D strips.
  (\emph{Mathematica} drawing by Sergey Fomel)
}

\par
Since the values output from filtering can be computed
in any order, we can slide the filter coil
over the data coil in any direction.
The order that you produce the outputs is irrelevant.
You could compute the results in parallel.
We could, however, slide the filter over the data in the screwing order
that a nut passes over a bolt.
The screw order is the same order that would be used
if we were to unwind the coils into one-dimensional strips
and convolve them across one another.
The same filter coefficients overlay the same data values
if the 2-D coils are unwound into 1-D strips.
The helix idea allows us to obtain the same convolution output
in either of two ways, a one-dimensional way, or a two-dimensional way.
I used the one-dimensional way to compute the obviously two-dimensional
result in Figure \ref{fig:diamond}.



\subsection{Review of 1-D recursive filters}
Convolution is the operation we do on polynomial coefficients
when we multiply polynomials.
Deconvolution is likewise for polynomial division.
Often these ideas are described
as polynomials in the variable $Z$.
Take $X(Z)$ to denote the polynomial
whose coefficients are samples of input data,
and let $A(Z)$ likewise denote the filter.
The convention I adopt here is that the first coefficient
of the filter has the value +1, so the filter's polynomial
is $A(Z) = 1 + a_1Z + a_2Z^2 + \cdots$.
To see how to convolve, we now identify the coefficient
of $Z^k$ in the product $Y(Z)=A(Z)X(Z)$.
The usual case ($k$ larger than the number $N_a$ of filter coefficients) is
\begin{equation}
y_k \quad=\quad x_k + \sum_{i=1}^{N_a} a_i x_{k-i}
\label{eqn:convolution}
\end{equation}
Convolution computes $y_k$ from $x_k$ whereas deconvolution
(also called back substitution) does the reverse.
Rearranging (\ref{eqn:convolution}) we get
\begin{equation}
x_k \quad=\quad y_k - \sum_{i=1}^{N_a} a_i x_{k-i}
\label{eqn:deconvolution}
\end{equation}
where now we are finding the output $x_k$ from
its past outputs $x_{k-i}$ and from the present input $y_k$.
We see that the deconvolution process is essentially
the same as the convolution process,
except that the filter coefficients
are used with opposite polarity;
and they are applied to the past {\em  outputs}
instead of the past {\em  inputs}.
That is why deconvolution must be done sequentially
while convolution can be done in parallel.



\subsection{Multidimensional deconvolution breakthrough}
Deconvolution (polynomial division)
can undo convolution (polynomial multiplication).
A magical property of the helix is that we can consider
1-D convolution to be the same as 2-D convolution.
Hence is a second magical property:
We can use 1-D
 {\em  de}convolution to undo convolution,
whether that convolution was 1-D or 2-D.
Thus, we have discovered how to undo 2-D convolution.
We have discovered that 2-D deconvolution on a helix
is equivalent to        1-D deconvolution.
The helix enables us to do multidimensional deconvolution.
\par
Deconvolution is recursive filtering.
Recursive filter outputs cannot be computed in parallel,
but must be computed sequentially
as in one dimension, namely,
in the order that the nut
screws on the bolt.  
\par
Recursive filtering sometimes solves big problems with astonishing speed.
It can propagate energy rapidly for long distances.
Unfortunately, recursive filtering can also be unstable.
The most interesting case, near resonance, is also near instability.
There is a large literature and extensive technology
about recursive filtering in one dimension.
The helix allows us to apply that technology to two (and more) dimensions.
It is a huge technological breakthrough.
\par
In 3-D we simply append one plane after
another (like a 3-D fortran array).
It is easier to code than to explain or visualize
a spool or torus wrapped with string, etc.



\subsection{Examples of simple 2-D recursive filters}
Let us associate $x$- and $y$-derivatives with
a finite-difference stencil or template.
(For simplicity take $\Delta x=\Delta y=1$.)
\begin{equation}
 \frac{\partial }{ \partial x } \eq
 \begin{array}{|c|c|} \hline
   1    & -1
   \\ \hline
 \end{array}
 \label{eqn:partialx}
\end{equation}
\begin{equation}
 \frac{\partial }{ \partial y } \eq
  \begin{array}{|r|} \hline
    1  \\
    \hline
    -1
    \\ \hline
  \end{array}
  \label{eqn:partialy}
\end{equation}
Convolving a data plane with
the stencil (\ref{eqn:partialx})
forms the $x$-derivative of the plane.
Convolving a data plane with
the stencil (\ref{eqn:partialy})
forms the $y$-derivative of the plane.
On the other hand,
{\it deconvolving}
with (\ref{eqn:partialx}) integrates data along the $x$-axis for each $y$.
Likewise, deconvolving
with (\ref{eqn:partialy}) integrates data along the $y$-axis for each $x$.
Next we look at a fully two-dimensional operator
(like the cross derivative $\partial_{xy}$).

\inputdir{helicon}

\par
A nontrivial two-dimensional convolution stencil is
\par
\begin{equation}
        \begin{array}{|r|r|}   \hline
                0    & -1/4 \\
                \hline
                1    & -1/4 \\
                \hline
                -1/4 & -1/4
                \\ \hline
        \end{array}
\label{eqn:filterfour}
\end{equation}
We will convolve and deconvolve a data plane with this operator.
Although everything is shown on a plane,
the actual computations are done in one dimension
with equations
(\ref{eqn:convolution}) and
(\ref{eqn:deconvolution}).
Let us manufacture the simple data plane
shown on the left in Figure \ref{fig:wrap-four}.
Beginning with a zero-valued plane, we add
in a copy of the filter (\ref{eqn:filterfour})
near the top of the frame.
Nearby add another copy with opposite polarity.
Finally add some impulses near the bottom boundary.
The second frame in Figure \ref{fig:wrap-four} is the result
of deconvolution by the filter (\ref{eqn:filterfour})
using the one-dimensional equation (\ref{eqn:deconvolution}).
Notice that deconvolution
turns the filter itself into an impulse,
while it turns the impulses
into comet-like images.
The use of a helix is evident
by the comet images wrapping around the vertical axis.


%\activeplot{wrap}{width=6in,height=3in}{} { 
%\activeplot{wrap}{width=3in,height=1.5in}{} { 
\plot{wrap-four}{width=4in,height=2in} { 
  Illustration of 2-D deconvolution.
  Left is the input.
  Right is after deconvolution with
  the filter (\protect\ref{eqn:filterfour})
  as preformed by
  by module
  \texttt{polydiv} %\vpageref{lst:polydiv}
}

The filtering in Figure \ref{fig:wrap-four}
ran along a helix from left to right.
Figure \ref{fig:back-four}
shows a second filtering running from right to left.
Filtering in the reverse direction is the adjoint.
After deconvolving both ways, we have accomplished a symmetrical smoothing.
The final frame undoes the smoothing to bring us exactly back
to where we started.
The smoothing was done with two passes of {\it deconvolution}
and it is undone by two passes of {\it convolution}.
No errors, no evidence remains of any of the boundaries
where we have wrapped and truncated.

%\activeplot{pdadj}{width=6in,height=6.0in}{} { 
\plot{back-four}{width=4in,height=2.0in}{ 
  Recursive filtering backwards (leftward on the space axis)
  is done by the {\em  adjoint} of 2-D deconvolution.
  Here we see that 2-D {\it deconvolution} compounded with its adjoint
  is exactly inverted by 2-D {\it convolution} and its adjoint.
}

%\begin{notforlecture}
\par
Chapter \ref{paper:prc} explains the important practical role
to be played by a multidimensional operator for which
we know the exact inverse.  
Other than multidimensional Fourier transformation,
transforms based on polynomial multiplication and division
on a helix are the only known easily invertible linear operators.

%\par
%I found myself with a powerful convolution/deconvolution
%program, but I did not have a bag full of 2-D filters.
%Further, I knew that a great number of filters are unstable
%for deconvolution.
%I recalled a variant of Rouch$\acute{\rm e}$'s theorem
%that if the sum of the absolute values of the filter coefficients
%after the onset pulse is less than the pulse,
%then the filter is ``positive real,''
%hence ``minimum phase,'' hence stable in polynomial division.
%Ironically, it is the ``almost unstable'' filters
%that hold the greatest interest,
%because they are the ones that move energy long distances.
%To make long impulse-responses of deconvolution,
%I chose all the adjustable filter coefficients to be negative
%and to sum to $-1$,
%as does (\ref{eqn:filterfour}).
%For damping, I took the absolute value of the sum to be
%a little less than $1$.

\par
In seismology we often have occasion to steer summation along beams.
Such an impulse response is shown in Figure \ref{fig:dip}.
\plot{wrap-waves}{width=3in,height=1.5in}{ 
  A simple low-order 2-D filter whose inverse
  contains plane waves of two different dips.
  One of them is spatially aliased.
}

Of special interest are filters that destroy plane waves.  The inverse
of such a filter creates plane waves.  Such filters are like wave
equations.  A filter that creates two plane waves is illustrated in
figure \ref{fig:wrap-waves}.

\plot{dip}{width=6in,height=2.0in}{ 
  A simple low-order 2-D filter whose inverse times its inverse adjoint,
  is approximately a dipping seismic arrival.
}

\subsection{Coding multidimensional de/convolution}
Let us unroll the filter helix seen in Figure \ref{fig:sergey-helix}
and see what we have.
Start from the idea that a 2-D filter is generally made
from a cluster of values near one another in two dimensions
similar to the Laplacian operator in the figure.
We see that in the helical approach,
a 2-D filter is a 1-D filter containing some long intervals of zeros.
The intervals are about the length of a 1-D seismogram.

\par
Our program for 2-D convolution with a 1-D convolution program,
could convolve with the somewhat long 1-D strip,
but it is much more cost effective to ignore the many zeros,
which is what we do.
We do not multiply by the backside zeros, nor do we even store them in memory.
Whereas an ordinary convolution program would do time shifting
by a code line like {\tt iy=ix+lag},
Module
\texttt{helicon} %\vpageref{lst:helicon}
ignores the many zero filter values on backside of the tube
by using the code {\tt iy=ix+lag[ia]}
where a counter {\tt ia} ranges over the nonzero filter coefficients.
Before operator {\tt helicon} is invoked,
we need to prepare two lists,
one list containing nonzero filter coefficients {\tt flt[ia]},
and the other list containing the corresponding lags {\tt lag[ia]}
measured to include multiple wraps around the helix.
For example, the 2-D Laplace operator
can be thought of as the 1-D filter
\begin{equation}
\label{eqn:2dlapfil}
\begin{array}{|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|} \hline
1&0&\cdots&0&1&-4&1&0& \cdots&0&1
\\ \hline
\end{array}
%
\quad
\rightarrow{\rm helical \; boundaries}
\quad
\begin{array}{|r|r|r|}  \hline
& 1 & \\
\hline
1 & -4 & 1\\
\hline
& 1 & \\
\hline
\end{array}
\end{equation}
%
%
%\begin{equation}
%\label{eqn:2dlapfil}
%\left[ \begin{array}{ccc}
%& 1 & \\ 1 & -4 & 1\\ & 1 &
%\end{array} \right]
%\xrightarrow{\rm helical \; boundary \; conditions}
%(1,\,0, \; ... \; 0,\,1,\,-4,\,1,\,0, \; ... \; 0,\,1).
%\end{equation}
%
The first filter coefficient in equation (\ref{eqn:2dlapfil})
is $+1$ as implicit to module {\tt helicon}.
To apply the Laplacian on a $1000\times 1000$ mesh
requires the filter inputs:
\par\noindent
\footnotesize
\begin{verbatim}
                i    lag[i]   flt[i]
               ---   ------   -----
                0      999      1
                1     1000     -4
                2     1001      1
                3     2000      1
\end{verbatim}
\normalsize

\par
Here we choose to use
``declaration of a type'',
a modern computer language feature that is absent from Fortran 77.
Fortran 77 has the built in complex arithmetic type.
In module \texttt{helix}
we define a type \texttt{filter}, actually, a helix filter.
After making this definition, it will be used by many programs.
The helix filter consists of three vectors,
a real valued vector of filter coefficients,
an integer valued vector of filter lags,
and an optional vector
that has logical values ``\texttt{true}''~for
output locations that will not be computed
(either because of boundary conditions or because of missing inputs).
The filter vectors are the size of the nonzero filter coefficients
(excluding the leading 1.) while the logical vector is long
and relates to the data size.
The \texttt{helix} module allocates and frees memory for a helix filter.
By default, the logical vector is not allocated but
is set to \texttt{null}
with the \texttt{nullify} operator and ignored.
\moddex{helix}{definition for helix-type filters}{30}{36}{api/c}
\par
For those of you with no C  experience,
the ``\verb#->#'' appearing in the helix module denotes a pointer.
Fortran 77 has no pointers (or everything is a pointer).
%The C, C++, and Java languages use ``\texttt{.}'' to denote pointers.
%C and C++ also have a second type of pointer denoted by ``\texttt{->}''.
The behavior of pointers is somewhat different in each language.
Never-the-less, the idea is simple.
In module \texttt{helicon} %\vpageref{lst:helicon}
you see the expression
\verb#aa->flt[ia]#.
It refers to the filter named \texttt{aa}.
Any filter defined by the \texttt{helix} module
contains three vectors, one of which is named \texttt{flt}.
The second component of the \texttt{flt} vector
in the \texttt{aa} filter
is referred to as
\verb#aa->flt[1]# which
in the example above refers to the value 4.0
in the center of the laplacian operator.
For data sets like above with 1000 points on the 1-axis,
this value 4.0 occurs after 1000 lags,
thus \verb#aa->lag[1]=1000#.

\par
Our first convolution operator
\texttt{tcai1}
\vpageref{lst:tcai1}
was limited to one dimension and a particular choice of end conditions.
With the helix and C pointers,
the operator
\texttt{helicon} %\vpageref{lst:helicon}
is a {\it multidimensional} filter
with considerable flexibility (because of the \texttt{mis} vector)
to work around boundaries and missing data.
\opdex{helicon}{helical convolution}{35}{54}{api/c}
The code fragment
\verb#aa->lag[ia]#
corresponds to 
\texttt{b-1}
in \texttt{tcai1} \vpageref{lst:tcai1}.


\par
Operator {\tt helicon} did the convolution job for Figure \ref{fig:diamond}.
As with
\texttt{tcai1} \vpageref{lst:tcai1}
the adjoint of filtering is filtering backwards
which means unscrewing the helix.

\par
The companion to convolution is deconvolution.
The module \texttt{polydiv} \vpageref{lst:polydiv}
is essentially the same as
\texttt{polydiv1} \vpageref{lst:polydiv1},
but here it was coded using
our new \texttt{filter} type in
module \texttt{helix} \vpageref{lst:polydiv1}
which will simplify our many future uses of
convolution and deconvolution.
Although convolution allows us to work around missing input values,
deconvolution does not
(any input affects all subsequent outputs),
so \texttt{polydiv} never references \verb#aa->mis[ia]#.
\opdex{polydiv}{helical deconvolution}{39}{70}{api/c}

\begin{exer}
\item
Observe the matrix (\ref{eqn:contran1})
which corresponds to
subroutine
\texttt{tcai1} \vpageref{lst:tcai1}.
What is the matrix corresponding to
\texttt{helicon}? % \vpageref{lst:helicon}?
\end{exer}

\subsection{Causality in two-dimensions}
In one dimension, most filters of interest have a short memory.
Significant filter coefficients are concentrated shortly after $t=0$.
The favorite example in Physics is the
damped harmonic oscillator,
all of which is packed into a two-lag filter
(second order differential equation).
The complete story is rich in mathematics and in concepts,
but to sum up, filters fall into two categories according to the
numerical values of their coefficients.
There are filters for which equations
(\ref{eqn:convolution}) and
(\ref{eqn:deconvolution})
work as desired and expected.
These filters are called ``minimum phase''.
There are also filters for which
(\ref{eqn:deconvolution}) is a disaster numerically,
the feedback process diverging to infinity.

\par
Divergent cases correspond to physical processes
that require boundary conditions.
Equation (\ref{eqn:deconvolution})
only allows for initial conditions.
I oversimplify by trying to collapse an entire book (FGDP)
into a few sentences by saying here that
for any fixed spectrum
there exist many filters.
Of these, only one has
stable polynomial division.
That filter has its energy compacted
as soon as possible after the ``1.0'' at zero lag.

%\end{notforlecture}
\par
Now let us turn to two dimensions.
Filters of interest will correspond to energy
concentrated near the end of a helix.
Let us examine the end of a helix.
At the very end, as in the 1-D case,
is a coefficient with the numerical value 1.0.
Keeping only coefficients within two mesh points
in any direction from the 1.0,
we copy the coefficients from
near the end of the helix to a cartesian mesh like this:
%A causal filter in one dimension has
%a curious shape on the two-dimensional helix.
%This strange shape plays a central role in coming chapters.
%I use the convention that the zero-lag
%response of the 1-D filter has the value ``1''.
%In one dimension,
%the causal filter has zeros before the ``1'' and various values after it.
%Let us consider a compact 2-D filter,
%one whose nonzero filter coefficients lie within
%a short distance in two dimensions (two lags) from the ``1''.
%Consider this compact filter to be compressed up against
%one end of a helix coil.
%We pluck it from the coil and
%display it as a two-dimensional array
\begin{equation}
\label{eqn:2dpef}
\begin{array}{ccccc}
        \begin{array}{ccc}
                h  & c &   0 \\
                p  & d &   0 \\
                q  & e &  \bold 1    \\
                s  & f &   a \\
                u  & g &   b
        \end{array}
        &\quad=\quad&
        \begin{array}{ccc}
                h  & c &  \cdot   \\
                p  & d &  \cdot   \\
                q  & e &  \cdot    \\
                s  & f &   a \\
                u  & g &   b
        \end{array}
         &\quad +\quad&
        \begin{array}{ccc}
                \cdot &\cdot &   0 \\
                \cdot &\cdot &   0 \\
                \cdot &\cdot &  \bold 1    \\
                \cdot &\cdot &  \cdot    \\
                \cdot &\cdot &  \cdot  
        \end{array}
  \\
  \\
  \\
        {\rm 2-D \ filter}
           &=&
        {\rm variable}
          &\quad +\quad&
        {\rm constrained}
\end{array}
\end{equation}
where $a,b,c,...,u$ are adjustable coefficients.

\par
Which side of the little rectangular patch of coefficients
we choose to place the 1.0 is rather arbitrary.
The important matter is that as a matter of principle,
the 1.0 is expected to lie along one side of the little patch.
It is rarely (if ever) found at a corner of the patch.
It is important that beyond the 1.0 (in whatever direction that may be)
the filter coefficients must be zero because in one dimension,
these coefficients lie before zero lag.
Our foundations,
the basic convolution-deconvolution pair
(\ref{eqn:convolution}) and
(\ref{eqn:deconvolution})
are applicable only to filters with all coefficients {\it after} zero lag.


\par
Time-series analysis is rich with concepts that
the helix now allows us to apply to many dimensions.
First is the notion of an impulse function.
Observe that an impulse function on the 2-D surface
of the helical cylinder maps to an impulse function
on the 1-D line of the unwound coil.
An autocorrelation function that is an impulse
corresponds both to a white (constant) spectrum in 1-D and
to a white (constant) spectrum in 2-D.
Next we look at a particularly important autocorrelation function
and see how 2-D is the same as 1-D.



\section{FINITE DIFFERENCES ON A HELIX}
The function
\begin{equation}
\label{eqn:2dlapin1d}
\bold r \eq
-\nabla^2
\eq
\begin{array}{|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|} \hline
-1&0&\cdots&0&-1&4&-1&0& \cdots&0&-1
\\ \hline
\end{array}
\end{equation}
is an autocorrelation function.
It is symmetrical about the ``4'' and its Fourier transform
is positive for all frequencies.
Digging out our old textbooks\footnote{
        PVI or FGDP, for example,
        explain spectral factorization.
        More concisely in PVI, more richly in FGDP.
        }
we discover
how to compute a causal wavelet with this autocorrelation.
I used the ``Kolmogoroff spectral-factorization method''
to find this wavelet $\bold h$:
\begin{equation}
{\bold h}
\eq
\begin{array}{|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
1.791&-.651&-.044&-.024&\cdots&\cdots&-.044&-.087&-.200&-.558 \\
\hline
\end{array}
\label{eqn:lapfacrow}
\end{equation}
According to the Kolmogoroff theory,
if we form the autocorrelation of
$\bold h$,
we will get $-\nabla^2$.
This is not obvious from the numbers themselves
because the computation requires a little work.
\par
Let the time reversed version of
$\bold h$
be denoted
$\bold h$.
This notation is consistent with an idea from
Chapter \ref{paper:ajt} that the adjoint of a filter matrix
is another filter matrix with a reversed filter.
In engineering it is conventional to use the asterisk symbol
``$\ast$'' to denote convolution.
Thus, the idea that the autocorrelation of a 
signal $\bold h$
is a convolution of the
signal $\bold h$
with its time reverse (adjoint)
can be written as
$ {\bold h}\T \ast {\bold h} = {\bold h} \ast {\bold h}\T = {\bold r}$.



\par
Wind the signal $\bold r$ around a
vertical-axis helix to see its two-dimensional shape $\bold R$:
\begin{equation}
\bold r
\quad
\rightarrow{\rm helical \; boundaries}
\quad
\begin{array}{|r|r|r|}  \hline
& -1 & \\
\hline
-1 & 4 & -1\\
\hline
& -1 & \\
\hline
\end{array}
\eq
\bold R
\label{eqn:lap2d}
\end{equation}


This 2-D filter is the negative of the finite-difference representation
of the Laplacian operator, generally denoted
$\nabla^2 = \frac{\partial^2}{ \partial x^2} + \frac{\partial^2}{ \partial y^2} $.
Now for the magic:
Wind the signal $\bold h$ around the same helix
to see its two-dimensional shape $\bold H$
\begin{equation}
\label{eqn:lapfac}
 {\bold H}
\eq
    \begin{array} {|r|r|r|r|r|r|r|r|r|} \hline
             &      &       &       & 1.791 &  -.651 & -.044  & -.024& \cdots \\
	     \hline 
      \cdots &-.044 & -.087 & -.200 & -.558 &        &        &      &
     \\ \hline
    \end{array}
\end{equation}
In the representation (\ref{eqn:lapfac}) we see the coefficients diminishing
rapidly away from maximum value 1.791.
My claim is that the two-dimensional autocorrelation of (\ref{eqn:lapfac})
is (\ref{eqn:lap2d}).
You verified this idea earlier when the numbers were all ones.
You can check it again in a few moments
if you drop the small values, say 0.2 and smaller.

\inputdir{helocut}

\par
Since the autocorrelation of ${\bold H}$ is
${\bold H}\T \ast {\bold H} = \bold R =-\nabla^2$
is a second derivative,
the operator $\bold H$ must be something like a first derivative.
As a geophysicist, I found it natural to compare
the operator $\frac{\partial}{\partial y}$
with $\bold H$ by applying them to a local topographic map.
The result shown in
Figure \ref{fig:helocut}
is that $\bold H$ enhances drainage patterns whereas
$\frac{\partial}{\partial y}$ enhances mountain ridges.


\plot{helocut}{width=6in,height=8.6in} {
  Topography, helical derivative, slope south.
}

\par
The operator $\bold H$ has
curious similarities and differences
with the familiar gradient and divergence operators.
In two-dimensional physical space,
the gradient maps one field to {\em two} fields
(north slope and east slope).
The factorization of $-\nabla^2$ with the helix
gives us the operator $\bold H$
that maps one field to {\em  one} field.
Being a one-to-one transformation
(unlike gradient and divergence)
the operator $\bold H$ is potentially invertible
by deconvolution (recursive filtering).

\par
I have chosen the name\footnote{
        Any fact this basic should be named in some earlier field
        of mathematics or theoretical physics.
        Admittedly, the concept exists on an infinite cartesian plane
        without a helix, but all my codes in a finite space involve the helix,
        and the helix concept led me to it.
        }
``helix derivative''
or ``helical derivative'' for the operator $\bold H$.
A telephone pole has a narrow shadow behind it.
The helix integral (middle frame of Figure \ref{fig:lapfac})
and the helix derivative (left frame)
show shadows with an angular bandwidth approaching $180^\circ$.

\par
Our construction makes $\bold H$ have the energy spectrum $k_x^2+k_y^2$,
so the magnitude of the Fourier transform is $\sqrt{k_x^2+k_y^2}$.
It is a cone
centered and with value zero at the origin.
By contrast, the components of the ordinary gradient
have amplitude responses $|k_x|$ and $|k_y|$
that are lines of zero across the
$(k_x,k_y)$-plane.

\par
The rotationally invariant cone in the Fourier domain
contrasts sharply with the nonrotationally invariant
function shape in $(x,y)$-space.
The difference must arise from the phase spectrum.
The factorization (\ref{eqn:lapfac})
is nonunique in that causality
associated with the helix mapping
can be defined along either $x$- or $y$-axes;
thus the operator 
(\ref{eqn:lapfac})
can be rotated or reflected.

\inputdir{helicon}

\par
This is where the story all comes together.
One-dimensional theory, either the old
Kolmogoroff spectral factorization,
or the new
Wilson-Burg spectral-factorization method
produces not merely a causal wavelet
with the required autocorrelation.
It produces one that is stable in deconvolution.
Using $\bold H$ in one-dimensional polynomial division,
we can solve many formerly difficult problems very rapidly.
Consider the Laplace equation with sources (Poisson's equation).
Polynomial division and its reverse (adjoint) gives us
$\bold p =(\bold q/\bold H)/\bold H\T$
which means that we have solved
$\nabla^2 \bold p = -\bold q$
by using polynomial division on a helix.
Using the seven coefficients shown,
the cost is fourteen multiplications
(because we need to run both ways) per mesh point.
An example is shown in Figure \ref{fig:lapfac}.

\plot{lapfac}{width=6in,height=2.0in}{
  Deconvolution by a filter whose autocorrelation
  is the two-dimensional Laplacian operator.
  Amounts to solving the Poisson equation.
  Left is $\bold q$;
  Middle is $\bold q/\bold H$;
  Right is $(\bold q/\bold H)/\bold H\T$.
}

\par
Figure \ref{fig:lapfac} contains both the helix derivative and its inverse.
Contrast them to the $x$- or $y$-derivatives (doublets) and their inverses
(axis-parallel lines in the $(x,y)$-plane).
Simple derivatives are highly directional
whereas the helix derivative is only slightly directional
achieving its meagre directionality entirely from its phase spectrum.

\par
In practice we often require an isotropic filter.
Such a filter is a function of $k_r=\sqrt{k_x^2 + k_y^2}$.
It could be represented as a sum of helix derivatives to integer powers.

\subsection{Matrix view of the helix}
Physics on a helix can be viewed thru the eyes
of matrices and numerical analysis.
This is not easy because the matrices are so huge.
Discretize the $(x,y)$-plane to an $N\times M$ array
and pack the array into a vector of $N\times M$ components.
Likewise pack the Laplacian operator $\partial_{xx}+\partial_{yy}$
into a matrix.
For a $4\times 3$ plane, that matrix is shown in equation (\ref{eqn:huge}).


\begin{equation}
\label{eqn:huge}
-\ \nabla^2 \eq
\left[
\begin{array}{rrrr|rrrr|rrrr}
  4 & -1 & \cdot & \cdot 
& -1 & \cdot & \cdot & \cdot 
& \cdot & \cdot & \cdot & \cdot  \\
  -1  &   4  &   -1     &  \cdot 
&  \cdot &-1 & \cdot  & \cdot 
& \cdot  &  \cdot  &  \cdot &  \cdot    \\
\cdot & -1  &   4  & -1  
&  \cdot  &  \cdot &-1  &  \cdot 
&  \cdot  &  \cdot  &  \cdot  &  \cdot   \\
\cdot &  \cdot & -1 &   4  
&  h  &  \cdot  &  \cdot &-1 
&  \cdot  &  \cdot  &  \cdot  &  \cdot  \\
\hline
  -1  &  \cdot  &  \cdot  &  h 
&   4   &  -1   &  \cdot  &  \cdot 
& -1  &  \cdot  &  \cdot &  \cdot 
\\
   \cdot &-1 & \cdot  &  \cdot 
& -1  &   4  &   -1     &  \cdot 
&  \cdot &-1 & \cdot  &  \cdot 
\\
  \cdot  &  \cdot &-1  &  \cdot 
&  \cdot &   -1     &   4   & -1  
& \cdot  &  \cdot &-1  &  \cdot 
\\
  \cdot  &  \cdot  &  \cdot &-1 
& \cdot  &  \cdot  & -1  &   4  
& h  &  \cdot  &  \cdot &-1 
\\
\hline
  \cdot  &  \cdot  &  \cdot  &  \cdot 
& -1  &  \cdot  &  \cdot  &  h 
&   4   &  -1   &  \cdot  &  \cdot 
\\
  \cdot  &  \cdot  &  \cdot  &  \cdot 
&  \cdot &-1 & \cdot  &  \cdot 
& -1  &   4  &   -1     &  \cdot 
\\
  \cdot  &  \cdot  &  \cdot  &  \cdot 
& \cdot  &  \cdot &-1  &  \cdot 
&  \cdot &   -1     &   4   & -1  
\\
  \cdot  &  \cdot  &  \cdot  &  \cdot 
& \cdot  &  \cdot  &  \cdot &-1 
& \cdot  &  \cdot  & -1  &   4  
\end{array}
\right]
\end{equation}












\par\noindent
The two-dimensional matrix of coefficients for the Laplacian operator
is shown in (\ref{eqn:huge}),
where, 
on a cartesian space, $h=0$,
and in the helix geometry, $h=-1$.
(A similar partitioned matrix arises from packing
a cylindrical surface into a $4\times3$ array.)
Notice that the partitioning becomes transparent for the helix, $h=-1$.
With the partitioning thus invisible, the matrix
simply represents one-dimensional convolution
and we have an alternative analytical approach,
one-dimensional Fourier Transform.
We often need to solve sets of simultaneous equations
with a matrix similar to (\ref{eqn:huge}).
The method we use is triangular factorization.

\par

Although the autocorrelation $\bold r$ has mostly zero values,
the factored autocorrelation $\bold a$ has a great number of nonzero terms,
but fortunately they seem to be converging rapidly (in the middle)
so truncation (of the middle coefficients) seems reasonable.
I wish I could show you a larger matrix, but all I can do is to pack
the signal $\bold a$ into shifted columns of
a lower triangular matrix $\bold A$ like this:
\begin{equation}
\bold A \ = \ \ 
\left[
\begin{array}{rrrrrrrrrrrr}
  1.8&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
  -.6&  1.8&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
  0.0&  -.6&  1.8&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
  -.2&  0.0&  -.6&  1.8&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
  -.6&  -.2&  0.0&  -.6&  1.8&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
\cdot&  -.6&  -.2&  0.0&  -.6&  1.8&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
\cdot&\cdot&  -.6&  -.2&  0.0&  -.6&  1.8&\cdot&\cdot&\cdot&\cdot&\cdot\\
\cdot&\cdot&\cdot&  -.6&  -.2&  0.0&  -.6&  1.8&\cdot&\cdot&\cdot&\cdot\\
\cdot&\cdot&\cdot&\cdot&  -.6&  -.2&  0.0&  -.6&  1.8&\cdot&\cdot&\cdot\\
\cdot&\cdot&\cdot&\cdot&\cdot&  -.6&  -.2&  0.0&  -.6&  1.8&\cdot&\cdot\\
\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&  -.6&  -.2&  0.0&  -.6&  1.8&\cdot\\
\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&  -.6&  -.2&  0.0&  -.6&  1.8
\end{array}
\right]
\label{eqn:lapfacmat}
\end{equation}

If you will allow me some truncation approximations,
I now claim that the laplacian represented by the
matrix in equation (\ref{eqn:huge})
is factored into two parts
$-\nabla^2 =\bold A\T\bold A$
which are upper and lower triangular matrices
whose product forms the autocorrelation seen in (\ref{eqn:huge}).
Recall that triangular matrices
allow quick solutions of simultaneous equations by backsubstitution.
That is what we do with our
deconvolution program.


\section{CAUSALITY AND SPECTRAL FACTORIZATION}

Mathematics sometimes seems a mundane subject,
like when it does the ``accounting'' for an engineer.
Other times it brings unexpected amazing new concepts into our lives.
This is the case with the study of causality and spectral factorization.
There are many little-known, amazing, fundamental ideas here,
some merely named, one worked through to results.

\par
Start with an example.  Consider a mechanical object.
We can strain it and watch it stress or we can stress it and watch it strain.
We feel knowledge of the present and past stress history is all we need
to determine the present value of strain.
Likewise, the converse, history of strain should tell us the stress.
We could say there is a filter that takes us from stress to strain;
likewise another filter takes us from strain to stress.
What we have here is a pair of filters that are mutually inverse
under convolution.
In the Fourier domain, one is literally the inverse of the other.
What is remarkable is that in the time domain, both are causal.
They both vanish before zero lag $\tau=0$.

\par
Not all causal filters have a causal inverse.
The best known name for one that does is ``minimum-phase filter.''
Unfortunately, this name is not suggestive of
the fundamental property of interest,
``causal with a causal (convolutional) inverse.''
I could call it CwCI.
An example of a causal filter without a causal inverse is the unit
delay operator --- with $Z$-transforms, the operator $Z$ itself.
If you delay something, you can't get it back without seeing into the future,
which you are not allowed to do.
Mathematically, $1/Z$ cannot be expressed as a polynomial
(actually, a convergent infinite series) in positive powers of $Z$.

\par
Physics books don't tell us where to expect to find
transfer functions that are CwCI.
I think I know why they don't.
Any causal filter has a ``sharp edge'' at zero time lag where it switches
from nonresponsiveness to responsiveness.
The sharp edge might cause the spectrum to be large at infinite frequency.
If so, the inverse filter is small at infinite frequency.
Either way,
one of the two filters is unmanageable with Fourier transform theory
which
(you might have noticed in the mathematical fine print)
requires signals (and spectra) to have finite energy
which means the function must get real small in that immense space
on the $t$-axis and the $\omega$ axis.
It is impossible for a function to be small and its inverse be small.
These imponderables become manageable in the world of Time Series Analysis
(discretized time axis).

\subsection{Constant Q medium}
\inputdir{futterman}
From the absorption law of a material, spectral factorization yields its impulse response.
The most basic absorption law is the {\em constant Q} model.
According to it, for a downgoing wave
the absorption is proportional to the frequency $\omega$,
proportional to time in the medium $z/v$,
and inversely proportional to the ``quality'' $Q$ of the medium.
Altogether the spectrum of a wave passing through a thickness $z$ will be changed by the factor
$ e^{-|\omega|\tau} = e^{-|\omega|(z/v)/Q} $.
This frequency function is plotted in the top line of Figure \ref{fig:futterman}.
\plot{futterman}{width=6in,height=2.5in}{
	Autocorrelate the bottom signal to get the middle whose FT is the top.
	Spectral factorization works the other way, from top to bottom.
	}

\par
The middle function in Figure \ref{fig:futterman}
is the autocorrelation giving the spectrum (top).
The third is the factorization.
An impulse entering the medium comes out with this shape.
It is causal.
It begins off with a fairly sharp corner and ends with a broad sweep.
A low frequency cannot be packed in a small space,
so we may say it is spread throughout the wave.
The high frequencies are near the sharp corner.

\par
The time-like variable $\tau=(z/v)/Q$ is the variable we get from inverse transforming $|\omega|$.
It is not the total travel time.
It is the extra delay low frequencies experience from the absorption.
In Figure \ref{fig:futterman} its origin is in the middle of the axis.
The one independent parameter is $z/Q$, not both $z$ and $Q$.


\par
There is no physics in this analysis, only mathematics.
A physical system could cause the factored wave to be more spread out
(effectively by an additional all-pass filter),
but physics cannot make it more compact
because the long wavelength cannot be compacted into a smaller space.
Starting from $t=0$ everything is as early as it can be.

\subsection{Uniqueness and invertability}
Interesting questions arise when we are given a spectrum
and find ourselves asking how to find a filter that has that spectrum.
Is the answer unique?  We'll see not.
Is there always an answer that is causal?
Almost always, yes.
Is there always an answer that is causal with a causal inverse (CwCI)?
Almost always, yes.

\par
Let us have an example.
Consider a filter like the familiar time derivative $(1,-1)$ except
let us downweight the $-1$ a tiny bit, say $(1,-\rho)$ where $0<<\rho<1$.
Now the filter $(1,-\rho)$
has a spectrum $(1-\rho Z)(1-\rho/Z)$ with autocorrelation
coefficients $(-\rho, 1+\rho^2,-\rho)$ that look a lot like a second
derivative, but it is a tiny bit bigger in the middle.
Two different waveforms, $(1,-\rho)$ and its time reverse
both have the same autocorrelation.
Spectral factorization could give us both $(1,-\rho)$ and $(\rho,-1)$
but we always want the one that is CwCI.
The bad one is weaker on its first pulse.
Its inverse is not causal.
Below are two expressions for the filter inverse to $(\rho,-1)$,
the first divergent
(filter coefficients at infinite lag are infinitely strong),
the second convergent but noncausal.
\begin{eqnarray}
\frac{1}{ \rho -Z} &=& \frac{ 1}{\rho}\ ( 1 +Z/\rho +Z^2/\rho^2+ \cdots)
\\
\frac{1}{ \rho -Z} &=& \frac{-1}{ Z}\ ( 1 + \rho/Z + \rho^2/Z^2 + \cdots)
\end{eqnarray}
(Please multiply each equation by $\rho -Z$ and see it reduce to $1=1$).

\par
We begin with a power spectrum and our goal is to find a CwCI filter with that spectrum.
If we input to the filter an infinite sequence of random numbers
(white noise)
we should output something with the original power spectrum.

\par
We easily inverse Fourier transform the square root of the power spectrum
getting a symmetrical time function, but
we need a function that vanishes before $\tau=0$.
On the other hand,
if we already had a causal filter with the correct spectrum
we could manufacture many others.
To do so all we need is a family of delay operators to convolve with.
A pure delay filter does not change the spectrum of anything.
Same for frequency-dependent delay operators.
Here is an example of a frequency-dependent delay operator:
First convolve with (1,2) and then deconvolve with (2,1).
Both these have the same amplitude spectrum so their ratio
has a unit amplitude (and nontrivial phase).
If you multiply $(1+2Z)/(2+Z)$ by its Fourier conjugate
(replace $Z$ by $1/Z$) the resulting spectrum is 1 for all $\omega$.

\par
Anything whose nature is delay is death to CwCI.
The CwCI has its energy as close as possible to $\tau=0$.
More formally, my first book, FGDP, proves that the CwCI filter
has for all time $\tau$ more energy between $t=0$ and $t=\tau$
than any other filter with the same spectrum.

\par
Spectra can be factorized by an amazingly wide variety of techniques,
each of which gives you a different insight into this strange beast.
They can be factorized by factoring polynomials, by inserting power series
into other power series, by solving least squares problems,
by taking logarithms and exponentials in the Fourier domain.
I've coded most of them and still find them all somewhat mysterious.

\par
Theorems in Fourier analysis can be interpreted physically in two
different ways, one as given, the other with time and frequency reversed.
For example, convolution in one domain amounts to multiplication in the other.
If we were to express the CwCI concept with reversed domains,
instead of saying the ``energy comes as quick as possible after $\tau=0$''
we would say ``the frequency function is as close to $\omega=0$ as possible.''
In other words, it is minimally wiggly with time.
Most applications of spectral factorization begin with a spectrum,
a real, positive function of frequency.
I once achieved minor fame by starting with a real, positive function of space,
a total magnetic field $\sqrt{H_x^2 +H_z^2}$ measured along the $x$-axis
and I reconstructed the magnetic field components $H_x$ and $H_z$
that were minimally wiggly in space (FGDP p.61).






\subsection{Cholesky decomposition}
Conceptually the simplest computational method of spectral factorization
might be ``Cholesky decomposition.''
For example, the matrix of (\ref{eqn:lapfacmat})
could have been found by Cholesky factorization of (\ref{eqn:huge}).
The Cholesky algorithm takes a positive-definite matrix
$\bold Q$ and factors it into a triangular matrix
times its transpose,
say $\bold Q = \bold T\T \bold T$.
%Equation (\ref{ajt/eqn:polydiv}) is an example of a banded triangular matrix.

\par
It is easy to reinvent the Cholesky factorization algorithm.
To do so,
simply write all the components of a $3\times 3$ triangular matrix
$\bold T$ and then explicitly multiply these elements
times the transpose matrix $\bold T\T$.
You will find that you have everything you need
to recursively build the elements of $\bold T$
from the elements of $\bold Q$.
Likewise for a $4\times 4$ matrix, etc.

\par
The $1\times 1$ case shows that the Cholesky algorithm requires square roots.
Matrix elements are not always numbers.
Sometimes they are polynomials such as $Z$-transforms.
To avoid square roots there is a variation
of the Cholesky method.
In this variation, we factor $\bold Q$ into
$\bold Q=\bold T\T\bold D\bold T$
where $\bold D$ is a diagonal matrix.

\par
Once a matrix has been factored into upper and lower triangles,
solving simultaneous equations
is simply a matter of two backsubstitutions:
(We looked at a special case of backsubstitution
with equation (\ref{eqn:polydiv}).)
For example, we often encounter simultaneous equations of the form
$\bold B\T\bold B\bold m=\bold B\T\bold d$.
Suppose the positive-definite matrix
$\bold B\T\bold B$ has been factored into triangle form
$\bold T\T\bold T\bold m=\bold B\T\bold d$.
To find 
$\bold m$
we first backsolve
$\bold T\T\bold x=\bold B\T\bold d$
for the vector
$\bold x$.
Then we backsolve
$\bold T\bold m=\bold x$.
When
$\bold T$
happens to be a band matrix,
then the first backsubstitution is filtering down a helix
and the second is filtering back up it.
Polynomial division is a special case of back substitution.

\par
Poisson's equation
$\nabla^2 \bold p = -\bold q$
requires boundary conditions which we can honor
when we filter starting from both ends.
We cannot simply solve Poisson's equation as
an initial-value problem.
We could insert the laplace operator
into the polynomial division program,
but the solution would diverge.

\par
Being a matrix method, the Cholesky method of factorization
has a cost proportional to the cube of the size of the matrix.
Because our applications are very large
and because the Cholesky method
does not produce a useful result if we stop part way to completion,
we look further.
The Cholesky method is a powerful method but it does more than we require.
The Cholesky method does not require band matrices,
yet these matrices are what we very often find in applications,
so we seek methods that take advantage of the special properties
of band matrices.

\subsection{Toeplitz methods}
Band matrices are often called Toeplitz matrices.
In the subject of Time Series Analysis are found
spectral factorization methods that require computations
proportional to the dimension of the matrix squared.
They can often be terminated early with a reasonable partial result.
Two Toeplitz methods, the Levinson method
and the Burg method are described in my first textbook, FGDP.
Our interest is multidimensional data sets so
the matrices of interest are truely huge and the cost
of Toeplitz methods is proportional to the square of the matrix size.
Thus, before we find Toeplitz methods
especially useful, we may need to find
ways to take advantage of the sparsity of our filters.

\section{KOLMOGOROFF SPECTRAL FACTORIZATION}
The most abstract method of spectral factorization is that of the Russian mathematician A.N.Kolmogoroff.
I include it here because it is by far the fastest,
so much so that giant problems become practical
such as the solar physics example coming up.

\par
Given that $C(\omega)$ fourier transforms to a causal function of time,
it is next proven that $e^C$ fourier transforms to a causal function of time.
Its filter inverse is $e^{-C}$.
Grab yourself a cup of coffee
and hide yourself away in a quiet place
while you focus on the proof in the next paragraph.

\par
A causal function $c_\tau$ vanishes at negative $\tau$.
Its $Z$ transform $C(Z) = c_0 + c_1 Z + c_2 Z^2 + c_3 Z^3 +\cdots$,
with $Z=e^{i\omega\Delta t}$ is really a Fourier sum.
Its square $C(Z)^2$
convolves a causal with itself so it is causal.
Each power of $C(Z)$ is causal, hence
$e^C=1+C+C^2/2+\cdots$, a sum of causals, is causal.
The time-domain coefficients for $e^C$ could be computed
putting polynomials into power series or faster by Fourier transforms.
The wavelet $e^C$ has inverse $e^{-C}$ also causal.
A causal with a causal inverse is said to be ``minimum phase''.
The filter $1-Z/2$ with inverse $1+Z/2+Z^2/4+\cdots$ is so.
The delay filter $Z^5$ has the noncausal inverse $Z^{-5}$ is not
(output before input).

\par
The next paragraph defines ``Kolmogoroff spectral factorization''.
This arises in applications where one begins with an energy spectrum $|r|^2$
and factors it into an $r e^{i\phi}$ times its conjugate.
The inverse fourier transform of that  $r e^{i\phi}$ is causal.

\par
Relate $c_\tau$ to amplitude $r=r(\omega)$ and phase $\phi=\phi(\omega)$.
\begin{eqnarray}
|r|e^{i\phi} &=&
e^{\ln|r|}e^{i\phi} \ =\
e^{\ln\,|r| + i\phi} \ =\  
e^{\sum_{\tau=0} c_\tau Z^\tau}
\ =\ \exp \left( \sum_{\tau=0} c_\tau Z^\tau\right)
\end{eqnarray}
Given a spectrum $r(\omega)$ we can make a minimum-phase filter with that spectrum.
Since $r(\omega)$ is a real even function of $\omega$, so is its logarithm.
Let the inverse Fourier transform of $\ln |r(\omega)|$ be $u_\tau$,
where $u_\tau$ is a real even function of time.
Imagine a real odd function of time $v_\tau$.
\begin{eqnarray}
|r|e^{i\phi} &=& e^{\ln\,|r| + i\phi} \ =\  e^{\sum_\tau (u_\tau+v_\tau) Z^\tau}
\end{eqnarray}
The phase $\phi(\omega)$ transforms to $v_\tau$.
We can assert causality
by choosing $v_\tau$ so that $u_\tau+v_\tau=0$ $ (=c_\tau)$ for all negative $\tau$.
This defines $v_\tau$ at negative $\tau$.
Since $v_\tau$ is odd, it is known at positive lags too.
More simply,
$v_\tau$ is created when $u_\tau$ is multiplied by a step function of size 2.  
This causal exponent
$(c_0,c_1,\cdots)$
creates a causal minimum-phase filter $|r|e^{i\phi}$
with the specified spectrum $r(\omega)$.

\begin{comment}
\subsection{Code}
\footnotesize
\begin{verbatim}
subroutine kolmogoroff( n, cx)  # Spectral factorization.
integer              i, n         # input:  cx = amplitude spectrum
complex              cx(n)        # output: cx = FT of min phase wavelet
do i= 1, n                     
        cx(i) = clog( cx(i) )
call ftu( -1., n, cx)
do i= 2, n/2 {                   # Make it causal changing only the odd part.
        cx(i)     = cx(i) * 2.
        cx(n-i+2) = 0.
        }
call ftu( +1., n, cx)
do i= 1, n
        cx(i) = cexp( cx(i))
return; end


subroutine ftu( signi, nx, cx )	          # Fourier transform
#   complex fourier transform with traditional scaling (FGDP)
#
#               1         nx          signi*2*pi*i*(j-1)*(k-1)/nx
#   cx(k)  =  -------- * sum cx(j) * e
#              scale     j=1             for k=1,2,...,nx=2**integer
#
#  scale=1 for forward transform signi=1, otherwise scale=1/nx
integer nx, i, j, k, m, istep
real    signi, arg
complex cx(nx), cmplx, cw, cdel, ct
i=1;  while( i<nx) i=2*i
if( i != nx )    call erexit('ftu: nx not a power of 2')
do i= 1, nx
        if( signi<0.)
                cx(i) = cx(i) / nx
j = 1;  k = 1
do i= 1, nx {
        if (i<=j) { ct = cx(j); cx(j) = cx(i); cx(i) = ct }
        m = nx/2
        while (j>m && m>1) { j = j-m; m = m/2 }         # "&&" means .AND.
        j = j+m
        }
repeat {
        istep = 2*k;   cw = 1.;   arg = signi*3.14159265/k
        cdel = cmplx( cos(arg), sin(arg))
        do m= 1, k {
                do i= m, nx, istep
                        { ct=cw*cx(i+k);  cx(i+k)=cx(i)-ct;  cx(i)=cx(i)+ct }
                cw = cw * cdel
                }
        k = istep
        if(k>=nx) break
        }
return; end
\end{verbatim}
\normalsize
\par
Included above is a fast Fourier transform code,
a compact version with the restriction the data length is a power of 2.
Zero time and frequency are the first point in the vector, then positive times, then negative times.
\end{comment}

\par
It is a exercise for the student to show that
a complex-valued time function has
a positive spectrum that is non-symmetrical in frequency
but it may be factored with the same code.


\subsection{Blind deconvolution}
An area of applications that leads directly to spectral factorization
is ``blind deconvolution.''
Here we begin with a signal.
We form its spectrum and factor it.
We could simply inspect the filter and interpret it,
or we might deconvolve it out from the original data.
This topic deserves a fuller exposition, say for example
as defined in some of my earlier books.
Here we inspect a novel example that incorporates the helix.

\inputdir{.}
\plot{solar}{width=6in,height=3.3in}{
	Raw seismic data on the sun (left).
	Impulse response of the sun (right)
	derived by Helix-Kolmogoroff spectral factorization.
	}

\par
Solar physicists have learned how to measure
the seismic field of the sun surface. It's chaotic.
If you created an impulsive explosion on the surface of the sun,
what would the response be?
James Rickett and I applied the helix idea along with Kolmogoroff
spectral factorization to find the impulse response of the sun.
Figure \ref{fig:solar} shows a raw data cube and the derived impulse response.
The sun is huge so the distance scale is in megameters (Mm).
The United States is 5 Mm wide.
Vertical motion of the sun is measured with a video-camera like device
that measures vertical motion by an optical doppler shift.
From an acoustic/seismic point of view,
the surface of the sun is a very noisy place.
The figure shows time in kiloseconds (Ks).
We see about 15 cycles in 5 Ks which is 1 cycle in about 333 sec.
Thus the sun seems to oscillate vertically with about a 5 minute period.
The top plane of the raw data
in Figure \ref{fig:solar} (left panel)
happens to have a sun spot in the center.
The data analysis here is not affected by the sun spot so please ignore it.

\par
The first step of the data processing is
to transform the raw data to its spectrum.
With the helix assumption, computing the spectrum is
virtually the same thing in 1-D space as in 3-D space.
The resulting spectrum was passed to Kolmogoroff spectral factorization code.
The resulting impulse response is on the right side of 
Figure \ref{fig:solar}.
The plane we see on the right top is not lag time $\tau=0$;
it is lag time $\tau=2$ Ks.
It shows circular rings, as ripples on a pond.
Later lag times (not shown) would be the larger circles of expanding waves.
The front and side planes show tent-like shapes.
The slope of the tent gives the (inverse) velocity of the wave
(as seen on the surface of the sun).
The horizontal velocity we see on the sun surface turns out
(by Snell's law)
to be the same as that at the bottom of the ray.
On the front face at early times we see the low velocity (steep) wavefronts
and at later times we see the faster waves.
This is because the later arrivals reach more deeply into the sun.
Look carefully, and you can see two (or even three!) tents inside one another.
These ``inside tents'' are the waves that have bounced once (or more!)
from the surface of the sun.
When a ray goes down and back up to the sun surface,
it reflects and takes off again with the same ray shape.
The result is that a given slope on the traveltime curve
can be found again at twice the distance at twice the time.

\begin{comment}
\section{WILSON-BURG SPECTRAL FACTORIZATION}
(If you are new to this material, you should pass over this section.)
Spectral factorization is the job of taking a power spectrum
and from it finding a causal (zero before zero time) filter with that spectrum.
Methods for this task (there are many)
not only produce a causal wavelet,
but they typically produce one whose
convolutional inverse is also causal.
(This is called the ``minimum phase'' property.)
In other words, with such a filter we can do stable deconvolution.
Here
I introduce a new method of spectral factorization
that looks particularly suitable for the task at hand.
I learned this new method from John Parker Burg who
attributes it to an old paper by Wilson
(I find Burg's explanation, below, much clearer than Wilson's.)
\par
%\begin{notforlecture}
\begin{comment}
Below find subroutine \texttt{lapfac()} which
was used in the previous section
to factor the Laplacian operator.
To invoke the factorization subroutine,
you need to supply one side of an autocorrelation function.
For example, let us specify the negative of the 2-D Laplacian
(an autocorrelation)
in a vector {\tt n = }$256\times 256$ points long.
\end{comment}
\par\noindent
\footnotesize
\begin{verbatim}
        rr[0]   =  4.
        rr[1]   = -1.
        rr[256] = -1.
\end{verbatim}
\normalsize
%\par\noindent
%\begin{comment}
%The reason for the
%$4.00004$ instead of $4.0$
%arises in the theory below where we are required
%to take the logarithm of the spectrum.
%The spectrum of the Laplacian operator is $k_x^2+k_y^2$,
%which vanishes at the point $(k_x,k_y)=(0,0)$
%where the logarithm is minus infinity.
%The .00004 prevents that.
%\end{comment}
\begin{comment}
\par
Subroutine \texttt{lapfac()}
finds the helical derivative (factored negative Laplacian)
and then prepares the required filter
coefficient tables for the helix convolution
and deconvolution subroutines.
\moddex{lapfac}{factor 2-D Laplacian}
Subroutine \texttt{lapfacn()} has its main job
done by subroutine \texttt{wilson\_factor()} \vpageref{lst:wilson}
shown after the Wilson-Burg theory.
\end{comment}
%
\begin{comment}
\subsection{Wilson-Burg theory}
Newton's iteration for square roots
\begin{equation}
a_{t+1} \eq \frac{1}{ 2} \ \left( a_t \ +\ \frac{s}{ a_t} \right)
\label{eqn:newton}
\end{equation}
converges quadratically starting from any real initial guess $a_0$ except zero.
When $a_0$ is negative,
Newton's iteration converges to the negative square root.
\par
Quadratic convergence means that the square of the error
$a_t-\sqrt{s}$
at one iteration is proportional to the error at the next iteration
\begin{eqnarray}
\label{eqn:quadconv}
a_{t+1} - \sqrt{s} \quad\sim\quad ( a_t-\sqrt{s})^2 
\eq  a_t^2 - 2 a_t \sqrt{s} + s \quad > \quad 0
\end{eqnarray}
so, for example if the error is one significant digit at one iteration,
at the next iteration it is two digits, then four, etc.
We cannot use equation (\ref{eqn:quadconv}) in place of the Newton iteration itself,
because it uses the answer $\sqrt{s}$ to get the answer $a_{t+1}$,
and also we need the factor of proportionality.  Notice, however,
if we take the factor to be $1/(2a_t)$,
then $\sqrt{s}$ cancels and
equation (\ref{eqn:quadconv}) becomes itself the Newton iteration (\ref{eqn:newton}).
\par
Another interesting feature of the Newton iteration is that
all iterations (except possibly the initial guess)
are above the ultimate square root.
This is obvious from equation (\ref{eqn:quadconv}).
\par
We can insert spectral functions in the Newton square-root iteration,
for example $s(\omega)$ and $a(\omega)$.
Where the first guess $a_0$ happens to match $\sqrt{s}$,
it will match $\sqrt{s}$ at all iterations.
The Newton iteration is
\begin{equation}
2\ \frac{a_{t+1}}{ a_t} \eq   1 \ +\ \frac{s}{ a_t^2}
\end{equation}
Something inspires Wilson to
express the spectrum $S=\bar A A$ as a $Z$-transform
and then write the iteration
%\cite{wilson}
\begin{equation}
  \frac{\bar A_{t+1}(1/Z) }{ \bar A_t(1/Z)}
 \ +\ 
 \frac{A_{t+1}(Z) }{ A_t(Z)}
\eq
1 \ +\ \frac{S(Z) }{ \bar A_t(1/Z)\  A_t(Z)}
\label{eqn:wilson}
\end{equation}
\par
Now we are ready for the algorithm:
Compute the right side of (\ref{eqn:wilson})
by polynomial division forwards and backwards and then add 1.
Then abandon negative lags and take half of the zero lag.
Now you have $A_{t+1}(Z) / A_t(Z)$.
Multiply out (convolve) the denominator $A_t(Z)$,
and you have the desired result $A_{t+1}(Z)$.
Iterate as long as you wish.
\par
(Parenthetically, for those people familiar with the idea
of minimum phase (if not, see FGDP or PVI),
we show that $A_{t+1}(Z)$ is minimum phase:
Both sides of (\ref{eqn:wilson}) are positive, as noted earlier.
Both terms on the right are positive.
Since the Newton iteration always overestimates,
the 1 dominates the rightmost term.
After masking off the negative powers of $Z$ (and half the zero power),
the right side of (\ref{eqn:wilson}) adds two wavelets.
The 1/2 is wholly real,
and hence its real part always dominates the real part of the rightmost term.
Thus (after masking negative powers) the wavelet on
the right side of  (\ref{eqn:wilson}) has a positive real part,
so the phase cannot loop about the origin.
This wavelet multiplies $A_t(Z)$ to give the final wavelet $A_{t+1}(Z)$
and the product of two minimum-phase wavelets is minimum phase.)
\par
%I plan to factor
%the Laplacian
%\begin{equation}
%S(Z) \eq -{1\over Z^{100}} - {1\over Z} + 4 -Z -Z^{100}
%\end{equation}
%by the Wilson-Burg method.
%It seems less confusing than the Kolmogoroff method.
%Furthermore, it might be better
%because we can think of
%upper and lower triangular matrices,
%instead of recursive filters.
%The advantage of the triangular matrices
%is that they embody time-variable filters.
\par
The input of the program is the spectrum $S(Z)$
and the output is the factor $A(Z)$,
a function with the spectrum $S(Z)$.
I mention here that in later chapters of this book,
the factor $A(Z)$ is known as the inverse Prediction-Error Filter (PEF).
In the Wilson-Burg code below,
$S(Z)$ and $A(Z)$ are $Z$-transform polynomials
but their lead coefficients are extracted off,
so for example, 
$A(z) = (a_0) + (a_1 Z + a_2 Z^2 + \cdots)$
is broken into the two parts \texttt{a0} and \texttt{aa}.
\moddex{wilson}{Wilson-Burg spectral factorization}{43}{82}{user/gee}
%
\begin{exer}
\begin{comment}
\item
You hear from three different people
that a more isotropic representation
of the Laplacian is minus one sixth of
$$
\begin{array} {rrr}
-1     & -4 &  -1 \\
-4     & 20 &  -4 \\
-1     & -4 &  -1
\end{array}
$$
What changes need to be made to subroutine \texttt{lapfac()}?
%
\item
Fomel's factorization:
A simple trick to avoid division in square root computation is to run
Newton's method on the inverse square root instead.
The iteration is then
$R' = \frac{1}{ 2} R(3-R^2 X^2)$
where $R$ converges (quadratically) to $1/\sqrt{X^2}$.
To get the square root, just multiply $R$ by $X^2$.
This leads to a reciprocal version of the Wilson-Burg algorithm.
$A\T/A + \bar A\T/ \bar A = 3 - A \bar A  S $
Here is how it can work:
Construct an inverse autocorrelation ---
for example,
an ideal isotropic smoother;
make a guess for $A$ (min-phase roughener); iterate:
(1) compute $3 - A A* S$,
(2) take its causal part,
(3) convolve with $A$ to get $A\T$.
Each iteration involves just three convolutions
(could be even done without helix).
\end{exer}

%\end{notforlecture}
\end{comment}

\section{HELIX LOW-CUT FILTER}
If you want to see some tracks on the side of a hill,
you want to subtract the hill and see only the tracks.
Usually, however, you don't have a very good model for the hill.
As an 
expedient you could apply a low-cut filter to remove all
slowly variable functions of altitude.
In chapter \ref{paper:ajt} we found the Sea of Galilee
in Figure \ref{fig:galbin} to be too smooth for viewing pleasure
so we made the roughened versions
in Figure \ref{fig:galocut}
using a filter based
on equation (\ref{eqn:lowcut}),
a one-dimensional filter that we could apply
over the $x$-axis or the $y$-axis.
In Fourier space such a filter has a response function of $k_x$
or a function of $k_y$.
The isotropy of physical space tells us
it would be more logical to design a filter that
is a function of
$k_x^2+k_y^2$.
In Figure \ref{fig:helocut} we saw that the helix derivative
$\bold H$
does a nice job.
The Fourier magnitude of its impulse response is $k_r=\sqrt{k_x^2+k_y^2}$.
There is a little anisotropy connected with phase (which way should
we wind the helix, on $x$ or $y$?) but it is
not nearly so severe as that of either component of the gradient,
the two components having wholly different spectra,
amplitude $|k_x|$ or $|k_y|$.

\inputdir{mam}

\par
It is nice having the 2-D helix derivative,
but we can imagine even nicer 2-D low-cut filters.
In one dimension,
equation 
(\ref{eqn:lowcut})
we designed a filters with an adjustable parameter,
a cutoff frequency.
We don't have such an object in 2-D so
I set out to define one.
It came out somewhat
abstract and complicated,
and didn't work very well, 
but along the way
I found a simpler parameter that is very effective in practice.
We'll look at it first.

\sideplot{mam}{width=3.8in,height=3.8in}{
  Mammogram (medical X-ray).
  The cancer is the ``spoked wheel.''
  (I apologize for the inability of paper publishing technology
  to exhibit a clear grey image.)
  The white circles are metal foil used for navigation.
  The little halo around a circle exhibits the impulse
  response of the helix derivative.
}

\par
First I had a problem preparing Figure \ref{fig:mam}.
It shows shows the application of the helix derivative
to a medical X-ray.
The problem was that the original X-ray was all positive
values of brightness so there was a massive amount of
spatial low frequency present.
Obviously an $x$-derivative or a $y$-derivative would
eliminate the low frequency, but the helix derivative did not.
This unpleasant surprise arises
because the filter in equation
(\ref{eqn:lapfac})
was truncated after a finite number of terms.
Adding up the terms actually displayed in equation
(\ref{eqn:lapfac}),
they sum to .183 whereas theoretically the sum of all the terms should be zero.
From the ratio of .183/1.791 we can say that the filter
pushes zero frequency amplitude 90\% of the way to zero value.
When the image contains very much zero frequency amplitude,
this is not good enough.
Better results could be obtained with more coefficients,
and I did use more coefficients,
but simply removing the mean saved me
from needing a costly number of filter coefficients.

\inputdir{helgal}

\par
We can visualize a plot of the magnitude of the 2-D
Fourier transform of the filter
(\ref{eqn:lapfac}).
It is a 2-D function of $k_x$ and $k_y$ and it should
resemble $k_r=\sqrt{k_x^2+k_y^2}$.
It does look like this even when the filter
(\ref{eqn:lapfac})
has been truncated.
The point of the cone $k_r=\sqrt{k_x^2+k_y^2}$ becomes
rounded and the truncated approximation of
$k_r$ does not reach zero at the origin of the $(k_x,k_y)$-plane.
We can force it to vanish at zero frequency
by subtracting .183 from the lead coefficient 1.791.
I did not do that subtraction in Figure
\ref{fig:helgal}
which explains the whiteness in the middle of the lake.

\plot{helgal}{width=6in,height=3.7in}{
  Galilee roughened by gradient and
  by helical derivative.
}


%Experimentation with this figure showed that the helix filter
%need not be long (16 points was enough) but,
%since the inputs were all positive numbers,
%the output was best displayed with its mean removed
%because the short 16 point filter did not remove the mean.


\par
Now let us return to my more logical but less effective approach.
I prepared a half dozen medical X-rays like Figure 
\ref{fig:mam}.
The doctor brought her young son to my office one evening
to evaluate the results.
In a dark room I would show the original X-ray on a big screen
and then suddenly switch to the helix derivative.
Every time I did this, her son would exclaim ``Wow!''
The doctor was not so easily impressed, however.
She was not accustomed to the unfamiliar image.
Fundamentally, the helix derivative applied to her data
does compress the dynamic range making weaker features more readily discernible.
We were sure of this from theory and from
various geophysical examples.
The subjective problem was her unfamiliarity with our display.
I found that I could always spot anomalies more quickly
on the filtered display, but then I would feel more comfortable
when I would discover those same anomalies also present
(though less evident) in the original data.
Thinking this through, I decided the doctor would likely have
been more impressed
had I used a spatial lowcut filter instead of the helix derivative.
That would have left the details of her image
(above the cutoff frequency)
unchanged
altering
only the low frequencies,
thereby allowing me to increase the gain.

%\par
%As we set out, keep in mind that many applications call for
%isotropic filters
%(because physics often tells us there is no prefered orientation).
%This requires us to build upon the operator $-\nabla^2$,
%because all isotropic functions are functions of it.
%It might look anisotropic (because rotating
%$45^\circ$ the finite difference representation of $-\nabla^2$
%seems to change it),
%but in Fourier space
%the well sampled data is at low frequencies,
%and the FT of $-\nabla^2$ is the isotropic expression
%$k_x^2+k_y^2$ which is independent of coordinate rotation.
%
%\par
%Recall the one-dimensional causal lowcut filters (\ref{ajt/eqn:lowcut})
%that we invented earlier.
%Applying one of these to its time reverse gives 
%an expression recognizable as a low-cut filter.
%\begin{equation}
%{1-Z^{-1}\over 1-\rho Z^{-1}}\ \ 
%{1-Z     \over 1-\rho Z     }
%\quad\approx\quad
%{\omega^2 \over \omega^2 + \omega_0^2}
%\label{eqn:lofac}
%\end{equation}
%As you look at this expression,
%think of the numerator and denominator
%as expressions that could be sent to a spectral factorization module.
%For example, in 1-D the numerator
%is the autocorrelation $(-1,2,-1)$.
%The denominator is an autocorrelation like $(-1,2.05,-1)$.
%These two autocorrelations can be factored
%by Kolmogoroff or Burg-Wilson methods.
%Equation
%(\ref{eqn:lofac}) shows them as already factored.
%In two dimensions we can use the same approach.
%The factor of the numerator is the helix derivative
%and the factor of the
%denominator being found likewise.

%\par
%The difficulty with medical x-rays is the large required range of amplitude.
%Part of the image is wholly dark and part wholly light.
%With mammograms, the helix filter ``worked'' because it cut out
%the very low spatial frequencies that amount to a background noise
%on the details.
%A less desireable aspect of the helix derivative is that it alters
%the spectrum at high frequencies making the image less familiar
%to long-time practitioners.
%What we really need here is not the helix derivative but a 2-D low-cut filter.

\par
In 1-D we easily make a low-cut filter
by compounding a first derivative (which destroys low frequencies)
with a leaky integration (which undoes the derivative at all other frequencies).
We can do likewise with a second derivative.
In $Z$-transform notation, we would use something like
$ (-Z^{-1} + 2.00 - Z) / (-Z^{-1} + 2.01 - Z)$.
(The numerical choice of the .01 controls the cutoff frequency.)
We could use spectral factorization to break this spectrum into
causal and anticausal factors.
The analogous filter in 2-D is
$-\nabla^2 /(-\nabla^2 + k_0^2)$
which could also be factored as we did the helix derivative.
I tried it.
I ran into the problem that my helix derivative operator
had a practical built-in parameter, the number of coefficients,
which also behaves like a cutoff frequency.
If I were to continue this project,
I would use expressions for
$-\nabla^2 /(-\nabla^2 + k_0^2)$
directly in the Fourier domain where there is only one adjustable parameter,
the cutoff frequency $k_0$,
and there is no filter length to confuse the issue and puff-up the costs.

\inputdir{mam}

\par
A final word about the doctor.
As she was about to leave my office she suddenly asked
whether I had scratched one of her X-rays.
We were looking at the helix derivative
and it did seem to show a big scratch.
What should have been a line was broken into a string of dots.
I apologized in advance and handed her the original film negatives
which she proceeded to inspect.
``Oh,'' she said,
``Bad news. There are calcification nodules along the ducts.''
So the scratch was not a scratch,
but an important detail that had not been noticed on the original X-ray.

\par
In preparing an illustration for here,
I learned one more lesson.
The scratch was small,
so I enlarged a small portion of the mammogram for display.
The very process of selecting a small portion
followed by scaling the amplitude
between maximum and minimum darkness of printer ink
had the effect enhancing the visibility of the scratch on
the mammogram itself.
Now Figure \ref{fig:scratch} shows it to be
perhaps even clearer than on the helix derivative.
\plot{scratch}{width=6in,height=3.0in}{
        Not a scratch.
        Reducing the $(x,y)$-space range of the illustration
	allowed boosting the gain,
	thus making the non-scratch more prominent.
}

\begin{comment}
\par
An operator for applying the helix filter is
\texttt{helderiv} \vpageref{lst:helderiv}.
\opdex{helderiv}{helix-derivative filter}
\end{comment}

\section{THE MULTIDIMENSIONAL HELIX}
Till now
the helix idea was discussed
as if it were merely a two-dimensional concept.
Here we explore its multidimensional nature.
Our main goal is to do multidimensional convolution
with a one-dimensional convolution program.
This allows us to do multidimensional deconvolution
with a one-dimensional deconvolutional program
which is ``magic'',
i.e.~many novel applications will follow.

\inputdir{XFig}

\par
We do multidimensional deconvolution with causal (one-sided)
one-dimensional filters.
Equation (\ref{eqn:2dpef}) shows such a one-sided filter as
it appears at the end of a 2-D helix.
Figure~\ref{fig:3dpef} shows it in three dimensions.
The top plane in Figure~\ref{fig:3dpef} 
is the 2-D filter seen in equation (\ref{eqn:2dpef}).
The top plane can be visualized as the area around the end of a helix.
Above the top plane are zero-valued anticausal filter coefficients.
\sideplot{3dpef}{width=2.3in}{
  A 3-D causal filter at the starting end of a 3-D helix.
}

\par
It is natural to ask,
``why not put the `1' on a corner of the cube?''
We could do that, but that is not the most general possible form.
A special case of Figure~\ref{fig:3dpef},
stuffing much of the volume with lots of zeros
would amount to a `1' on a corner.
On the other hand, if we assert the basic form has a `1' on a corner
we cannot get Figure~\ref{fig:3dpef} as a special case.
In a later chapter we'll see that we often need as many coefficients
as we can have near the `1'.
In Figure~\ref{fig:3dpef} we lose only those neighboring coefficients
that 1-D causality requires.



\par
Geometrically, the three-dimensional generalization of a helix
is like string on a spool,
but that analogy does not illuminate our underlying conspiracy,
which is to represent multidimensional convolution and deconvolution
as one-dimensional.

%Figure~\ref{fig:3dpef} shows its end, the beginning of a 1-D filter.
%Although Figure~\ref{fig:3dpef} merely shows a half-volume
%of filter coefficients,
%you can see that neighborhood of the ``end of string''
%requires some centering that
%makes it a little tricky to map the half volume
%to a multidimensional cartesian mesh,
%and even a little more tricky
%to convert the cartesian mesh to a helix.

\section{SUBSCRIPTING A MULTIDIMENSIONAL HELIX}
Basic utilities transform back and forth between
multidimensional matrix coordinates and helix coordinates.
The essential module used repeatedly in applications
later in this book is
\texttt{createhelix} \vpageref{lst:createhelix}.
We begin here from its intricate underpinnings.

\par
Fortran77 has a concept of a multidimensional array being equivalent
to a one-dimensional array.
Given that the hypercube specification
\texttt{nd=(n1,n2,n3,...)} defines the storage
\texttt{dimension} of a data array,
we can refer to a data element as either
\texttt{dd(i1,i2,i3,...)} or 
\texttt{dd( i1 +n1*(i2-1) +n1*n2*(i3-1) +...)}.
The helix says to refer to the multidimensional data
by its equivalent one-dimensional index
(sometimes called its vector subscript or linear subscript).

\par
The filter, however, is a much more complicated story than the data:
First, we require all filters to be causal.
In other words, the Laplacian doesn't fit very well,
since it is intrinsically noncausal.
If you really want noncausal filters,
you will need to provide your own time shifts outside the tools supplied here.
Second, a filter is usually a small hypercube, say
\texttt{aa(a1,a2,a3,...)}
and would often be stored as such.
For the helix we must store it in a special one-dimensional form.
Either way, the numbers
\texttt{na= (a1,a2,a3,...)}
specify the dimension of the hypercube.
In cube form, the entire cube could be indexed
multidimensionally as \texttt{aa(i1,i2,...)} or it could be indexed
one-dimensionally as \texttt{aa(ia,1,1,...)} or sometimes
\begin{comment}
\footnote{
        Some programming minutia:
        Fortran77 does not allow you to refer to an array
        by both its cartesian coordinates
        and by its linear subscript in the same subroutine.
        To access it both ways, you need a subroutine call,
        or you dimension it as 
        \texttt{data(n1,n2,...)}
        and then you refer to it as
        \texttt{data(id,1,1,...)}.
        Fortran90 follows the same rule outside modules.
        Where modules use other modules,
        the compiler does not allow you to refer
        to data both ways,
        unless the array is declared as
        \texttt{allocatable}.
        }
\end{comment}
\texttt{aa[ia]} by letting \texttt{ia} cover a large range.
When a filter cube is stored in its normal ``tightly packed'' form
the formula for computing
its one-dimensional index
\texttt{ia} is
\begin{verbatim}
  ia = i1 +a1*i2 +a1*a2*i3 + ...
\end{verbatim}
When the filter cube is stored in an array
with the same dimensions as the data,
\texttt{data[n3][n2][n1]},
the formula for \texttt{ia} is
\begin{verbatim}
  ia = i1 +n1*i2 +n1*n2*i3 + ...
\end{verbatim}

\par
%The fortran compiler knows how to convert from the multidimensional
%cartesian indices to the linear index.
%We will need to do that, as well as the converse.
Module \texttt{decart} below contains
two subroutines that
explicitly provide us the transformations
between the linear index
\texttt{i}
and the multidimensional indices
\texttt{ii= (i1,i2,...)}.
The two subroutines have the logical names
\texttt{cart2line} and
\texttt{line2cart}.
\moddex{decart}{helical-cartesian coordinate conversion}{29}{57}{api/c}

\par
The fortran linear index is closely related to the helix.
There is one major difference, however,
and that is the origin of the coordinates.
To convert from the linear index
to the helix lag coordinate,
we need to subtract the fortran linear index of the ``1.0''
which is usually taken at
\texttt{center= (1+a1/2, 1+a2/2, ..., 1)}.
(On the last dimension, there is no shift because nobody stores the
volume of zero values that would occur before the 1.0.)
The \texttt{decart} module fails for negative subscripts.
Thus we need to be careful to avoid thinking of the filter's 1.0 
(shown in Figure \ref{fig:3dpef})
as the origin of the multidimensional coordinate system
although the 1.0 is the origin in the one-dimensional coordinate system.

\par
Even in one dimension
(see the matrix in equation (\ref{eqn:contran1})),
to define a filter {\it operator} we need to know
not only filter coefficients and a filter length,
but we also need to know the data length.
To define a multidimensional filter using the helix idea,
besides the properties intrinsic to the filter,
we also need to know the circumference of the helix,
i.e., the length on the 1-axis of the data's hypercube
as well as the other dimensions
\texttt{nd=(n1,n2,...)}
of the data's hypecube.

\par
Thinking about convolution on the helix,
it is natural to think about the filter and data being stored
in the same way, that is, by reference to the data size.
This would waste so much space, however,
that our helix filter module
\texttt{helix} \vpageref{lst:helix}
instead stores the filter coefficients in one vector
and their lags in another.
The \texttt{i}-th coefficient value
of the filter goes in  \verb#aa->flt[i]# and
the \texttt{i}-th lag \texttt{ia[i]} goes in \verb#aa->lag[i]#.
The lags are the same as the fortran linear index
except for the overall shift of the 1.0 of a cube of 
data dimension \texttt{nd}.
Our module for convolution on a helix,
\texttt{helicon}. % \vpageref{lst:helicon},
has already an implicit
``1.0'' at the filter's zero lag so we do not store it.
(It is an error to do so.)

%\par
%A helix subscript is similar to a fortran vector subscript.
%To convert, we need to keep track of the data size as mentioned above,
%and we need to shift to lag zero the filter's 1.0
%labeled ``A'' in the example below.
%Logic for identifying the first point A on the starting end of the helix
%and those that lie before the first point (and are hence off the helix) is
%\par\noindent\footnotesize
%\begin{verbatim}
%      . . . . .      . . B . .      C C C C C       D D . . .        0 0 0 . .
%      . . . . .      . . B . .      C C C C C       D D . . .        0 0 0 . .
%      . . A . .      . . B . .      . . . . .       D D . . .   =    0 0 0 . .
%      . . . . .      . . B . .      . . . . .       D D . . .        0 0 . . .
%      . . . . .      . . B . .      . . . . .       D D . . .        0 0 . . .
%      
%             Cube locations for (h <= 0) are A | (B & C) | D.
%\end{verbatim}
%\par\noindent\normalsize
%These are locations for which we will not have adjustable filter values
%because the filter lags are zero or negative.
%Using this logic in multidimensional space,

\par
Module
\texttt{createhelix} \vpageref{lst:createhelix}
allocates memory for a helix filter and builds
filter lags along the helix from the hypercube description.
The hypercube description is not the literal cube
seen in Figure~\ref{fig:3dpef} but
some integers specifying that cube:
the data cube dimensions            \texttt{nd},
likewise the filter cube dimensions \texttt{na},
the parameter \texttt{center} identifying
the location of the filter's ``1.0'',
and a \texttt{gap} parameter used in a later chapter.
To find the lag table,
module \texttt{createhelix} %\vpageref{lst:createhelixmod}
first finds the
fortran linear index of the \texttt{center} point on the filter hypercube.
Everything before that has negative lag on the helix and can be ignored.
(Likewise, in a later chapter we see a \texttt{gap} parameter
that effectively sets even more filter coefficients to zero
so their lags can be ignored too.)
Then it sweeps from the center point over the rest of the filter hypercube
calculating for a data-sized cube \texttt{nd},
the fortran linear index of each filter element.
\moddex{createhelix}{constructing helix filter in N-D}{36}{80}{user/gee}
Near
the end of the code you see the calculation of a parameter \texttt{lag0d}.
This is the count of the number of zeros that
a data-sized fortran array would store
in a filter cube before the filter's 1.0.
We need to subtract this shift
from the filter's fortran linear index to get the lag on the helix.

\par
A filter can be represented literally as a multidimensional cube
like equation (\ref{eqn:2dpef}) shows us in two dimensions
or like Figure~\ref{fig:3dpef} shows us in three dimensions.
Unlike the helical form, in literal cube form,
the zeros preceding the ``1.0'' are explicitly present
so \texttt{lag0} needs to be added back in to get the fortran subscript.
To convert a helix filter \texttt{aa} to fortran's multidimensional hypercube
\texttt{cube(n1,n2,...)} is
module \texttt{boxfilter}: %\vpageref{lst:boxfilter}.
\moddex{boxfilter}{Convert helix filter to (n1,n2,...)}{24}{48}{user/gee}
The \texttt{boxfilter} module is normally used
to display or manipulate a filter
that was estimated in helical form
(usually estimated by the least-squares method).

\begin{comment}
\par
The inverse process to \texttt{boxfilter} is to
convert a fortran hypercube to a helix filter.
For this we have module \texttt{unbox}. %\vpageref{lst:unbox}.
It abandons all zero-valued coefficients
such as those that should be zero before the box's 1.0.
It abandons the ``1.0'' as well, because it is implicitly present
in the helix convolution module \texttt{helicon} \vpageref{lst:helicon}.
\moddex{unbox}{Convert hypercube filter to helix}
An example of using \texttt{unbox} would be copying some numbers
such as the factored laplacian in equation (\ref{eqn:lapfac})
into a cube and then converting it to a helix.
\end{comment}

\par
A reasonable arrangement for a small 3-D filter is 
\texttt{na=\{5,3,2\}}
and
\texttt{center=\{3,2,1\}}.
Using these arguments, I used 
\texttt{createhelix} \vpageref{lst:createhelix} to create a filter.
I set all the helix filter coefficients to 2.
Then I used module \texttt{boxfilter} \vpageref{lst:boxfilter}
to put it in a convenient form for display.
%After this conversion, the coefficient \texttt{aa[0][1][2]} is 1, not 2.
Finally, I printed it:
\par\noindent
\footnotesize
\begin{verbatim}
          0.000  0.000  0.000  0.000  0.000
          0.000  0.000  1.000  2.000  2.000
          2.000  2.000  2.000  2.000  2.000
          ---------------------------------
          2.000  2.000  2.000  2.000  2.000
          2.000  2.000  2.000  2.000  2.000
          2.000  2.000  2.000  2.000  2.000
\end{verbatim}
\normalsize
\par\noindent
%using \texttt{printn} \vpageref{lst:print}:
%\moddex{print}{print out helix filter coefficients}

\par
Different data sets have different sizes.
To convert a helix filter from one data size to another,
we could drop the filter into a cube with module \texttt{cube}.
Then we could extract it with module \texttt{unbox}
specifying any data set size we wish.
Instead we use module
\texttt{regrid} %\vpageref{lst:regrid}
prepared by Sergey Fomel
which does the job without reference to an underlying filter cube.
He explains his \texttt{regrid} module thus:
        \begin{quotation}
        Imagine a filter being cut out of a piece of paper and
        glued on another paper, which is then rolled to form a helix.
\par            % change this to a blank line and POOF! paragraph disappears.
        We start by picking a random point (let's call it
        \texttt{rand}) in the
        cartesian grid and placing the filter so that its center
        (the leading 1.0) is on top of that point.
        \texttt{rand} should be larger than (or equal to)
        \texttt{center} and
        smaller than \texttt{min (nold, nnew)},
        otherwise the filter might stick outside the grid
        (our piece of paper.)
        \texttt{rand=nold/2} will do (assuming the filter is small),
        although nothing should change
        if you replace \texttt{nold/2} with a random integer array
        between \texttt{center} and \texttt{nold - na}.
\par
        The linear coordinate of \texttt{rand} is \texttt{h0}
        on the old helix and \texttt{h1} on the new helix.
        Recall that the helix lags \verb#aa->lag#
        are relative to the center.
        Therefore, we need to add \texttt{h0}
        to get the absolute helix coordinate (\texttt{h}).
        Likewise, we need to subtract \texttt{h1}
        to return to a relative coordinate system.
        \end{quotation}

\moddex{regrid}{Convert filter to different data size}{24}{43}{user/gee}

%Sergey Fomel comments,
%\begin{quotation}
%``A confusing part of
%\texttt{regrid}
%is the shift to the middle of the grid.
%Imagine a filter which looks like
%\begin{verbatim}
%                x x
%                x x
%              1 x x
%              x x x
%              x x x
%\end{verbatim}
%If the "1" was placed at the beginning of the helix grid,
%the top two rows would have troubles in the cart2helix conversion.
%The shift solves this problem.
%The amount of the shift is not important
%as long as it is greater than (or equal to) the lag of "1" in the filter.''
%\end{quotation}
%
%
%\par
%Sergey Fomel
%(defining \texttt{c=ii})
%comments that the cartesian-to-helix transform
%has the following strange properties:
%\begin{verbatim}
%         h(c1) + h(c2)  =  h(c1+c2-1)
%         h(c1) - h(c2)  =  1 + h(c1-c2+1)
%\end{verbatim}
%which could be used to simplify code.

%\bibliographystyle{sep}
%\bibliography{MISC}

%\end{notforlecture}

\clearpage

