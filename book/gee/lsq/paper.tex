% copyright (c) 2005 Jon Claerbout

\title{Model fitting by least squares}
\author{Jon Claerbout}

\maketitle

\label{paper:lsq}

%\def\figdir{Fig}
%\def\sx#1{}
%\def\bx{}
%\def\bxbx#1#2{#1}
%\def\eq{\quad =\quad}
\def\ff{{\bf f}}
\def\dd{{\bf d}}

\sx{least squares}
%\begin{notforlecture}
\par
The first level of computer use in science and engineering is \bx{modeling}.
Beginning from physical principles and design ideas,
the computer mimics nature.
After this, the worker looks at the result and thinks a while,
then alters the modeling program and tries again.
The next, deeper level of computer use is that the computer itself
examines the results of modeling and reruns the modeling job.
This deeper level
is variously called
``\bx{fitting}'' or
``\bx{estimation}'' or
``\bx{inversion}.''
We inspect the \bx{conjugate-direction method} of fitting
and write a subroutine for it that will be used in most of
the examples in this book.
%\end{notforlecture}

\section{HOW TO DIVIDE NOISY SIGNALS}
%\begin{notforlecture}
If "inversion" is dividing by a matrix,
then the place to begin is dividing one number by another,
say one function of frequency by another function of frequency.
A single parameter fitting problem arises in Fourier analysis,
where we seek a ``best answer'' at each frequency,
then combine all the frequencies to get a best signal.
Thus emerges a wide family of interesting and useful applications.
However, Fourier analysis first requires us to introduce complex numbers
into statistical estimation.
\par
Multiplication in the Fourier domain is \bx{convolution} in the time domain.
Fourier-domain division is time-domain \bx{deconvolution}.
This division is challenging when $F$ has observational error.
Failure erupts if zero division occurs.
More insidious are the poor results we obtain
when zero division is avoided by a near miss.

\subsection{Dividing by zero smoothly}
\sx{divide by zero}
\sx{zero divide}
Think of any real numbers $x$, $y$, and $f$ and any
program containing $x=y/f$.
How can we change the program so that it never divides by zero?
A popular answer is to change $x=y/f$
to $x=yf/(f^2+\epsilon^2)$, where $\epsilon$ is any tiny value.
When $|f| >> |\epsilon|$,
then $x$ is approximately $y/f$ as expected.
But when the divisor $f$ vanishes,
the result is safely zero instead of infinity.
The transition is smooth,
but some criterion is needed to choose the value of $\epsilon$.
This method may not be the only way or the best way
to cope with
\bxbx{zero division}{zero divide},
but it is a good way,
and it permeates the subject of signal analysis.

\par
To apply this method in the Fourier domain,
suppose that $X$, $Y$, and $F$ are complex numbers.
What do we do then with $X=Y/F$?
We multiply the
top and bottom by the complex conjugate $\overline{F}$,
and again add $\epsilon^2$ to the denominator.
Thus,
\begin{equation}
X(\omega) \eq
\frac{ \overline{F(\omega)} \ Y(\omega)  }{ \overline{F(\omega)} F(\omega) \ +\ \epsilon^2}
\label{eqn:z1}
\end{equation}
Now the denominator must always be a positive number greater than zero,
so division is always safe.
Equation~(\ref{eqn:z1}) ranges continuously from
\bx{inverse filter}ing, with
\sx{filter ! inverse}
$X=Y/F$, to filtering with $X=\overline{F}Y$,
which is called ``\bx{matched filter}ing.''
\sx{filter ! matched}
Notice that for any complex number $F$,
the phase of $1/F$ equals the phase of $\overline{F}$,
so the filters 
have the same phase.

\subsection{Damped solution}
Equation (\ref{eqn:z1}) is the solution to an optimization problem
that arises in many applications.
Now that we know the solution, let us formally define the problem.
First, we will solve a simpler problem with real values:
we will choose to minimize the \bx{quadratic function} of $x$:
\begin{equation}
Q(x) \eq (f x-y)^2 + \epsilon^2 x^2
\label{eqn:z2}
\end{equation}
The second term is called a ``\bx{damping} factor''
because it prevents $x$ from going to $\pm \infty$ when $f\rightarrow 0$.
Set $dQ/dx=0$, which gives
\begin{equation}
0 \eq f(f x-y) + \epsilon^2 x
\label{eqn:z3}
\end{equation}
This yields the earlier answer $x=fy/(f^2+\epsilon^2)$.

\par
With Fourier transforms,
the signal $X$ is a complex number at each frequency $\omega$.
So we generalize equation~(\ref{eqn:z2}) to
\begin{equation}
Q(\bar X, X) \eq
(\overline{FX-Y})        (FX-Y) + \epsilon^2 \bar X X \eq
(\bar X \bar F - \bar Y) (FX-Y) + \epsilon^2 \bar X X
\label{eqn:z4}
\end{equation}
To minimize $Q$ we could use a real-values approach,
where we express
$X=u+iv$ in terms of two real values $u$ and $v$
and then set $\partial Q/\partial u=0$ and $\partial Q/\partial v=0$.
The approach we will take, however,
is to use complex values,
where we set
$\partial Q/\partial X=0$ and $\partial Q/\partial \bar X=0$.
Let us examine $\partial Q/\partial \bar X$:
\begin{equation}
{\partial Q(\bar X, X)\over \partial  \bar X}  \eq
\bar F (FX-Y) + \epsilon^2  X  \eq 0
\label{eqn:z5}
\end{equation}
The derivative $\partial Q/\partial X$ is
the complex conjugate of $\partial Q/\partial \bar X$.
So if either is zero, the other is too.
Thus we do not need to specify both
$\partial Q/\partial X=0$ and $\partial Q/\partial \bar X=0$.
I usually set
$\partial Q/\partial \bar X$ equal to zero.
Solving equation~(\ref{eqn:z5}) for $X$
gives equation~(\ref{eqn:z1}).

\par
Equation~(\ref{eqn:z1}) solves $Y=XF$ for $X$,
giving the solution for what is called
``the \bx{deconvolution} problem with a known wavelet $F$.''
Analogously we can use $Y=XF$ when the filter $F$ is unknown,
but the input $X$ and output $Y$ are given.
Simply interchange $X$ and $F$ in the derivation and result.

\subsection{Smoothing the denominator spectrum}

\par
Equation~(\ref{eqn:z1}) gives us one way to divide by zero.
Another way is stated by the equation
\begin{equation}
X(\omega) = \frac{ \overline{F}(\omega) Y(\omega) }
{\left<
\overline{F}(\omega)  F(\omega)
\right> }
\label{eqn:z6}
\end{equation}
where the strange notation in the denominator means
that the spectrum there should be smoothed a little.
Such smoothing fills in the holes in the spectrum
where zero-division is a danger,
filling not with an arbitrary numerical value $\epsilon$
but with an average of nearby spectral values.
Additionally, if the denominator spectrum
$\overline{F}(\omega) F(\omega)$ is rough,
the smoothing creates a shorter autocorrelation function.
\par
Both divisions,
equation~(\ref{eqn:z1}) and
equation~(\ref{eqn:z6}),
irritate us by requiring us to specify a parameter,
but for the latter, the parameter has a clear meaning.
In the latter case we smooth a spectrum with a smoothing
window of width, say $\Delta\omega$
which this corresponds inversely to a time interval over which we smooth.
Choosing a numerical value for  $\epsilon$ has not such a simple interpretation.

\inputdir{antoine}

\par
We jump from simple mathematical theorizing
towards a genuine practical application when I grab some real data,
a function of time and space from another textbook.
Let us call this data $f(t,x)$ and its 2-D Fourier transform
$F(\omega, k_x)$.
The data and its autocorrelation are in Figure~\ref{fig:antoine10}.

\par
The autocorrelation $a(t,x)$ of $f(t,x)$ is
the inverse 2-D Fourier Transform  of 
$\overline{F}(\omega, k_x) F(\omega, k_x)$.
Autocorrelations $a(x,y)$
satisfy the symmetry relation
$a(x,y)=a(-x,-y)$.
Figure~\ref{fig:antoine11}
shows only the interesting quadrant of the two independent quadrants.
We see the autocorrelation of a 2-D function has some
resemblance to the function itself but differs in important ways.

\par
Instead of messing with two different functions $X$ and $Y$ to divide,
let us divide $F$ by itself.
This sounds like $1=F/F$ but we will
watch what happens when we do the division carefully
avoiding zero division in the ways we usually do.
\par
Figure~\ref{fig:antoine11} shows
what happens with
\begin{equation}
\label{eqn:z7}
1 = F/F \quad \approx \quad \frac{\overline{F}F}{\overline{F}F+\epsilon^2}
\end{equation}
and with
\begin{equation}
\label{eqn:z8}
1 = F/F \quad \approx\quad  \frac{\overline{F}F}{\left< \overline{F}F \right>}
\end{equation}
%
\plot{antoine10}{width=\textwidth,height=7in}{
  2-D data (right) and a quadrant of its autocorrelation (left).
  Notice the longest nonzero time lag on the data is about 5.5 sec
  which is the latest nonzero signal on the autocorrelation.
}
%
From Figure~\ref{fig:antoine11} we notice that both methods of
avoiding zero division give similar results.
By playing with the $\epsilon$ and the smoothing width
the pictures could be made even more similar.
My preference, however, is the smoothing.
It is difficult to make physical sense of choosing a numerical value
for $\epsilon$.
It is much easier to make physical sense of choosing a smoothing window.
The smoothing window is in $(\omega,k_x)$ space,
but Fourier transformation tells us its effect in $(t,x)$ space.

\plot{antoine11}{width=\textwidth,height=7in}{
  Equation~\ref{eqn:z7} (left) and
  equation~\ref{eqn:z8} (right).
  Both ways of dividing by zero give similar results.
}

\subsection{Imaging}
The example of dividing a function by itself $(1=F/F)$ might not
seem to make much sense, but it is very closely related to estimation
often encountered in imaging applications.
It's not my purpose here to give a lecture on imaging theory, but
here is an over-brief explanation.
\par
Imagine a downgoing wavefield $D(\omega,x,z)$. Propagating against irregularities in  the medium $D(\omega, x , z )$
creates by scattering  an upgoing wavefield $U(\omega,x,z)$.
Given $U$ and $D$, if there is a strong temporal correlation between them
at any $(x,z)$ it likely means there is a reflector nearby that is
manufacturing $U$ from $D$.
This reflectivity could be quantified by $U/D$.
At the earth's surface the surface boundary condition says something like
$U=D$ or $U=-D$.   Thus at the surface we have something like $F/F$.
As we go down in the earth, the main difference is that $U$ and $D$ get
time-shifted in opposite directions, so $U$ and $D$ are similar but
for that time difference.  Thus, a study of how we handle $F/F$ is worthwhile.

\subsection{Formal path to the low-cut filter}

This book defines many geophysical estimation applications.
Many of them amount to statement of two goals.
The first goal is a data fitting goal,
the goal that the model should imply some observed data.
The second goal is that the model be not too big or too wiggly.
We will state these goals as two residuals, each of which is ideally zero.
A very simple data fitting goal would be that
the model $m$ equals the data $d$,
thus the difference should vanish, say $0\approx  m- d$.
A more interesting goal is that the model should match the data
especially at high frequencies but not necessarily at low frequencies.
\begin{equation}
0 \quad\approx\quad  -i\omega(m - d)
\end{equation}
A danger of this goal is that the model could have a zero-frequency component
of infinite magnitude as well as large amplitudes for low frequencies.
To suppress this, we need the second goal, a model residual
which is to be minimized.  We need a small number $\epsilon$.
The model goal is
\begin{equation}
0 \quad\approx\quad \epsilon \ m
\end{equation}
To see the consequence of these two goals,
we add the squares of the residuals
\begin{equation}
 Q(m) \eq \omega^2 (m-d)^2 + \epsilon^2  m^2
\end{equation}
and then we minimize $Q(m)$ by setting its derivative to zero
\begin{equation}
0\eq {dQ\over dm} \eq 2 \omega^2 (m-d) + 2\epsilon^2  m
\end{equation}
or
\begin{equation}
m \eq  {\omega^2 \over \omega^2+ \epsilon^2}\  d
\label{eqn:lowcut}
\end{equation}
which is the low-cut filter
with a cutoff frequency of $\omega_0=\epsilon$.
%we found less formally earlier, equation (\ref{ajt/eqn:locut}).

\par
Of some curiosity and significance is the numerical choice of $\epsilon$.
The general theory says we need an epsilon,
but does not say how much.
For now let us simply rename $\epsilon=\omega_0$
and think of it as a ``cut-off frequency''.

%Our low-pass filter approach in Chapter \ref{ajt/paper:ajt}
%made it quite clear that $\epsilon$ is a filter cutoff
%which might better have been named $\omega_0$.
%We experimented with some objective tests
%for the correct value of $\omega_0$,
%a subject that we will return to later.

%	
%Figure~\ref{fig:antoine11}.
%\activeplot{antoine11}{width=6in,height=8.5in}{ER}{
%        Smoothing the denominator spectrum. Update makefile.
%	}
	
%

\subsection{The plane-wave destructor}
%
We address the question of shifting signals into best alignment. The most natural approach might seem to be via cross correlations.  That is indeed a good approach when signals are shifted by large amounts.  Here we assume signals are shifted by small amounts, often less than a single pixel.  We'll take an approach closely related to differential equations. Consider this definition of a residual. 
\begin{equation}
0 \quad \approx \quad \hbox{residual}(t,x) \eq \left( \frac{\partial}{\partial x} + p \frac{\partial}{\partial t} \right) u(t,x)
\label{eqn:PWDresidual}
\end{equation}
By taking derivatives we see the residual vanishes when the two-dimensional observation $u(t,x)$ matches the equation of moving waves $u(t-px)$.  The parameter $p$ has units inverse to velocity, the velocity of propagation. 
\par
In practice, $u(t,x)$ might not be a perfect wave but an observed field of many waves that we might wish to fit to the idea of a single wave of a single $p$. We seek the parameter $p$.  First we need a method of discretization that allows the mesh for ${\rm d}u/{\rm d}t$ to overlay exactly $\partial u /\partial x$.  To this end I chose to represent the $t$-derivative by averaging a finite difference at $x$ with one at $x+\Delta x$. 
\begin{equation}
\frac{\partial u}{\partial t} \quad \approx \quad \frac{1}{2} 
\left(
\frac{u(t+\Delta t,x) - u(t,x) }{\Delta t}
\right) + \frac{1}{2}
\left(
\frac{u(t+\Delta t,x+\Delta x) - u(t,x+\Delta x) }{\Delta t}
\right)
\end{equation}
Likewise there is an analogous expression for the $x$-derivative with $t$ and $x$ interchanged.  Now the difference operator $\delta_x + p\delta_t$ is a two-dimensional filter that fits  on a 2 x 2 differencing star.  We may represent equation~(\ref{eqn:PWDresidual}) as a matrix operation,
\begin{equation}
{\bf 0} \quad \approx\quad  {\bf r} = {\bf A}{\bf u}
\end{equation}
where the two-dimensional convolution with the differential operator is denoted ${\bf A}$.
\par
%The module {\tt wavekill()}  applies the operator a $\delta_x + b \delta_t$, which can be specialized to the operators we will actually need, namely $\delta_x, \delta_t, \delta_x+p_i\delta_t$. 
%\moddex{wavekill}{wavekill()}
%\par
Now let us find the numerical value of $p$ that fits a plane wave $u(t-px)$ to  observations $u (t , x )$. Let $x$ be an abstract vector whose components are values of $\partial u /\partial x$ taken everywhere on a 2-D mesh in $(t,x)$. Likewise, let $t$ contain $\partial u /\partial t$.  Since we want ${\bf x} + p {\bf t} \approx {\bf 0}$, we minimize the quadratic function of $p$, 
\begin{equation}
Q( p) = ({\bf x} + p {\bf t}) \cdot ({\bf x} + p {\bf t}) 
\end{equation}
by setting to zero the derivative. We get 
\begin{equation}
p \eq - \ \frac{ {\bf x} \cdot {\bf t} }{ {\bf t} \cdot {\bf t} } 
\end{equation}
Since data will not always fit the model very well, it may be helpful to have some way to measure how good the fit is. I suggest
\begin{equation}
C^2 \eq 1 \ -\  \frac{ ({\bf x} + p {\bf t}) \cdot ({\bf x} + p {\bf t}) }{ {\bf x} \cdot {\bf x}} 
\end{equation}
which, on inserting $p=-({\bf x} \cdot {\bf t})/({\bf t} \cdot {\bf t})$, leads to $C$ , where 
\begin{equation}
C \eq \frac{ {\bf x} \cdot {\bf t} } { \sqrt{ ( {\bf x} \cdot {\bf x} ) ( {\bf t}\cdot{\bf t}) }}
\end{equation} 
is known as the ``{\bf normalized correlation}.''   
%The program for this calculation is straightforward.
%The name {\tt puck2d()}  denotes {\it picking} on a contin{\it uu}m. 

%\moddex{puck2d}{puck2d()}

\inputdir{puck}

\sideplot{puckin}{width=3in}{
	Input synthetic seismic data includes a low level of noise.}  

\sideplot{residual5}{width=3in}{
	Residuals, i.e., an evaluation of $U_x + pU_t$.}  

\sideplot{puckout}{width=3in}{
	Output values of $p$ are shown by the slope of short line segments.
	}

Subroutine {\tt puck2d} shows the code that generated
Figure~\ref{fig:puckin} through \ref{fig:puckout}.
An example based on synthetic data is shown
in Figures~\ref{fig:puckin}-\ref{fig:puckout}.
The synthetic data in Figure~\ref{fig:puckin} mimics
a reflection seismic field profile,
including one trace that is slightly delayed
as if recorded on a patch of unconsolidated {\bf soil}. 

\par
Figure~\ref{fig:residual5} shows the {\bf residual}.
The residual is small in the central region of the data;
it is large where there is {\bf spatial alias}ing;
and it is large at the transient onset of the signal.
The residual is rough because of the noise in the signal,
because it is made from derivatives,
and because the synthetic data was made by nearest-neighbor interpolation.
Notice that the residual is not particularly large for the delayed trace. 
\par
Figure~\ref{fig:puckout} shows the dips.
The most significant feature of this figure
is the sharp localization of the dips surrounding the delayed trace.
Other methods based on ``beam stacks'' or Fourier concepts
might lead us to conclude that the aperture must be large
to resolve a wide range of angles.
Here we have a narrow aperture (two traces),
but the dip can change rapidly and widely.
\par
Finally, an important practical matter.   Taking derivatives
boosts high frequencies (effectively multiplying by $-i\omega$).
These high frequencies may have a poorer signal to noise ratio than the raw data.
To compensate for that, I commonly integrate (or leaky integrate)
the time axis on the raw data before I begin.

\inputdir{lomask}

\par
Once the stepout $p = d t /d x$ is known between each of the signals, it is a simple matter to integrate to get the total time shift.  A real-life example is shown in Figure~\ref{fig:twod}.
%
\sideplot{twod}{width=3in}{
	A seismic line before and after flattening.}  
%
In this case the flattening was a function of $x$ only.  More interesting (and more complicated) cases arise when the stepout $p = d t /d x$ is a function of both $x$ and $t$.  The code shown here should work well in such cases.
\par
A disadvantage, well known to people who routinely work with finite-difference solutions to partial differential equations, is that for short wavelengths a finite difference operator is not the same as a differential operator; therefore the numerical value of $p$ is biased.  This problem can be overcome in the following way.  First estimate the slope $p= d t /d x$ between each trace. Then shift the traces to flatten them.  Now there may be a residual $p$ because of the bias in the initial estimate of $p$.  This process can be iterated until the data is flattened.  Everywhere in a plane we have solved a least squares problem for a single value $p$.  In the next section we undertake to solve least squares problems for multiple parameters. 
\begin{exer}
\item It is possible to reject two dips with the operator
\begin{equation}
(\partial_x + p_1 \partial_t)(\partial_x + p_2 \partial_t)
\end{equation}
This is equivalent to
\begin{equation}
\left(
\frac{\partial^2}{\partial x^2} + a \frac{\partial^2}{\partial x \partial t} + b \frac{\partial^2}{\partial t^2} 
\right)
u(t,x) \eq v(t,x) \quad \approx \quad 0
\end{equation}
where $u$ is the input signal and $v$ is the output signal.  Show how to solve for $a$ and $b$ by minimizing the energy in $v$.
\item Given $a$ and $b$ from the previous exercise, what are $p_1$ and $p_2$?
\end{exer}
	
	
\section{MULTIVARIATE LEAST SQUARES}

\subsection{Inside an abstract vector}
In engineering uses,
a vector has three scalar components that
correspond to the three dimensions of the space in which we live.
In least-squares data analysis, a vector is a one-dimensional array
that can contain many different things.
Such an array is an ``\bx{abstract vector}.''
For example, in earthquake studies,
the vector might contain the time
an earthquake began, as well as its latitude, longitude, and depth.
Alternatively, the abstract vector
might contain as many components as there are seismometers,
and each component might be the arrival time of an earthquake wave.
Used in signal analysis,
the vector might contain the values of a signal
at successive instants in time or,
alternatively, a collection of signals.
These signals might be ``\bx{multiplex}ed'' (interlaced)
or ``demultiplexed'' (all of each signal preceding the next).
When used in image analysis,
the one-dimensional array might contain an image,
which could itself be thought of as an array of signals.
Vectors, including abstract vectors,
are usually denoted by \bx{boldface letters} such as $\bold p$ and $\bold s$.
Like physical vectors,
abstract vectors are \bx{orthogonal}
when their dot product vanishes: $\bold p \cdot \bold s =0$.
Orthogonal vectors are well known in physical space;
we will also encounter them in abstract vector space.

\par
We consider first a hypothetical application
with one data vector $\dd$ and two
fitting vectors $\bold f_1$ and $\bold f_2$.
Each fitting vector is also known as a ``\bx{regressor}.''
Our first task is to approximate the data vector $\dd$
by a scaled combination of the two regressor vectors.
The scale factors $x_1$ and $x_2$
should be chosen so that the model matches the data; i.e.,
\begin{equation}
        \dd  \quad \approx \quad \bold f_1 x_1 + \bold f_2 x_2
        \label{eqn:wish1}
\end{equation}
\par
Notice that we could take the partial derivative
of the data in (\ref{eqn:wish1}) with respect to an unknown,
say $x_1$,
and the result is the regressor $\bold f_1$.
The \bx{partial derivative} of all theoretical data
with respect to any model parameter
gives a \bx{regressor}.
\par
\boxit{
        A \bx{regressor} is a column in the
        matrix of partial-derivatives, 

        $\partial d_i /\partial m_j$.
        }

\par
The fitting goal (\ref{eqn:wish1}) is often expressed in the more compact
mathematical matrix notation $\dd  \approx \bold F   \bold x $,
but in our derivation here
we will keep track of each component explicitly
and use mathematical matrix notation to summarize the final result.
Fitting the observed data $\bold d = \bold d^{\rm obs}$
to its two theoretical parts
          $\bold f_1x_1$ and $\bold f_2x_2$
can be expressed
as minimizing the length of the residual vector $\bold r$, where
\begin{eqnarray}
        \bold 0 \quad\approx\quad
        \bold r &=&  \bold d^{\rm theor} -  \bold d^{\rm obs}
        \\
        \bold 0 \quad\approx\quad
        \bold r &=&  \bold f_1 x_1 + \bold f_2 x_2  \ -\ \dd
        \label{eqn:resdef}
\end{eqnarray}

We use a dot product to construct a sum of squares (also called a ``\bx{quadratic form}'')
of the components of the residual vector:
\begin{eqnarray}
Q(x_1,x_2) &=& \bold r \cdot \bold r \\
Q(x_1,x_2) &=&
                   (\ff_1 x_1 + \ff_2 x_2 - \dd )
           \cdot
                   (\ff_1 x_1 + \ff_2 x_2 - \dd )
\end{eqnarray}
To find the gradient of the quadratic form $Q(x_1,x_2)$,
you might be tempted to expand out the dot product into all nine terms
and then differentiate.
It is less cluttered, however, to remember the product rule, that
\begin{equation}
{d\over dx} \bold r \cdot \bold r
\eq
{d\bold r \over dx} \cdot \bold r
+
\bold r
\cdot
{d\bold r \over dx}
\end{equation}
Thus, the gradient of $ Q(x_1,x_2)$  is defined by its two components:

\begin{eqnarray} \label{eqn:Q1comp}
 {\partial Q \over \partial x_1} &= &
                    \ff_1  \cdot (\ff_1 x_1 + \ff_2 x_2 -\dd )  
                         +        (\ff_1 x_1 + \ff_2 x_2 -\dd ) \cdot  \ff_1
 \\
 {\partial Q \over \partial x_2} &= &
                            \ff_2  \cdot (\ff_1 x_1 + \ff_2 x_2 -\dd )  
                         +   (\ff_1 x_1 + \ff_2 x_2 -\dd ) \cdot  \ff_2
\end{eqnarray}
Setting these derivatives to zero and using
$(\ff_1 \cdot \ff_2)=(\ff_2 \cdot \ff_1)$ etc.,
we get
\begin{eqnarray}
(\ff_1 \cdot \dd ) &= & (\ff_1 \cdot \ff_1) x_1 + (\ff_1 \cdot \ff_2)  x_2  \\
(\ff_2 \cdot \dd ) &= & (\ff_2 \cdot \ff_1) x_1 + (\ff_2 \cdot \ff_2)  x_2
\end{eqnarray}
We can use these two equations to solve for
the two unknowns $x_1$ and $x_2$.
Writing this expression in matrix notation, we have
\begin{equation}
\left[ 
\begin{array}{c}
  (\ff_1 \cdot \dd ) \\ 
  (\ff_2 \cdot \dd ) \end{array} \right] 
\eq \left[ 
\begin{array}{ccc}
  (\ff_1 \cdot \ff_1) & (\ff_1 \cdot \ff_2)  \\
  (\ff_2 \cdot \ff_1) & (\ff_2 \cdot \ff_2)  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  x_1 \\ 
  x_2 \end{array} \right]  \label{eqn:2by2}
\end{equation}
It is customary to use matrix notation without dot products.
To do this, we need some additional definitions.
To clarify these definitions,
we inspect vectors 
$\ff_1$, $\ff_2$, and $\dd$ of three components.
Thus 
\begin{equation}
\bold F \eq [ \ff_1 \quad \ff_2 ] \eq 
\left[ 
\begin{array}{ccc}
  f_{11} & f_{12}  \\
  f_{21} & f_{22}  \\
  f_{31} & f_{32}  \end{array} \right] 
\end{equation}
Likewise, the {\it transposed} matrix $\bold F\T$ is defined by
\begin{equation}
\bold F\T \eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\end{equation}
Using this matrix ${\bold F\T}$ there is a simple expression
for the gradient calculated in equation~(\ref{eqn:Q1comp}).
It is used in nearly every example in this book.
%
\begin{equation}
\bold g \eq 
\left[
\begin{array}{c}
\frac{\partial Q}{\partial x_1} \\
\frac{\partial Q}{\partial x_2}
\end{array}
\right]
\eq
\left[
\begin{array}{c}
{\bf f}_1 \cdot {\bf r} \\
{\bf f}_2 \cdot {\bf r}
\end{array}
\right]
\eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21}  & f_{31} \\
  f_{12} & f_{22}  & f_{32}
 \end{array} \right] 
\left[
\begin{array}{c}
r_1 \\
r_2 \\
r_3
\end{array}
\right]
\eq
{\bf F}\T {\bf r}
\label{eqn:gexample}
\end{equation}
%
In words this expression says, the gradient is found by putting the residual into the adjoint operator ${\bf g} = {\bf F}\T {\bf r}$. Notice the gradient ${\bf g}$ has the same number of components as the unknown solution ${\bf x}$, so we can think of the gradient as a $\Delta {\bf x}$, something we could add to ${\bf x}$ getting ${\bf x}  + \Delta {\bf x}$.  Later we'll see how much of $\Delta {\bf x}$ we'll want to add to ${\bf x}$.  We will have reached the best solution when we find the gradient ${\bf g} = {\bf 0}$ vanishes which happens, as equation~(\ref{eqn:gexample}) says, when the residual is orthogonal to all the fitting functions (all the rows in the matrix ${\bf F}\T$, the columns in ${\bf F}$, are perpendicular  to ${\bf r}$). 
\par
The matrix in equation (\ref{eqn:2by2})
contains dot products.
Matrix multiplication is an abstract way of representing the dot products:
\begin{equation}
\left[ 
\begin{array}{ccc}
  (\ff_1 \cdot \ff_1) & (\ff_1 \cdot \ff_2)  \\
  (\ff_2 \cdot \ff_1) & (\ff_2 \cdot \ff_2)  \end{array} \right] 
 \eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\left[ 
\begin{array}{ccc}
  f_{11} & f_{12}  \\
  f_{21} & f_{22}  \\
  f_{31} & f_{32}  \end{array} \right] 
\label{eqn:covmat}
\end{equation}
Thus, equation (\ref{eqn:2by2}) without dot products is
\begin{equation}
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  d_1 \\ 
  d_2 \\ 
  d_3 \end{array} \right]
\eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\left[ 
\begin{array}{ccc}
  f_{11} & f_{12}  \\
  f_{21} & f_{22}  \\
  f_{31} & f_{32}  \end{array} \right] 
\left[ 
\begin{array}{ccc}
  x_1  \\
  x_2  \end{array} \right] 
\end{equation}
which has the matrix abbreviation
\begin{equation}
\bold F\T \dd \eq ( \bold F\T \; \bold F )  \bold x
\label{eqn:analytic}
\end{equation}
Equation
(\ref{eqn:analytic})
is the classic result of least-squares
fitting of data to a collection of regressors.
Obviously, the same matrix form applies when there are more than
two regressors and each vector has more than three components.
Equation
(\ref{eqn:analytic})
leads to an \bx{analytic solution} for $\bold x$
using an inverse matrix.
To solve formally for the unknown $\bold x$,
we premultiply by the inverse matrix $( \bold F\T \; \bold F )^{-1}$:

\par
\begin{equation}
\bold x \eq
( \bold F\T \; \bold F )^{-1} \;
\bold F\T \dd 
\label{eqn:sln}
\end{equation}
\boxit{
The central result of
\bxbx{least-squares}{least squares, central equation of}
theory is
$\bold x =
( \bold F\T \; \bold F )^{-1} \;
\bold F\T \dd $
We see it everywhere.
}

\par
Let us examine all the second derivatives of $Q(x_1,x_2)$ defined by equation (\ref{eqn:allquadratics}).
Any multiplying $\dd$ will not survive the second derivative, so the terms we are left with are
\begin{equation}
Q(x_1,x_2) = (\ff_1\cdot\ff_1) x_1^2 +
           2 (\ff_1\cdot\ff_2) x_1 x_2 +
             (\ff_2\cdot\ff_2) x_2^2
\end{equation}
After taking the second derivative, we can organize all these terms in a matrix
\begin{equation}
\frac{\partial^2 Q}{\partial x_i\partial x_j} \eq
        \left[
                \begin{array}{cc}
                        (\ff_1\cdot  \ff_1) &  (\ff_1 \cdot \ff_2)   \\
                        (\ff_2\cdot  \ff_1) &  (\ff_2 \cdot \ff_2)  
                \end{array}
        \right]
\end{equation}
Comparing this to equation (\ref{eqn:covmat}) we conclude
that $\bold F\T \bold F$ is a matrix of second derivatives.
This matrix is also known as the
\bxbx{Hessian}{Hessian}.
This matrix often plays an important role in small problems.

\par
Larger problems tend to have insufficient computer memory for the Hessian matrix
because it is the size of model space squared.
Where model space is a multidimensional earth image,
that's a large number of values even before squaring.
Therefore, this book rarely works with the Hessian,
working instead with gradients.
\par
Rearrange parentheses representing (\ref{eqn:allmats}).
\begin{equation}
\bold F\T \dd \eq  \bold F\T \; (\bold F   \bold x )
\label{eqn:comp}
\end{equation}
Equation
(\ref{eqn:analytic})
led to the ``analytic'' solution (\ref{eqn:sln}).
In a later section on conjugate directions,
we will see that equation
(\ref{eqn:comp})
expresses better than
(\ref{eqn:sln})
the philosophy of iterative methods.

\par
Notice how equation
(\ref{eqn:comp})
invites us to cancel the matrix
$\bold F\T$
from each side.
We cannot do that of course, because
$\bold F\T$
is not a number, nor is it a square matrix with an inverse.
If you really want to cancel the matrix $\bold F\T$, you may,
but the equation is then only an approximation
that restates our original goal (\ref{eqn:wish1}):
\begin{equation}
\dd  \quad \approx \quad \bold F   \bold x 
\label{eqn:wish2}
\end{equation}

\par
A speedy problem solver might
ignore the mathematics covering the previous page,
study his or her application until he or she
is able to write the \bx{statement of goals}
                     \sx{goals, statement of}
(\ref{eqn:wish2}) = (\ref{eqn:wish1}),
premultiply by $\bold F\T$,
replace $\approx$ by =,
getting~(\ref{eqn:analytic}),
and take
(\ref{eqn:analytic})
to a simultaneous equation-solving program to get $\bold x$.

\par
What I call ``\bx{fitting goals}'' are called
``\bx{regressions}'' by statisticians.
In common language the word regression means
to ``trend toward a more primitive perfect state''
which vaguely resembles reducing the size of (energy in)
the residual $\bold r = \bold F \bold x - \bold d $.
Formally this is often written as:
\begin{equation}
\min_{\bold x} \ \  \| \bold F \bold x - \bold d \| 
\label{eqn:norm}
\end{equation}
The notation above with two pairs of vertical lines
looks like double absolute value,
but we can understand it as a reminder to square and sum all the components.
This formal notation is more explicit
about what is constant and what is variable during the fitting.

\subsection{Normal equations}
\par

An important concept is that when energy is minimum,
the residual is orthogonal to the fitting functions.
The fitting functions are the column vectors
$\bold f_1$, $\bold f_2$, and $\bold f_3$.
Let us verify only that the dot product $ \bold r \cdot \bold f_2 $ vanishes;
to do this, we'll show
that those two vectors are orthogonal.
Energy minimum is found by
\begin{equation}
0  \quad = \quad {\partial\over \partial x_2}\ \bold r \cdot \bold r
   \quad = \quad 2\; \bold r \cdot {\partial \bold r\over \partial x_2}
   \quad = \quad 2\; \bold r \cdot \bold f_2
\label{eqn:orthogtofit}
\end{equation}
(To compute the derivative refer to equation (\ref{eqn:resdef}).)
Equation (\ref{eqn:orthogtofit}) shows that
the residual is orthogonal to a fitting function.
The fitting functions are the column vectors in the fitting matrix.

\par
The basic least-squares equations are often called
the ``\bx{normal}'' equations.
The word ``normal'' means perpendicular.
We can rewrite equation
(\ref{eqn:comp})
to emphasize the perpendicularity.
Bring both terms to the right,
and recall the definition of the residual $\bold r$
from equation (\ref{eqn:resdef}):
\begin{eqnarray}
\label{eqn:fitorth1}
\bold 0  &=& \bold F\T ( \bold F   \bold x - \dd)  \\
\bold 0  &=& \bold F\T  \bold r 
\label{eqn:fitorth2}
\end{eqnarray}
Equation (\ref{eqn:fitorth2}) says that the \bx{residual} vector $\bold r$
is perpendicular to
each row in the $\bold F\T$ matrix.
These rows are the \bx{fitting function}s.
Therefore, the residual, after it has been minimized,
is perpendicular to
{\it all}
the fitting functions.

%\begin{notforlecture}
\subsection{Differentiation by a complex vector}

\sx{differentiate by complex vector}
\sx{complex vector}
\sx{complex operator}
\par
Complex numbers frequently arise in physical applications,
particularly those with Fourier series.
Let us extend the multivariable least-squares theory
to the use of complex-valued unknowns $\bold x$.
First recall how complex numbers were handled
with single-variable least squares;
i.e.,~as in the discussion leading up to equation~(\ref{eqn:z5}).
Use a prime, such as $\bold x\T$, to denote the complex conjugate
of the transposed vector $\bold x$.
Now write the positive \bx{quadratic form} as
\begin{equation}
Q(\bold x\T, \bold x) \eq
(\bold F\bold x - \bold d)\T
(\bold F\bold x - \bold d)
\eq
(\bold x\T \bold F\T - \bold d\T)
(\bold F\bold x - \bold d)
\label{eqn:6-1-23}
\end{equation}

After equation (\ref{eqn:z4}),
we minimized a quadratic form $Q(\bar X,X)$
by setting to zero both
$\partial Q/\partial \bar X$ and $\partial Q/\partial X$.
We noted that only one of
$\partial Q/\partial \bar X$ and $\partial Q/\partial X$
is necessarily zero
because they are conjugates of each other.
Now take the derivative of $Q$
with respect to the (possibly complex, row) vector $\bold x\T$.
Notice that $\partial Q/\partial  \bold x\T$ is the complex conjugate transpose
of $\partial Q/\partial  \bold x$.
Thus, setting one to zero sets the other also to zero.
Setting $\partial Q/\partial \bold x\T =\bold 0$ gives the normal equations:
\begin{equation}
\bold 0 \eq \frac{\partial Q}{\partial \bold x\T} \eq
\bold F\T (\bold F \bold x - \bold d)
\label{eqn:normeq}
\end{equation}
The result is merely the complex form of
our earlier result (\ref{eqn:fitorth1}).
Therefore,
differentiating by a complex vector
is an abstract concept,
but it gives the same set of equations
as differentiating by each scalar component,
and it saves much clutter.
%\end{notforlecture}

%\begin{notforlecture}
\subsection{From the frequency domain to the time domain}
Equation~(\ref{eqn:z4}) is a frequency-domain quadratic form
that we minimized by varying a single parameter,
a Fourier coefficient.
Now we will look at the same problem in the time domain.
We will see that the time domain offers flexibility with
boundary conditions, constraints, and weighting functions.
The notation will be that a filter $f_t$ has input $x_t$ and output $y_t$.
In Fourier space this is $Y=XF$.
There are two applications to look at,
unknown filter $F$ and unknown input $X$.

\subsubsection{Unknown filter}
When inputs and outputs are given,
the problem of finding an unknown filter appears to be overdetermined,
so we write $\bold y \approx \bold X \bold f$
where the matrix $\bold X$ is a matrix of downshifted columns like
(\ref{eqn:contran2}).
Thus the quadratic form to be minimized
is a restatement of equation~(\ref{eqn:6-1-23})
with filter definitions:
\begin{equation}
Q(\bold f\T, \bold f) \eq
(\bold X\bold f - \bold y)\T
(\bold X\bold f - \bold y)
\end{equation}
The solution $\bold f$ is found just as we found
(\ref{eqn:normeq}),
and it is the set of simultaneous equations
$ \bold 0 = \bold X\T(\bold X\bold f - \bold y)$.

\subsubsection{Unknown input: deconvolution with a known filter}
\sx{deconvolution}
For solving the unknown-input problem,
we put the known filter $f_t$ in a matrix of downshifted columns $\bold F$.
Our statement of wishes is now to find $x_t$ so that
$\bold y \approx \bold F \bold x$.
We can expect to have trouble finding unknown inputs $x_t$
when we are dealing with certain kinds of filters,
such as \bx{bandpass filter}s.
If the output is zero in a frequency band,
we will never be able to find the input in that band
and will need to prevent $x_t$ from diverging there.
We do this by the statement that we wish
$\bold 0\approx\epsilon\,\bold x$,
where $\epsilon$ is a parameter that is small
and whose exact size will be chosen by experimentation.
Putting both wishes into a single, partitioned matrix equation gives
\begin{equation}
 \left[ 
  \begin{array}{c}
   \bold 0 \\ 
   \bold 0
  \end{array}
 \right] 
\quad\approx\quad
 \left[ 
  \begin{array}{c}
   \bold r_1 \\ 
   \bold r_2
  \end{array}
 \right] 
\eq
 \left[ 
  \begin{array}{c}
   \bold F \\ 
   \epsilon \  \bold I
  \end{array}
 \right] 
 \ 
 \bold x
\quad - \quad
 \left[ 
  \begin{array}{c}
   \bold y \\ 
   \bold 0
  \end{array}
 \right] 
\end{equation}
To minimize the residuals $\bold r_1$ and $\bold r_2$,
we can minimize the scalar
$\bold r\T \bold r = \bold {r'}_1\bold r_1 + \bold {r'}_2\bold r_2$.
This is
\begin{eqnarray}
Q(\bold x\T, \bold x) &=& (\bold F  \bold x  - \bold y)\T (\bold F\bold x-\bold y)
                                        + \epsilon^2 \bold x\T \bold x
                                                                \nonumber \\
                     &=& (\bold x\T \bold F\T - \bold y\T) (\bold F\bold x-\bold y)
                                        + \epsilon^2 \bold x\T \bold x
\label{eqn:knownfilt}
\end{eqnarray}
We solved this minimization
in the frequency domain
(beginning from equation~(\ref{eqn:z4})).
\par
Formally the solution is found just as with equation (\ref{eqn:normeq}),
but this solution looks unappealing in practice
because there are so many unknowns and because
the problem can be solved much more quickly
in the Fourier domain.
To motivate ourselves to solve this problem in the time domain,
we need either to find an approximate solution method that is
much faster, or to discover that
constraints or time-variable weighting functions
are required in some applications.
This is an issue we must be continuously alert to,
whether the cost of a method is justified by its need.

%\item
%Try other lags in~\EQN{2conv} such as
%$(0,1,0)'$ and 
%$(0,0,1)'$.
%Which works best?
%Why?

%\end{notforlecture}

\begin{exer}
\item
\sx{zero absolute temperature}
In 1695, 150 years before Lord Kelvin's absolute temperature scale,
120 years before Sadi Carnot's PhD thesis,
40 years before Anders Celsius,
and 20 years before Gabriel Farenheit,
the French physicist Guillaume
\bx{Amontons},
deaf since birth,
took a mercury manometer (pressure gauge) and
sealed it inside a glass pipe (a constant volume of air).
He heated it to the boiling point of water at $100^\circ$C.
As he lowered the temperature to freezing at $0^\circ$ C,
he observed the pressure dropped by 25\% .
He could not drop the temperature any further
but he supposed that if he could drop it further by a factor of three,
the pressure would drop to zero (the lowest possible pressure)
and the temperature would have been the lowest possible temperature.
Had he lived after Anders Celsius he might have calculated
this temperature to be $-300^\circ$ C (Celsius).
Absolute zero is now known to be $-273^\circ$ C.
\par
It is your job to be Amontons' lab assistant.
Your $i$th measurement of temperature
$T_i$ you make with Issac Newton's thermometer and
you measure pressure $P_i$ and volume $V_i$ in the metric system.
Amontons needs you to fit his data with the regression
$
0\approx \alpha(T_i-T_0)-P_i V_i
$
and calculate the temperature shift $T_0$ that Newton should have made
when he defined his temperature scale.
%In the theory of least squares fitting, what is the data?
%What are the fitting functions?
%and what are the two model parameters?
Do not solve this problem!
Instead, cast it in the form of equation (\ref{eqn:wish1}),
identifying the data $d$ and the two column vectors
$f_1$ and
$f_2$
that are the fitting functions.
Relate the model parameters $x_1$ and $x_2$
to the physical parameters $\alpha$ and $T_0$.
Suppose you make ALL your measurements at room temperature,
can you find $T_0$?  Why or why not?

\end{exer}

\section{KRYLOV SUBSPACE ITERATIVE METHODS}
The \bx{solution time} for simultaneous \bx{linear equations}
grows cubically with the number of unknowns.
There are three regimes for solution;
which one is applicable
depends on the number of unknowns $m$.
For $m$ three or less, we use analytical methods.
We also sometimes use analytical methods on matrices of size $4\times 4$
if the matrix contains many zeros.
A 1988 my desktop workstation solved a $100 \times 100$
system in a minute.
Ten years later it would do a $600\times 600$ system in about a minute.
A nearby more powerful computer would do
$1000\times 1000$ in a minute.
Since the computing effort increases with the third power of the size,
and since $4^3=64\approx 60$,
an hour's work solves a four times larger matrix,
namely $4000\times 4000$ on the more powerful machine.
For significantly larger values of $m$,
exact numerical methods must be abandoned
and \bx{iterative method}s must be used.
\par
(The compute time for a rectangular matrix is slightly more pessimistic.
It is the product of the number of data points $n$
times the number of model points squared $m^2$.
This happens to be the cost of computing the matrix
$\bold F\T\bold F$ from $\bold F$.
Since the number of data points generally exceeds the number of model
points $n>m$ by a substantial factor
(to allow averaging of noises),
it leaves us with significantly fewer than 4000 points in model space.)
\par
A square image packed into a 4096 point vector is a $64\times 64$ array.
The computer power for linear algebra to give us solutions that
fit in a $k\times k$ image is thus proportional
to $k^6$, which means that even though computer power grows rapidly,
imaging resolution using ``exact numerical methods'' hardly
grows at all from our $64\times 64$ current practical limit.

\par
The retina in our eyes captures an image of size about $1000\times 1000$
which is a lot bigger than $64\times 64$.
Life offers us many occasions where final images exceed the 4000
points of a $64\times 64$ array.
To make linear algebra (and inverse theory) relevant to such applications,
we investigate special techniques.
A numerical technique known as the
``\bx{conjugate-direction method}''
works well for all values of $m$ and is our subject here.
As with most simultaneous equation solvers,
an exact answer (assuming exact arithmetic)
is attained in a finite number of steps.
And if $n$ and $m$ are too large to allow enough iterations,
the iterative methods can be interrupted at any stage,
the partial result often proving useful.
Whether or not a partial result actually is useful
is the subject of much research;
naturally, the results vary from one application to the next.

\subsection{Sign convention}
On the last day of the survey, a storm blew up,
the sea got rough, and the receivers drifted further downwind.
The data recorded that day
had a larger than usual difference
from that predicted by the final model.
We could call
$(\bold d-\bold F\bold m)$
the {\it \bx{experimental error}}.
(Here
$\bold d$ is data,
$\bold m$ is model parameters, and
$\bold F$ is their linear relation).

\par
The alternate view is that our theory was too simple.
It lacked model parameters for the waves and the drifting cables.
Because of this model oversimplification
we had a {\it \bx{modeling error}} of the opposite polarity
$(\bold F\bold m-\bold d)$.
\par
A strong experimentalist prefers to think of the error
as experimental error, something for him or her to work out.
Likewise a strong analyst likes to think
of the error as a theoretical problem.
(Weaker investigators might be inclined to take the opposite view.)

\par
Regardless of the above, and opposite to common practice,
I define the \bx{sign convention} for the error (or residual) as
$(\bold F\bold m-\bold d)$.
When we choose this sign convention,
our hazard for analysis errors will be reduced
because $\bold F$ is often complicated and formed by combining many parts.
\begin{comment}
So in this book
we see positive signs on operators
and we see residuals initialized by the negative of the data,
often with subroutine \texttt{negcopy()}.
\progdex{negcopy}{copy and negate}
\end{comment}
\par
\boxit{
        Beginners often feel disappointment
        when the data does not fit the model very well.
        They see it as a defect in the data
        instead of an opportunity
        to design a stronger theory.
        }
\par

\subsection{Method of random directions and steepest descent}
\sx{random directions} \sx{steepest descent}
\par
Let us minimize the sum of the squares of the components
of the \bx{residual} vector given by
\begin{eqnarray}
{\rm residual}
&=&
{\rm 
\hbox{transform} \ \ \  \ \hbox{model space}
\ \ -\ \  \
\ \hbox{data space}
}
        \\                                                                               
\left[
%\matrix { \matrix { \cr \cr \bold r  \cr \cr \cr } }
\begin{array}{c}
\, \\
\, \\
\bold r\\
\, \\
\, \\
\,
\end{array}
\right]
       &=&
       \ \ 
\left[
\begin{array}{cccccc}
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \bold F \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \,
%  \Matrix {
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr \bold F \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    }
\end{array}
  \right]
 \ \ \ \
\left[
\begin{array}{c}
\, \\
\, \\
\bold x\\
\, \\
\, \\
\,
\end{array}
%  \matrix {
%    \matrix {  \cr \bold x \cr \cr }
%    }
  \right]
  \ \ \ 
\ \ \ \ \ -\ \ \ \ \ 
\left[
\begin{array}{c}
\, \\
\, \\
\bold d\\
\, \\
\, \\
\,
\end{array}
%  \matrix { 
%    \matrix {  \cr  \cr \bold d \cr  \cr  \cr}
%    }
  \right]
\label{eqn:cg1b}
\end{eqnarray}
\par
A \bx{contour plot} is based on an altitude function of space.
The altitude is the \bx{dot product}  $\bold r \cdot \bold r$.
By finding the lowest altitude,
we are driving the residual vector  $\bold r$  as close as we can to zero.
If the residual vector  $\bold r$  reaches zero, then we have solved
the simultaneous equations  $\bold d= \bold F \bold x$.
In a two-dimensional world the vector  $\bold x$  has two components,
$(x_1,x_2)$.
A contour is a curve of constant
$\bold r \cdot \bold r$  in $(x_1 , x_2 )$-space.
These contours have a statistical interpretation as contours
of uncertainty in $(x_1 , x_2 )$, with measurement errors in $\bold d$.
\par
Let us see how a random search-direction
can be used to reduce the residual
$0\approx \bold r= \bold F \bold x - \bold d$.
Let $\Delta \bold x$ be an abstract vector
with the same number of components as the solution $\bold x$,
and let $\Delta \bold x$ contain arbitrary or random numbers.
We add an unknown quantity $\alpha$
of vector $\Delta \bold x$ to the vector $\bold x$,
and thereby create $\bold x_{\rm new}$:
\begin{equation}
\bold x_{\rm new} \eq \bold x+\alpha \Delta \bold x
\label{eqn:oldx}
\end{equation}
This gives a new residual:
\begin{eqnarray}
\bold r_{\rm new} &=& \bold F\ \bold x_{\rm new}           -\bold d \\
\bold r_{\rm new} &=& \bold F( \bold x+\alpha\Delta\bold x)-\bold d\\
\bold r_{\rm new} \eq
\bold r+\alpha \Delta\bold r
                  &=& (\bold F \bold x-\bold d)
                                                +\alpha\bold F\Delta\bold x 
\label{eqn:resupdatelin}
\end{eqnarray}
which defines $\Delta \bold r = \bold F \Delta \bold x$.

\par
Next we adjust $\alpha$ to minimize the dot product:
$ \bold r_{\rm new} \cdot \bold r_{\rm new} $
\begin{equation}
(\bold r+\alpha\Delta \bold r)\cdot (\bold r+\alpha\Delta \bold r) \eq
\bold r\cdot \bold r + 2\alpha (\bold r \cdot \Delta \bold r) \ +\ 
\alpha^2 \Delta \bold r \cdot \Delta \bold r
\label{eqn:mindot}
\end{equation}
Set to zero its derivative with respect to  $\alpha$ using the chain rule
\begin{equation}
0\eq 
(\bold r+\alpha\Delta \bold r)
\cdot
\Delta \bold r
\ +\ 
\Delta \bold r
\cdot
(\bold r+\alpha\Delta \bold r)
\eq
2
(\bold r+\alpha\Delta \bold r)
\cdot
\Delta \bold r
\label{eqn:newresperp}
\end{equation}
which says that
the new residual $\bold r_{\rm new} = \bold r + \alpha \Delta \bold r$ is
perpendicular to the ``fitting function'' $\Delta \bold r$.
Solving gives the required value of $\alpha$.
\begin{equation}
\alpha \eq - \ { (\bold r \cdot \Delta \bold r ) \over
( \Delta \bold r \cdot \Delta \bold r ) }
\label{eqn:alfa}
\end{equation}

\par
A ``computation \bx{template}'' for the method of random directions is
\def\padarrow{\quad\longleftarrow\quad}
\label{lsq/'randtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow {\rm random\ numbers}$          \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \  \Delta \bold x$      \\
\>      \> $\alpha \padarrow
                -(       \bold r \cdot \Delta\bold r )/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\Delta \bold x$       \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}
A nice thing about the method of random directions is that you
do not need to know the adjoint operator $\bold F\T$.

\par
In practice, random directions are rarely used.
It is more common to use the \bx{gradient} direction than a random direction.
Notice that a vector of the size of $\Delta \bold x$ is
\begin{equation}
\bold g \eq  \bold F\T \bold r
\end{equation}
Notice also that this vector can be found by taking the gradient
of the size of the residuals:
\begin{equation}
{\partial \over  \partial \bold x\T }  \ \bold r \cdot \bold r
\eq
{\partial \over  \partial \bold x\T }  \ 
( \bold x\T \, \bold F\T  \ -\  \bold d\T) \,
( \bold F  \, \bold x   \ -\  \bold d)
\eq
\bold F\T \  \bold r
\end{equation}
Choosing $\Delta \bold x$ to be the gradient vector
$\Delta\bold x = \bold g = \bold F\T \bold r$
is called ``the method of \bx{steepest descent}.''

\par
Starting from a model $\bold x = \bold m$ (which may be zero),
below is a \bx{template} of pseudocode for minimizing the residual
$\bold 0\approx \bold r = \bold F \bold x - \bold d$
by the steepest-descent method:
\label{lsq/'sdtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow \bold F\T\         \bold r$      \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \  \Delta \bold x$      \\
\>      \> $\alpha \padarrow
                -(       \bold r \cdot \Delta\bold r )/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\Delta \bold x$       \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}

\subsection{The meaning of the gradient}
Imagine yourself doing a big modeling job by these methods.
At each iteration you will be pushing the residual into
the adjoint operator to see which direction you will move next,
$\Delta \bold m = \bold F\T\bold r$.
If each iteration is taking you a day,
you will be looking at all your intermediate results each day.
You will be looking at your current model $\bold m$ and your
current residual $\bold r$.
But your residual $\bold r$ can be carried into model space
with the $\bold F\T$ operator.
We have been calling this the gradient and the model change
$\Delta \bold m = \bold F\T\bold r$,
and it is, but it is also the current residual
as it appears in model space.
Viewing it, you'll see where in model space
your theoretical data mismatches your observed data.
In some applications it might be more informative than $\bold r$ itself.
\par

\subsection{Null space and iterative methods}
In applications where we fit
$\bold d \approx\bold F \bold x$,
there might exist a vector (or a family of vectors)
defined by the condition $\bold 0 =\bold F \bold x_{\rm null}$.
This family is called a \bx{null space}.
For example, if the operator $\bold F$ is a time derivative,
then the null space is the constant function;
if the operator is a second derivative,
then the null space has two components, a constant function
and a linear function, or combinations of them.
The null space is a family of model components that have no effect on the data.
\par
When we use the steepest-descent method,
we iteratively find solutions by this updating:
\begin{eqnarray}
\bold x_{i+1} &=& \bold x_i + \alpha \Delta \bold x                     \\
\bold x_{i+1} &=& \bold x_i + \alpha \bold F\T \bold r                   \\
\bold x_{i+1} &=& \bold x_i + \alpha \bold F\T(\bold F\bold x -\bold d)
\end{eqnarray}
After we have iterated to convergence,
the gradient $ \Delta \bold x$ vanishes
as does $\bold F\T(\bold F\bold x -\bold d)$.
Thus, an iterative solver gets the same solution
as the theory leading to equation (\ref{eqn:sln}).
\par
Suppose that by adding a huge amount of $\bold x_{\rm null}$,
we now change $\bold x$
and continue iterating.
Notice that $ \Delta \bold x$ remains zero
because $\bold F \bold x_{\rm null}$ vanishes.
Thus we conclude that any null space in the initial guess $\bold x_0$
will remain there unaffected by the gradient-descent process.

\par
Linear algebra theory enables us to dig up the entire null space
should we so desire.
On the other hand, the computer demands might be vast.
Even the memory for holding the many $\bold x$ vectors could be prohibitive.
A much simpler and more practical goal
is to find out if the null space has any members,
and if so, to view some of them.
To try to see a member of the null space,
we take two starting guesses
and we run our iterative solver for each of them.
If the two solutions,
$\bold x_1$ and $\bold x_2$,
are the same, there is no null space.
If the solutions differ, the difference
is a member of the null space.
Let us see why:
Suppose after iterating to minimum residual we find
\begin{eqnarray}
\bold r_1 &=& \bold F\bold x_1 - \bold d
\\
\bold r_2 &=& \bold F\bold x_2 - \bold d 
\end{eqnarray}
We know that the residual squared is a convex quadratic function
of the unknown $\bold x$.
Mathematically that means the minimum value is unique,
so $\bold r_1 =\bold r_2$.
Subtracting
we find
$0=\bold r_1-\bold r_2 =\bold F(\bold x_1-\bold x_2)$
proving that $\bold x_1-\bold x_2$ is a model in the null space.
Adding $\bold x_1-\bold x_2$ to any to any model $\bold x$
will not change the theoretical data.
Are you having trouble visualizing $\bold r$ being unique,
but $\bold x$ not being unique?  Imagine that $\bold r$
happens to be independent of one of the components of $\bold x$.
That component is non-unique.
More generally, it is some linear combination of components of $\bold x$
that $\bold r$ is independent of.

\par
\boxit{
        A practical way to learn about the existence of null spaces
        and their general appearance is simply to try
        gradient-descent methods
        beginning from various different starting guesses.
        }

\par
``Did I fail to run my iterative solver long enough?'' is
a question you might have.
If two residuals from two starting solutions are not equal,
$\bold r_1 \ne \bold r_2$,
then you should be running your solver through more iterations.

\par
\boxit{
        If two different starting solutions
        produce two different residuals,
        then you didn't run your solver through enough iterations.
        }

\subsection{Why steepest descent is so slow}
\inputdir{Matlab}
Before we can understand why the \bx{conjugate-direction method} is so fast,
we need to see why the
\bxbx{steepest-descent method}{steepest descent}
is so slow.
The process of selecting $\alpha$ is called ``\bx{line search}'',
but for a linear problem like the one we have chosen here,
we hardly recognize choosing $\alpha$ as searching a line.
A more graphic understanding of the whole process is possible
from considering a two-dimensional space
where the vector of unknowns $\bold x$
has just two components, $x_1$ and $x_2$.
Then the size of the residual vector $\bold r \cdot \bold r$ can be
displayed with a contour plot in the plane of $(x_1,x_2)$.
Figure \ref{fig:yunyue}
shows a contour plot of the penalty function
of $(x_1,x_2)=(m_1,m_2)$.
The gradient is perpendicular to the contours.
Contours and gradients are {\it curved lines}.
When we use the steepest-descent method we start at a point
and compute the gradient direction at that point.
Then we begin a %
\it straight-line %
\rm descent in that direction.
The gradient direction curves away from our direction of travel,
but we continue on our straight line
until we have stopped descending and are about to ascend.
There we stop, compute another gradient vector,
turn in that direction, and descend along a new straight line.
The process repeats until we get to the bottom,
or until we get tired.

\sideplot{yunyue}{width=3.6in}{
	Route of steepest descent (black)
	and route of conjugate direction (light grey or green).
	}

\par
What could be wrong with such a direct strategy?
The difficulty is at the stopping locations.
These occur where the descent direction
becomes %
\it parallel %
\rm to the contour lines.
(There the path becomes level.)
So after each stop, we turn $90^\circ$,
from parallel to perpendicular to the local contour line
for the next descent.
What if the final goal is at a $45^\circ$ angle to our path?
A $45^\circ$ turn cannot be made.
Instead of moving like a rain drop down the centerline of a rain gutter,
we move along a fine-toothed zigzag path,
crossing and recrossing the centerline.
The gentler the slope of the rain gutter,
the finer the teeth on the zigzag path.

\todo{Missing figure (ls-sawtooth) A search path for steepest descent.}

\subsection{Conjugate direction}
In the \bx{conjugate-direction method}, not a line, but rather a plane,
is searched.
A plane is made from an arbitrary linear combination of two vectors.
One vector will be chosen to be the gradient vector, say  $\bold g$.
The other vector will be chosen to be the previous descent step vector,
say  $\bold s = \bold x_j - \bold x_{j-1}$.
Instead of  $\alpha \, \bold g$  we need a linear combination,
say  $\alpha \bold g + \beta  \bold s$.
For minimizing quadratic functions the plane search requires
only the solution of a two-by-two set of linear equations
for  $\alpha$  and  $\beta$.
The equations will be specified here along with the program.
(For %
\it nonquadratic %
\rm functions a plane search is considered intractable,
whereas a line search proceeds by bisection.)
\par
For use in linear problems,
the conjugate-direction method described in this book
follows an identical path with the more well-known conjugate-gradient method.
We use the conjugate-direction method
for convenience in exposition and programming.

\par
%\boxit{
        The simple form of the conjugate-direction algorithm covered here
        is a sequence of steps.
        In each step the minimum is found in the plane given by two vectors:
        the gradient vector and the vector of the previous step.
%        }

%\begin{comment}
%\subsection{Magic (abandoned)}
%Some properties of the conjugate-gradient and the conjugate-direction approach
%are well known but hard to explain.
%D. G. \bx{Luenberger}'s book,
%{\it Introduction to Linear and Nonlinear Programming},
%is a good place to find formal explanations of this magic.
%(His book also provides other forms of these algorithms.)
%Another helpful book is \bx{Strang}'s {\it Introduction to Applied Mathematics}.
%Known properties follow:
%\begin{itemize}
%\item[{1.}]
%The \bx{conjugate-gradient method} and the
%\bx{conjugate-direction method}
%get the exact answer
%(assuming exact arithmetic) in  $n$  descent steps (or less),
%where  $n$  is the number of unknowns.
%\item[{2.}]
%It is helpful to use the previous step,
%so you might wonder why not use the previous two steps,
%because it is not hard to solve a three-by-three set
%of simultaneous linear equations.
%It turns out that the third direction does not help:
%the distance moved in the extra direction is zero.
%\end{itemize}
%\end{comment}

%\subsection{Magical properties of conjugate directions (Sergey)}
Given the linear operator $\bold F$ and a generator of solution steps
(random in the case of random directions or gradient in the case of
steepest descent),
we can construct an optimally convergent iteration process,
which finds the solution in no more than $n$ steps,
where $n$ is the size of the problem.
This result should not be surprising.
If $\bold F$ is represented by a full matrix,
then the cost of direct inversion is proportional to $n^3$,
and the cost of matrix multiplication is $n^2$.
Each step of an iterative method boils down to a matrix multiplication.
Therefore, we need at least $n$ steps to arrive at the exact solution.
Two circumstances make large-scale optimization practical.
First, for sparse convolution matrices
the cost of matrix multiplication is $n$ instead of $n^2$.
Second, we can often find a reasonably good solution
after a limited number of iterations.
If both these conditions are met, the cost of optimization
grows linearly with $n$,
which is a practical rate even for very large applications.



%\subsection{Conjugate-direction theory for programmers}
Fourier-transformed variables are often capitalized.
This convention will be helpful here,
so in this subsection only,
we capitalize vectors transformed by the  $\bold F$  matrix.
As everywhere, a matrix such as $\bold F$
is printed in {\bf boldface} type
but in this subsection,
vectors are {\it not} printed in boldface print.
Thus we define the solution, the solution step
(from one iteration to the next),
and the gradient by
\begin{eqnarray}
X   &=& \bold F \  x\   \quad\quad\quad  {\rm solution}         \\
S_j &=& \bold F \  s_j  \quad\quad\quad  {\rm solution\ step}    \\
G_j &=& \bold F \  g_j  \quad\quad\quad  {\rm solution\ gradient}  
\end{eqnarray}
A linear combination in solution space,
say  $s+g$,  corresponds to  $S+G$  in the conjugate space,
because $S+G = \bold F s + \bold F g = \bold F(s+g)$.
According to equation 
(\ref{eqn:cg1b}),
the residual is the theoretical data minus the observed data.
\begin{equation}
R \eq \bold F  x \ -\ D
  \eq          X \ -\ D
\end{equation}
The solution  $x$  is obtained by a succession of steps  $s_j$, say
\begin{equation}
x \eq s_1 \ +\  s_2 \ +\  s_3 \ +\  \cdots
\end{equation}
The last stage of each iteration is to update the solution and the residual:
\begin{eqnarray}
\label{eqn:xupdate}
{\rm solution\ update:} \quad\quad\quad  & x \ \leftarrow  x&  +\  s\\
\label{eqn:Rupdate}
{\rm residual\ update:} \quad\quad\quad  & R \ \leftarrow  R&  +\  S
\end{eqnarray}

\par
The {\it gradient} vector $g$ is a vector with the same number
of components as the solution vector $x$.
A vector with this number of components is
\begin{eqnarray}
g &=& \bold F\T \  R \eq \hbox{gradient}                 \label{eqn:g} \\
G &=& \bold F  \  g \eq \hbox{conjugate gradient}       \label{eqn:G}
\end{eqnarray}
The gradient $g$ in the transformed space is $G$,
also known as the \bx{conjugate gradient}.
\par
What will our solution update $\Delta \bold x=\bold s$ be?
It will be some unknown amount $\alpha$ of the gradient $\bold g$ plus
another unknown amount         $\beta$  of the previous step $\bold s$.
Likewise in residual space.
\begin{eqnarray}
\Delta \bold x = \alpha \bold g + \beta \bold s
\\
\Delta \bold r = \alpha \bold G + \beta \bold S
\end{eqnarray}

\par
The minimization (\ref{eqn:mindot}) is now generalized
to scan not only in a line with $\alpha$,
but simultaneously another line with $\beta$.
The combination of the two lines is a plane:
\begin{equation}
Q(\alpha ,\beta ) \eq
( R + \alpha G + \beta S) \ \cdot\  (R + \alpha G + \beta S )
\label{eqn:cgqf}
\end{equation}
The minimum is found at  $\partial Q / \partial \alpha \,=\,0$  and
$\partial Q / \partial \beta \,=\,0$, namely,
\begin{equation}
0\eq G \ \cdot\  ( R + \alpha G + \beta S )
\end{equation}
\begin{equation}
0\eq S \ \cdot\  ( R + \alpha G + \beta S )
\end{equation}
The solution is
\begin{equation}
\label{eqn:twobytwosln}
\left[ 
\begin{array}{c}
  \alpha \\ 
  \beta \end{array} \right] 
\ = \ 
        {-1 \over (G\cdot G)(S\cdot S)-(G\cdot S)^2}
\left[ 
\begin{array}{rr}
  (S \cdot S) & -(S \cdot G)  \\
  -(G \cdot S) & (G \cdot G)  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  (G\cdot R) \\
  (S\cdot R) \end{array} \right]
\end{equation}

\par
The many applications in this book all need to
find $\alpha$ and $\beta$ with (\ref{eqn:twobytwosln}) and then
update the solution with (\ref{eqn:xupdate}) and
update the residual with (\ref{eqn:Rupdate}).
Thus we package these activities in a subroutine
named \texttt{cgstep()}.
To use that subroutine we will have a computation \bx{template}
like we had for steepest descents, except that we
will have the repetitive work done by subroutine {\tt cgstep()}.
This template (or pseudocode) for minimizing the residual
$\bold 0\approx \bold r = \bold F \bold x - \bold d$
by the conjugate-direction method is
\label{lsq/'cgtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$                \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow \bold F\T\         \bold r$      \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \  \Delta \bold x$      \\
\>      \>  $(\bold x,\bold r) \padarrow {\rm cgstep}
             (\bold x,\bold r, \Delta\bold x,\Delta\bold r )$
        \\
\>      \> \}                                           
\end{tabbing}
where
the subroutine {\tt cgstep()}
remembers the previous iteration and
works out the step size and adds in
the proper proportion of the $\Delta \bold x$ of
the previous step.

%\par
%Most of the least-squares solver subroutines in this book
%follow the above \bx{template}.
%They may look less complicated when they start from $\bold x=\bold 0$
%or more complicated when $\bold F$ has several parts.
%$\bold F$ is often a partitioned matrix of operators
%and the code for applying it
%will have subtle differences from the code
%for applying its adjoint $\bold F\T$.
%Often we fit two expressions simultaneously,
%$\bold 0\approx \bold L\bold x-\bold d$ and
%$\bold 0\approx \bold A\bold x$, and then
%$\bold F$
%is the column matrix
%\begin{equation}
%\bold F \eq
%        \left[
%        \begin{array}{c}
%        \bold L \\
%        \bold A
%        \end{array}
%        \right]
%\end{equation}

%\begin{notforlecture}
\subsection{Routine for one step of conjugate-direction descent}
\par
\begin{comment}
The \bx{conjugate-direction program}
can be divided into two parts:
an inner part that is used almost without change
over a wide variety of applications,
and an outer part containing memory allocations,
operator invocations, and initializations.

Because \bx{Fortran} does not recognize the difference between upper- and
lower-case letters,
\end{comment}
the conjugate vectors $G$ and $S$ in the program are denoted by
{\tt gg} and {\tt ss}.
The inner part of the conjugate-direction task is in
function {\tt cgstep()}.%
%\moddex[f90]{cgstep}{one step of CD}
\moddex{cgstep}{one step of CD}{51}{86}{api/c}
\par
Observe the \texttt{cgstep()} function has a logical parameter
called \texttt{forget}.
This parameter does not need to be input.
In the normal course of things, \texttt{forget} will be true
on the first iteration and false on subsequent iterations.
This refers to the fact that on the first iteration,
there is no previous step,
so the conjugate direction method
is reduced to the steepest descent method.
At any iteration, however, you have the option to set
\texttt{forget=true}
which amounts to restarting the calculation
from the current location,
something we rarely find reason to do.


\subsection{A basic solver program}
There are many different methods for iterative least-square estimation
some of which will be discussed later in this book.
The conjugate-gradient (CG) family
(including the first order conjugate-direction method described above)
share the property that theoretically they achieve the solution
in $n$ iterations, where $n$ is the number of unknowns.
The various CG methods differ
in their numerical errors,
memory required,
adaptability to non-linear optimization,
and their requirements on accuracy of the adjoint.
What we do in this section is to show you the generic interface.
\par
None of us is an expert in both geophysics and in optimization theory (OT),
yet we need to handle both.
We would like to have each group write its own code with
a relatively easy interface.
The problem is that the OT codes must invoke the physical operators
yet the OT codes should not
need to deal with all the data and parameters needed by the physical operators.
\par
In other words,
if a practitioner decides to
swap one solver for another,
the only thing needed is the name of the new solver.
\par
The operator entrance is for the geophysicist,
who formulates the estimation application.
The solver entrance is for the specialist in numerical algebra,
who designs a new optimization method.
The C programming language allows us
to achieve this design goal by means of generic function interfaces.
\par
A basic solver is \texttt{tinysolver}. It is simplified substantially from the library version,
which has a much longer list of optional arguments. (The \texttt{forget} parameter
is not needed by the solvers we discuss first.)

\moddex{tinysolver}{tiny solver}{23}{58}{api/c}

\par
The two most important arguments in \texttt{tinysolver()}
are the operator function \texttt{Fop},
which is defined by the interface from Chapter \ref{paper:ajt},
and the stepper function \texttt{stepper},
which implements one step of an iterative estimation.
For example, a practitioner who choses to use our new
\texttt{cgstep()} \vpageref{lst:cgstep}
for iterative solving the operator
\texttt{matmult} \vpageref{lst:matmult}
would write the call
\par
\texttt{tinysolver ( matmult\_lop, cgstep, ...}
\par
The other required parameters to \texttt{tinysolver()} 
are \texttt{dat} (the data we want to fit),
\texttt{x} (the model we want to estimate),
and \texttt{niter} (the maximum number of iterations).
There is also a couple of optional arguments.
For example, \texttt{x0} is the starting guess for the model.
If this parameter is omitted, the model is initialized to zero.
To output the final residual vector,
we include a parameter called \texttt{res},
which is optional as well.
We will watch how the list of optional parameters
to the generic solver routine grows
as we attack more and more complex applications in later chapters.

\subsection{The modeling success and the solver success}
Every time we run a data modeling program
we have access to two publishable numbers
$1-|\bold r|/|\bold d|$ and
$1-|\bold F\T\bold r|/|\bold F\T\bold d|$.
The first says how well the model fits the data.
The second says how well we did the job of finding out.

\par
Define the residual $\bold r=\bold F\bold m-\bold d$ and
the ``size'' of any vector, such as the data vector,
as $|\bold d|=\sqrt{\bold d \cdot \bold d}$.
The number $1-|\bold r|/|\bold d|$, will be called the
``modeling success at fitting data.''
(When the data fitting includes a residual weighting function,
it should be incorporated in $\bold F$ and $\bold d$.)

\par
While the modeling success is of interest to everyone, the second number
$1-|\bold F\T\bold r|/|\bold F\T\bold d|$,
to be called the ``solver success at achieving goals,''
is more of interest to us, the analysts.
It tells us to what extent our program
has achieved our stated goals
of data fitting (and regularization).
Experience seeing this number may give us guidance
to where opportunities have been missed
and where more work might be worthwhile.
In applications of low dimensionality
we can normally drive the solver success to unity.
Conjugate-gradient theory says given infinite precision arithmetic,
iteration should converge to the exact solution in a number of
iterations equal to the number of unknowns.
In reality, however,
in Geophysics we often find ourselves iterating on applications
far too large to run to completion.
The solver success number tells us how well we are doing.
\par

There are three ways to understand the important expression $\bold F\T\bold r$.
First, it is the residual $\bold r$, 
originally in data space, transformed to model space by $\bold F\T$.
The second is that $\bold F\T \bold r$
is the gradient of the penalty function,
namely $d(\bold r \cdot \bold r)/d\bold m$.
This gradient vanishes as the best fit is found.
The statement that this gradient vanishes is the ``normal equations''
\begin{equation}
 \bold 0 \eq
 \bold F\T \bold r 
 \eq  \bold F\T(\bold F\bold m-\bold d) 
 \eq (\bold F\T\bold F) \bold m - \bold F\T\bold d
\end{equation}
The third way to understand the gradient $\bold F\T \bold r$
is that it is called $\Delta \bold m$ in many solver programs
because of its role in building an update for the model $\bold m$.

\par
The progress of a solver can be observed by a plot of 
$|\Delta\bold m| = \sqrt{\Delta\bold m \cdot \Delta\bold m }$ versus iteration.
Ideally it converges to zero.
Unfortunately the amplitude axis of this plot
scales with the units of model space.
We should nondimensionalize it.
Then we could make a simple statements like,
``We iterated 90\% of the way to the solution.''

\par
Starting from a zero model $\bold m=\bold 0$
the first residual is $\bold r = -\bold d$.
Its size in model space is $|\bold F\T\bold d|$
(which also happens to be our first estimated model).
After many iterations the residual in model space is
$|\bold F\T\bold r|$.
The ratio
$|\bold F\T\bold r|/|\bold F\T\bold d|$
progresses towards zero as we progress.
The degree of success our solver is having is
$1-|\bold F\T\bold r|/|\bold F\T\bold d|$.
We could say,
``Starting from our first model, we iterated
until we had gone
$100\times (1- |\bold F\T\bold r|/\bold F\T\bold d|)$\%
of the way to the ultimate model $\bold m$.''
More simply,
$1- |\bold F\T\bold r|/\bold F\T\bold d|$ is the ``solver success.''


%\par
%For almost two decades
%I have been iterating to a limit \texttt{niter} set by experience.
%This page sets the ground for a more rational limit.

\par
\boxit{
The ``modeling success at fitting data'' is the number
$1-|\bold r|/|\bold d|$.
The ``solver success at achieving goals'' is the number
$1-|\bold F\T\bold r|/|\bold F\T\bold d|$.
}
\par

\subsection{Measuring success}
In image estimation we have data $\bold d$, residual $\bold r$, operator $\bold F$,
and final solution, the image $\bold m$.
We minimize the norm of $\bold r(\bold m) = \bold F\bold m-\bold d$
by variation of $\bold m$.
With least squares, minimizing the norm squared is equivalent to minimizing the norm.
We measure the success of the model fitting the data by
\begin{equation}
\label{eqn:datafit}
{\rm Fitting~success} = \ 1 - |\bold r|/|\bold d|
\end{equation}
where the norm of any vector, say $\bold r$, is $|\bold r|=\sqrt{\bold r \cdot \bold r}$.
\par
Since with image estimation applications
the number of unknowns (dimension of $\bold m$) is usually hopelessly large,
we can never iterate long enough to actually solve
the normal equations $\bold F\T \bold r=\bold 0$
(which is the same as iterating until the gradient $\bold F\T\bold r$ vanishes)
so we are obliged to report this fact.  I advocate this:
\begin{equation}
\label{eqn:modelfit}
{\rm Computational~success} = \ 1 - |\bold F\T\bold r|/|\bold F\T\bold d|
\end{equation}
There are three ways to think about this:
First $\bold F\T\bold r$ is the gradient which should vanish.
Second, it is the data fitting residual transformed into model space.
Third, $\bold F\T\bold r$ is the final $\Delta \bold m$, while
$\bold F\T\bold d$ is the first estimated model $\bold m_1$ (often called the adjoint model).
Although it seems like the gradient should diminish monotonically in size
(it certainly would in one dimension), I am not able to prove it here.

\subsection{Roundoff}

Surprisingly, as a matter of practice, the simple conjugate-direction method
defined in this book is more reliable than the conjugate-gradient method
defined in the formal professional literature.
I know this sounds unlikely, but I'll tell you why.

\par
In large applications numerical roundoff can be a problem. 
Calculations need to be done in higher precision.
The conjugate gradient method depends on you to supply an operator
whose adjoint is correctly computed.
Any roundoff in computing the operator should somehow be matched
by the roundoff in the adjoint.   This is unrealistic.
Thus optimization may diverge while theoretically converging.
The conjugate direction method doesn't mind the roundoff.
It simply takes longer to converge.

\par
Let us see an example of a situation where roundoff becomes a problem.
Suppose we add 100 million ones.  You expect the sum to be 100 million.
I got a sum of 16.7 million.
Why is this?
After the sum gets to 16.7 million adding a one to it adds nothing.
The extra 1.0 disappears in single precision roundoff.
\par
\begin{verbatim}
        real function one(sum);  one=1.;  return; end
        integer i;   real sum
        do i=1, 100000000
                sum = sum + one(sum)
        write (0,*) sum;    stop;  end
	1.6777216E+07
\end{verbatim}
\par
The code above must be a little more complicated than I had hoped
because modern compilers are so clever.
When told to add all the values in a vector they know
it is wise to add the numbers in groups, and then add the groups.
Thus I had to hide the fact I was adding ones by getting them
from a subroutine that seems to depend upon the sum (but really doesn't).


\begin{comment}
\subsection{Why C is much better than Fortran 77}
I'd like to digress from our geophysics-mathematics themes
to explain why C has been a great step forward
over Fortran 77.
All the illustrations in this book were originally computed in F77.
Then module
\texttt{tinysolver} \vpageref{lst:tinysolver}
was simply a subroutine.
It was not one module for the whole book, as it is now,
but it was many conceptually identical subroutines,
dozens of them, one subroutine for each application.
The reason for the proliferation was that F77 lacks the ability of C
to represent operators as having two ways to enter,
one for science and another for math.
On the other hand, F77 did not require the half a page
of definitions that we see here in C.
But the definitions are not difficult to understand,
and they are a clutter that we must see once and never again.
Another benefit is that the book in F77 had no easy way to switch
from the \texttt{cgstep} solver to other solvers.
\end{comment}

\subsection{Test case: solving some simultaneous equations}
\par
Now we assemble a module \texttt{cgtest} for solving simultaneous equations.
Starting with the conjugate-direction module {\tt cgstep} 
\vpageref{lst:cgstep}
we insert the module \texttt{matmult} \vpageref{lst:matmult} as the linear operator.
\moddex{cgtest}{demonstrate CD}{23}{31}{user/pwd}

\par
Below shows the solution to $5 \times 4$ set of simultaneous equations.
Observe that the ``exact'' solution is obtained in the last step.
Because the data and answers are integers,
it is quick to check the result manually.
%\newpage
\par
\noindent
\footnotesize
\begin{verbatim}
d transpose
      3.00      3.00      5.00      7.00      9.00

F transpose
      1.00      1.00      1.00      1.00      1.00
      1.00      2.00      3.00      4.00      5.00
      1.00      0.00      1.00      0.00      1.00
      0.00      0.00      0.00      1.00      1.00

for iter = 0, 4
x    0.43457383  1.56124675  0.27362058  0.25752524
res -0.73055887  0.55706739  0.39193487 -0.06291389 -0.22804642
x    0.51313990  1.38677299  0.87905121  0.56870615
res -0.22103602  0.28668585  0.55251014 -0.37106210 -0.10523783
x    0.39144871  1.24044561  1.08974111  1.46199656
res -0.27836466 -0.12766013  0.20252672 -0.18477242  0.14541438
x    1.00001287  1.00004792  1.00000811  2.00000739
res  0.00006878  0.00010860  0.00016473  0.00021179  0.00026788
x    1.00000024  0.99999994  0.99999994  2.00000024
res -0.00000001 -0.00000001  0.00000001  0.00000002 -0.00000001
\end{verbatim}
\normalsize

\begin{exer}
\item
One way to remove a mean value $m$ from signal $s(t)= \bold s$
is with the fitting goal $\bold 0 \approx \bold s - m$.
What operator matrix is involved?
\item
What linear operator subroutine from Chapter \ref{paper:ajt}
can be used for finding the mean?
\item
How many CD iterations should be required to get the exact mean value?
\item
Write a mathematical expression for finding the mean by the CG method.
\end{exer}

%\end{notforlecture}

\section{INVERSE NMO STACK}
\inputdir{invstack}

\sx{NMO stack}
To illustrate an example of solving a huge set of simultaneous
equations without ever writing down the matrix of coefficients
we consider how
{\it \bx{back projection}} can be upgraded towards
{\it \bx{inversion}} in the application called \bx{moveout and stack}.
\sideplot{invstack}{width=3in}{
  Top is a model trace $\bold m$.
  Next are the synthetic data traces, $\bold d = \bold M \bold m$.
  Then, labeled {\tt niter=0} is the \protect\bx{stack},
  a result of processing by adjoint modeling.
  Increasing values of {\tt niter} show $\bold x$
  as a function of iteration count in the fitting goal
  $\bold d \approx \bold M \bold m$.
  (Carlos Cunha-Filho)
}
\par
The seismograms at the bottom of Figure~\ref{fig:invstack}
show the first four iterations of conjugate-direction inversion.
You see the original rectangle-shaped waveform returning
as the iterations proceed.
Notice also on the \bx{stack}
that the early and late events have unequal amplitudes,
but after enough iterations they are equal,
as they began.
Mathematically,
we can denote the top trace as the model $\bold m$,
the synthetic data signals as $\bold d = \bold M \bold m$,
and the stack as $\bold M\T \bold d$.
The conjugate-gradient algorithm optimizes the fitting goal
$\bold d \approx \bold M \bold x$ by variation of $\bold x$,
and the figure shows $\bold x$ converging to $\bold m$.
Because there are 256 unknowns in $\bold m$,
it is gratifying to see good convergence occurring
after the first four iterations.
The fitting is done by module {\tt invstack},
which is just like
\texttt{cgtest} \vpageref{lst:cgtest} except that the matrix-multiplication operator
\texttt{matmult} \vpageref{lst:matmult} has been replaced by
\texttt{imospray}. % \vpageref{lst:imospray}.
Studying the program,
you can deduce that,
except for a scale factor,
the output at {\tt niter=0} is identical to the stack $\bold M\T \bold d$.
All the signals in Figure~\ref{fig:invstack} are intrinsically the same scale.%
\moddex{invstack}{inversion stacking}{24}{34}{user/gee}
\par
This simple inversion is inexpensive.
Has anything been gained over conventional stack?
First,
though we used \bx{nearest-neighbor} interpolation,
we managed to preserve the spectrum of the input,
apparently all the way to the Nyquist frequency.
Second, we preserved the true amplitude scale
without ever bothering to think about
(1) dividing by the number of contributing traces,
(2) the amplitude effect of NMO stretch, or
(3) event truncation.
\par
With depth-dependent velocity,
wave fields become much more complex at wide offset.
NMO soon fails,
but wave-equation forward modeling
offers interesting opportunities for inversion.

\section{FLATTENING 3-D SEISMIC DATA}
\inputdir{lomask}

Here we follow the doctoral dissertation of Jesse Lomask.
In Figure~\ref{fig:TwoD} we have seen how to flatten 2-D seismic data.
The 3-D process is much more interesting. 
To see why,
consider this: Starting from the origin $(x , y ) = (0, 0)$
we move along the $x$-axis flattening until we come to $(10, 0)$.
From there we move along the $y$-axis flattening until we get to
$(x , y ) = (10, 10)$.
Move backwards on the $x$-axis to $(0, 10)$ flattening as you go.
Finally, return to the origin.
Along our journey around this square we have integrated
$p = d t /d x$ (and $d t /d y$) to find the total time shift.
Upon returning to the starting point,
we would like the total time shift to return to zero. 
Dealing with real data of less than perfect coherence this might not happen.
Old time seismologists would say, ``The survey lines don't tie.''
As we push to the limits of our knowledge (which we normally do)
this problem always arises.
We would like a solution that gives the best fit of all the data in a volume.
Given a volume of data $u (t , x , y )$ we seek
the best $\tau (x , y )$ such that
$w(t , x , y )= u (t-\tau (x , y ), x , y )$
is  flattened. Let's get it. 
\par
Here is an expression that on first sight seems to say nothing
\begin{equation}
\nabla \tau \eq 
\left[
\begin{array}{c}
\frac{\partial \tau}{\partial x} \\
\\
\frac{\partial \tau}{\partial y}
\end{array}
\right] \label{eqn:TauFields}
\end{equation}
Equation~(\ref{eqn:TauFields}) looks like a tautology,
a restatement of basic mathematical notation. 
This is so, however,
only if $\tau (x , y )$ is known and the derivatives are derived from it.
When $\tau (x , y )$ is not known but the partial derivatives are observed,
then we have two measurements at each $(x , y )$ location
for the one unknown $\tau$ at that location. 
The same is true at all locations,
so we write it as a regression, a residual ${\bf r}$ that we 
will work to get small to find a best fitting
$\tau (x , y )$ or maybe $\tau (x , y , t )$.
Let $d$ be the measurements in the vector in equation~(\ref{eqn:TauFields}),
the measurements throughout the $(t , x , y )$-volume.
Expressed as a regression equation~(\ref{eqn:TauFields}) becomes 
\begin{equation}
{\bf 0} \quad \approx\quad  {\bf r} \eq \nabla \tau \ -\  {\bf d}
\end{equation}

\plot{chev}{width=\textwidth}{
	Chevron data cube from the Gulf of Mexico.
	A salt dome (lower left corner in the top plane) has pushed upwards,
	dragging bedding planes (seen in the 
	bottom two orthogonal planes) along with it.}
Let us see how the coming 3-D illustrations were created.
First we need code for vector gradient with its adjoint,
negative vector divergence. Here it is: 
\moddex{igrad2}{igrad} {48}{60}{api/c}
In a kind of magic, all we need to fit our regression~(\ref{eqn:TauFields})
is to pass the {\tt igrad2} 
module to the Krylov subspace solver,
simple solver using {\tt cgstep},
but first we need 
to compute {\bf d} by calculating $dt/dx$ and $dt/dy$ between
all the mesh points. 
\begin{verbatim}
do iy=1,ny { # Calculate x-direction dips: px 
             call puck2d(dat(:,:,iy),coh_x,px,res_x,boxsz,nt,nx) 
           } 
do ix=1,nx { # Calculate y-direction dips: py 
             call puck2d(dat(:,ix,:),coh_y,py,res_y,boxsz,nt,ny) 
           } 
do it=1,nt { # Integrate dips: tau 
             call dipinteg(px(it,:,:),py(it,:,:),tau,niter,verb,nx,ny) 
           } 
\end{verbatim}
The code says first to initialize the gradient operator.
Convert the 2-D plane of $dt/dx$ to a vector.
Likewise $dt/dy$.
Concatenate these two vectors
into a single column vector {\bf d} like in
equation~(\ref{eqn:TauFields}).
Tell the simple solver to make its steps
with to use {\tt cgstep} with the linear operator {\tt igrad2}. 
%\moddex{dipinteg}{dipinteg}

\subsection{Gulf of Mexico Salt Piercement Example (Jesse Lomask) }
Figure~\ref{fig:chev} shows a 3-D seismic data cube from the Gulf of Mexico provided by 
Chevron. A volume of data cannot be displayed on the page of a book. The display 
here consists of three slices from the volume. Top is a $(t_0 , x , y )$ slice, also called a 
``time slice.'' Beneath it is a $(t , x , y_0 )$ slice; aside that is a $(t , x_0 , y )$ slice, depth slices 
in orthogonal directions. Intersections of the slices within the cube are shown by 
the heavy black lines on the faces of the cube. The circle in the lower right corner 
of the top slice is an eruption of salt (which, like ice, under high pressure will flow 
like a liquid). Inside the salt there are no reflections so the data should be ignored 
there. Outside the salt we see layers, simple horizons of sedimentary rock. As the 
salt has pushed upward it has dragged bedding planes upward with it.
Presuming the bedding to contain permeable sandstones and impermeable shales,
the pushed up bedding around the salt is a prospective oil trap.
The time slice in the top panel shows ancient river channels,
some large, some small, that are now deeply buried. 
These may also contain sand.
Being natural underground ``oil pipes'' they are of great interest.
To see these pipes as they approach the salt dome
we need a picture not at a constant $t$,
but at a constant $t -\tau (x , y )$. 

\par
Figure~\ref{fig:slicecomp22} shows a time slice of the original cube
and the flattened cube of Figure~\ref{fig:chev}.
The first thing to notice on the plane before flattening is that the panel 
drifts from dark to light in place to place.
This is because the horizontal layers are not fully horizontal.
Approaching the dome the change from dark to light and 
back again happens so rapidly that the dome appears surrounded by rings.
After flattening, the drift and rings disappear.
The reflection horizons are no longer cutting across the image.
Channels no longer drift off (above or below) the viewable time slice.
Carefully viewing the salt dome it seems smaller after flattening because the 
rings are replace by a bedding plane.
%

%
\sideplot{slicecomp22}{width=\textwidth}{
	Slices of constant time before and after flattening.
	Notice the rings surrounding the dome are gone giving
	the dome a reduced diameter. 
	(Ignore the inside of the dome.)}
%

\section{VESUVIUS PHASE UNWRAPPING}
\inputdir{vesuvio}
\sx{Vesuvius}
\sx{phase unwrapping}
\sx{phase}
Figure \ref{fig:vesuvio} shows
radar\footnote{
	Here we do not require knowledge of radar fundamentals.
	Common theory and practice is briefly surveyed in
	Reviews of Geophysics, Vol 36, No 4, November 1998,
	Radar Interferometry and its application to changes
	in the earth's surface, Didier Massonnet and Kurt Feigl.
	}
images of
Mt.~Vesuvius\footnote{
        A web search engine quickly finds you other views.
        }
in Italy.
These images are made from backscatter
signals $s_1(t)$ and $s_2(t)$,
recorded along two \bx{satellite orbit}s 800 km high and 54 m apart.
The signals are very high frequency
(the radar wavelength being 2.7 cm).
They were Fourier transformed
and one multiplied by the complex conjugate of the other,
getting the product $Z=S_1(\omega) \bar S_2(\omega)$.
The product's amplitude and phase are shown in Figure \ref{fig:vesuvio}.
Examining the data,
you can notice that where the signals are strongest (darkest on the left),
the phase (on the right)
is the most spatially consistent.
Pixel by pixel evaluation with the two frames in a movie program
shows that there are a few somewhat large local amplitudes
(clipped in Figure \ref{fig:vesuvio})
but because these generally have spatially consistent phase,
I would not describe the data as containing noise bursts.
\plot{vesuvio}{width=6in,height=3in}{
  Radar image of Mt. Vesuvius.
  Left is the amplitude $|Z(x,y)|$.
  Non-reflecting ocean in upper left corner.
  Right is the phase $\arctan( \Re Z(x,y), \Im Z(x,y)\;)$.
  (from Umberto Spagnolini)
}

\par
To reduce the time needed for analysis and printing,
I reduced the data size two different ways,
by decimation and by local averaging,
as shown in Figure \ref{fig:squeeze}.
The decimation was to about 1 part in 9 on each axis,
and the local averaging was done in $9\times 9$ windows
giving the same spatial resolution in each case.
The local averaging was done independently in the
plane of the real part and the plane of the imaginary part.
Putting them back together again showed that the phase
angle of the averaged data behaves much more consistently.
This adds evidence that the data is not troubled by noise bursts.
\plot{squeeze}{width=6in,height=3in}{
  Phase based on decimated data (left)
  and smoothed data (right).
}

\par
From Figures \ref{fig:vesuvio} and \ref{fig:squeeze}
we see that \bx{contour}s of constant phase
appear to be contours of constant altitude;
this conclusion leads us to suppose that a study of radar theory
would lead us to a relation like $Z(x,y)=e^{ih(x,y)}$
where $h(x,y)$ is altitude.
We non-radar-specialists often think of phase in
$e^{i\phi} = e^{i\omega t_0(x,y)}$
as being caused by some time delay, and
being defined for some constant frequency $\omega$.
Knowledge of this $\omega$ (as well as some angle parameters)
would define the physical units of $h(x,y)$.

Because the flat land away from the mountain is all at the same phase
(as is the altitude),
the distance as revealed by the phase does not represent
the distance from the ground to the satellite viewer.
We are accustomed to measuring altitude along a vertical line to a datum,
but here the distance seems to be measured
from the ground along a $23^\circ$ angle from the vertical
to a datum at the satellite height.

\par
Phase is a troublesome measurement because
we generally see it modulo $2\pi$.
Marching up the mountain we see the phase getting lighter and lighter
until it suddenly jumps to black which then continues to lighten
as we continue up the mountain to the next jump.
Let us undertake to compute the phase including
all of its jumps of $2\pi$.
Begin with a complex number $Z$ representing
the complex-valued image at any location
in the $(x,y)$-plane.
\begin{eqnarray}
r e^{i \phi}   &=& Z \\
\ln |r| + i \phi &=& \ln Z \\
\phi(x,y)            &=&  \Im \ln Z(x,y) ~+~  2\pi N(x,y)
\end{eqnarray}
A computer will find the imaginary part of the logarithm
with the arctan function of two arguments, {\tt atan2(y,x)},
which will put the phase in the range $-\pi < \phi \le \pi$
although any multiple of $2\pi$ could be added.
We seem to escape the $2\pi N$ phase ambiguity by differentiating:
\begin{eqnarray}
{\partial\phi \over \partial x}&=& \Im {1 \over Z}{\partial Z \over \partial x}\\
{\partial\phi \over \partial x}&=&
        {\Im  \bar Z {\partial Z \over \partial x} \over \bar Z Z }
\label{eqn:integrate1D}
\end{eqnarray}
For every point on the $y$-axis, equation (\ref{eqn:integrate1D})
is a differential equation on the $x$-axis,
and we could integrate them all to find $\phi(x,y)$.
That sounds easy.
On the other hand,
the same equations are valid when $x$ and $y$ are interchanged,
so we get twice as many equations as unknowns.
For ideal data, either of these sets of equations
should be equivalent to the other,
but for real data we expect to be fitting the fitting goal
\begin{equation}
\nabla \phi \quad \approx \quad {\Im  \bar Z \nabla Z \over \bar Z Z}
\label{eqn:integrateme}
\end{equation}
where
$\nabla = ({\partial \over \partial x}, {\partial \over \partial y} ) $.
This is essentially the same application we solved flattening seismic data with the regression 
$\nabla \tau \approx {\bf d}$. Taking measurements to be phase differences 
between neighboring mesh points, it is more correct to interpret equation~\ref{eqn:integrateme} as 
a difference equation than a differential equation. Since we measure phase differences only over tiny distances (one pixel) we hope not to worry about phases greater than $2\pi$.
But if such jumps do occur, they will contribute to overall error.
\par
Let us consider a typical location in the $(x , y )$ plane where the complex numbers
$Z_{i,j}$ are given. Define a shorthand $a , b, c$, and $d$ as follows: 

\par
%We will be handling the differential equation as a difference equation
%using an exact representation on the data mesh.
%By working with the phase difference of neighboring data values,
%we do not have to worry about phases greater than $2\pi$
%(except where phase jumps that much between mesh points).
%Thus we solve (\ref{eqn:integrateme})
%with finite differences instead of differentials.
%Module \texttt{igrad2} is a linear operator for the difference
%representation of the operator representing
%the gradient of a potential field.
%Its adjoint is known as the divergence of a vector field.
%\opdex{igrad2}{gradient in 2-D}
%To do the least-squares fitting
%(\ref{eqn:integrateme})
%we pass the \texttt{igrad2} module to the Krylov subspace solver.
%(Other people might prepare a matrix and give it to Matlab.)
%\par
%The difference equation representation of the fitting goal
%(\ref{eqn:integrateme}) is:
%\begin{equation}
%        \begin{array}{rcl}
%                \phi_{i+1,j} -\phi_{i,j} &\approx& \Delta\phi_{ac} \\
%                \phi_{i,j+1} -\phi_{i,j} &\approx& \Delta\phi_{ab}
%        \end{array}
%\label{eqn:diffgrad}
%\end{equation}
%where we still need to define the right-hand side.
%Define the parameters
%$a$,
%$b$,
%$c$, and
%$d$ as follows:
\begin{equation}
        \left[
                \begin{array}{ll}
                a & b \\
                c & d
                \end{array}
        \right]
        \eq
        \left[
                \begin{array}{ll}
                Z_{i,j}   & Z_{i,j+1} \\
                Z_{i+1,j} & Z_{i+1,j+1}
                \end{array}
        \right]
\end{equation}
With this shorthand, the difference equation representation of the fitting goal (\ref{eqn:integrateme}) is: 
\begin{equation}
        \begin{array}{rcl}
                \phi_{i+1,j} -\phi_{i,j} &\approx& \Delta\phi_{ac} \\
                \phi_{i,j+1} -\phi_{i,j} &\approx& \Delta\phi_{ab}
        \end{array}
\label{eqn:diffgrad}
\end{equation}
Now let us find the phase jumps between the various locations. Complex numbers $a$ and $b$ may be expressed in polar form, say $a=r_ae^{i\phi_a}$ and $b=r_be^{i\phi_b}$.
The complex number 
$\bar a b = r_a r_b e^{i(\phi_b-\phi_a)}$ has the desired phase
$\Delta \phi_{ab}$.
To obtain it we take the imaginary part of the complex logarithm
$\ln |r_a r_b| + i\Delta \phi_{ab}$.
\begin{equation}
  \begin{array}{lllll}
        \phi_b-\phi_a &=& \Delta \phi_{ab} &=& \Im \ln  \bar a b\\
        \phi_d-\phi_c &=& \Delta \phi_{cd} &=& \Im \ln  \bar c d\\
        \phi_c-\phi_a &=& \Delta \phi_{ac} &=& \Im \ln  \bar a c\\
        \phi_d-\phi_b &=& \Delta \phi_{bd} &=& \Im \ln  \bar b d
  \end{array}
\label{eqn:thedeltas}
\end{equation}
which gives the information needed to fill in the right-hand side of
(\ref{eqn:diffgrad}), as done by subroutine \texttt{igrad2init()} from
module \texttt{unwrap} \vpageref{lst:unwrap}.
% \progdex{igrad2init}{gradient 2-D init.}
\subsection{Estimating the inverse gradient}
To optimize the fitting goal (\ref{eqn:diffgrad}),
module \texttt{unwrap()} uses the conjugate-direction method
like the modules \texttt{cgmeth()}
%\vpageref{/prog:cgmeth}
and 
\texttt{invstack()}
%\vpageref{/prog:invstack}.
\moddex{unwrap}{Inverse 2-D gradient}{65}{67}{user/gee}
An open question is whether the required number of iterations is reasonable
or whether we would need to uncover a preconditioner
or more rapid solution method.
I adjusted the frame size 
(by the amount of smoothing in Figure \ref{fig:squeeze90})
so that I would get the solution in about ten seconds with 400 iterations.
Results are shown in Figure~\ref{fig:veshigh}.
\plot{veshigh}{width=6in,height=3in}{
        Estimated altitude.}

\begin{comment}
When we are working with figures like Figure~\ref{fig:veshigh},
the number of iterations often exceeds the number of intermediate
output frames that we care to deal with.
The computer function \texttt{klick()} is a simple tool
to detect logarithmically spaced intervals for taking snapshots
of iterative descent.
\progdex{klick}{Logarithmic increment detect}
\end{comment}

%\begin{notforlecture}
\subsection{Digression: curl grad as a measure of bad data}
\sx{curl grad}
The relation (\ref{eqn:thedeltas}) between the phases and the phase differences is
\begin{equation}
\left[
        \begin{array}{rrrr}
                -1 &  1&  0& 0 \\
                 0 &  0& -1& 1 \\
                -1 &  0&  1& 0 \\
                 0 & -1&  0& 1
        \end{array}
\right]
\left[
        \begin{array}{l}
                \phi_a \\
                \phi_b \\
                \phi_c \\
                \phi_d 
        \end{array}
\right]
\eq
\left[
        \begin{array}{l}
                \Delta \phi_{ab} \\
                \Delta \phi_{cd} \\
                \Delta \phi_{ac} \\
                \Delta \phi_{bd} 
        \end{array}
\right]
\label{eqn:fourbyfour}
\end{equation}
Starting from the phase differences,
equation (\ref{eqn:fourbyfour}) hope to find all the phases themselves
because an additive constant cannot be found.
In other words,
the column vector $[1,1,1,1]'$ is in the null space.
Likewise, if we add phase increments while we move around a loop,
the sum should be zero.
Let the loop be
$ a \rightarrow c \rightarrow d \rightarrow b \rightarrow a $.
The phase increments that sum to zero are:

\begin{equation}
  \Delta \phi_{ac} + \Delta \phi_{cd} - \Delta \phi_{bd} - \Delta \phi_{ab}
  \eq 0
\label{eqn:curly}
\end{equation}
Rearranging to agree with the order in equation (\ref{eqn:fourbyfour}) yields
\begin{equation}
  - \Delta \phi_{ab}
  + \Delta \phi_{cd}
  + \Delta \phi_{ac}
  - \Delta \phi_{bd}
  \eq 0
\end{equation}
which says that the row vector $[-1,+1,+1,-1]$
premultiplies (\ref{eqn:fourbyfour}),
yielding zero.
Rearrange again
\begin{equation}
  - \Delta \phi_{bd}
  + \Delta \phi_{ac}
  \eq 
    \Delta \phi_{ab}
  - \Delta \phi_{cd}
\end{equation}
and finally interchange signs and directions
(i.e., $\Delta \phi_{db} = -\Delta \phi_{bd}$)
\begin{equation}
    (\Delta \phi_{db} - \Delta \phi_{ca})
  \ -\ 
    (\Delta \phi_{dc} - \Delta \phi_{ba})
  \eq 0
\end{equation}
This is the finite-difference equivalent of
\begin{equation}
{\partial^2 \phi \over \partial x \partial y}
\ -\ 
{\partial^2 \phi \over \partial y \partial x}
\eq 0
\end{equation}
and is also
the $z$-component of the theorem that the curl of a gradient
$\nabla\times\nabla\phi$ vanishes for any $\phi$.
\par
The four $\Delta\phi$ summed around the $2\times 2$ mesh
should add to zero.
I wondered what would happen if random complex numbers
were used for $a$, $b$, $c$, and $d$,
so I computed the four $\Delta\phi$s with equation (\ref{eqn:thedeltas}),
and then computed the sum with (\ref{eqn:curly}).
They did sum to zero for 2/3 of my random numbers.
Otherwise,
with probability 1/6 each, they summed to $\pm2\pi$.
The nonvanishing curl represents a phase that is changing
too rapidly between the mesh points.
Figure \ref{fig:screw} shows the locations
at Vesuvius where bad data occurs.
This is shown at two different resolutions.
The figure shows a tendency for
bad points with curl $2\pi$ to have a neighbor with $-2\pi$.
If Vesuvius were random noise instead of good data,
the planes in Figure \ref{fig:screw} would be one-third covered with dots
but as expected, we see considerably fewer.
\plot{screw}{width=6in,height=3in}{ 
  Values of curl at Vesuvius.
  The bad data locations at both coarse and fine resolution
  tend to occur in pairs of opposite polarity.
}

\subsection{Discontinuity in the solution}
The viewing angle (23 degrees off vertical)
in Figure \ref{fig:vesuvio} might be such
that the mountain blocks some of the landscape behind it.
This leads to the interesting possibility
that the phase function must have a discontinuity
where our viewing angle jumps over the hidden terrain.
It will be interesting to discover whether
we can estimate functions with such discontinuities.
I am not certain that the Vesuvius data
really has such a shadow zone, so I prepared the synthetic
data in Figure \ref{fig:synmod},
which is noise free and definitely has one.
\par
We notice the polarity of the synthetic data in \ref{fig:synmod}
is opposite that of the Vesuvius data. 
This means that the radar altitude of Vesuvius is
not measured from sea level but from the satellite level.

\par
A reason I particularly like this Vesuvius exercise
is that slight variations on the theme occur in various other fields.
For example,
in 3-D seismology
we can take the cross-correlation of each seismogram
with its neighbor and pick the time lag of the maximum correlation.
Such time shifts from trace to trace
can be organized as we have organized the $\Delta\phi$ values of Vesuvius.
The discontinuity in phase along the skyline of our Vesuvius view
is like the faults we find in the earth.

\begin{exer}
\begin{comment}
\item
  
 At iterations determined by
 \texttt{klick()} \vpageref{lst:klick}
 make snapshots 
 {\tt call snap( 'resphz.H', n1,n2, phz)}
 of a residual phase
 {\tt phz(i1,i2) = aimag( clog( zr))}
 of a complex number that you compute by multiplying
 the raw data by the complex conjugate
 of the theoretical data
 {\tt zr = zz(i1,i2) * cexp( cmplx( 0., -hh(i1,i2)))}.
 Examine the movie with
 {\tt <resphz.H Byte | Ta2vplot | Tube}.
 Has the iteration converged?
 Does it look like the correct answer was obtained?  Why or why not?
\end{comment}
\item
In differential equations,
boundary conditions are often (1) a specified function value
or (2) a specified derivative.
These are associated with (1) transient convolution
or (2) internal convolution.
Gradient operator \texttt{igrad2} \vpageref{lst:igrad2}
is based on internal convolution with the filter $(1,-1)$.
Revise \texttt{igrad2} to make a module called {\tt tgrad2}
which has transient boundaries.
%Hint: Compare with
%\texttt{icai2} \vpageref{lst:icai2} and
%\texttt{tcai2} \vpageref{lst:tcai2}.
\end{exer}

\plot{synmod}{width=6in,height=1.5in}{ 
  Synthetic mountain with hidden backside.
  For your estimation enjoyment.
}

\subsection{Analytical solutions}
%
We have found a numerical solution to fitting applications such as this 
\begin{equation}
{\bf 0} \quad \approx \quad \nabla \tau \ - \ {\bf d}
\label{eqn:analyticsolution}
\end{equation}
An analytical solution will be much faster.
From any regression we get the least 
squares solution when we multiply by the transpose of the operator. Thus 
\begin{equation}
{\bf 0} \eq \nabla\T \nabla \tau \ -\ \nabla\T {\bf d}
\end{equation} 
We need to understand what is the transpose of the gradient operator.
Recall the finite difference representation of a derivative in chapter~1.
Ignoring end effects,
the transpose of a derivative is the negative of a derivative.
Since the transpose of a column vector is a row vector,
the adjoint of a gradient $\nabla$, namely,
$\nabla\T$ is more commonly known as the vector divergence 
($\nabla \cdot$).
Likewise $\nabla\T\nabla$ is a positive definite matrix,
the negative of the Laplacian $\nabla^2$.
Thus, in more conventional mathematical notation,
the solution $\tau$ is that of Poisson's equation. 
\begin{equation}
\nabla^2 \tau \eq - \ \nabla \cdot {\bf d} 
\label{eqn:PoissonEQ}
\end{equation} 
In the Fourier domain we can have an analytic solution.
There $-\nabla^2 = k_x^2 + k_y^2$ 
where $(k_x , k_y)$ are the Fourier frequencies on the $(x , y )$ axes.
Instead of thinking 
of equation~(\ref{eqn:PoissonEQ}) as a convolution in physical space,
think of it as a product in Fourier space.
Thus, the analytic solution is
\begin{equation}
\tau(x,y) \eq
{\bf FT}^{-1} \frac{ {\bf FT} \ \ \nabla \cdot {\bf d} } { k_x^2+k_y^2}
\end{equation}
where {\bf FT} denotes two-dimensional Fourier transform over $x$ and $y$ . 
Here is a trick from numerical analysis that gives better results: Instead of 
representing the denominator $k_x^2+k_y^2$ in the most obvious way, let us represent it 
in a manner consistent with the finite-difference way we expressed the numerator 
$\nabla \cdot {\bf d}$. Recall that $- i\omega \Delta t \approx i \hat{\omega} \Delta t = 1-Z = 1-{\rm exp}(-i \omega \Delta t)$ which is a Fourier 
domain way of saying that difference equations tend to differential equations at low 
frequencies. Likewise a symmetric second time derivative has a finite-difference 
representation proportional to $(-2+Z+1/Z)$ and in a two-dimensional space, a 
finite-difference representation of the Laplacian operator is proportional to $(-4+X+1/X+Y+1/Y)$ where $X = {\rm exp}(i k_x \Delta x )$ and $Y = {\rm exp}(i k_y \Delta y )$. 
Fourier solutions have their own peculiarities (periodic boundary conditions) 
which are not always appropriate in practice, but having these solutions available 
is often a nice place to start from when solving an application that cannot be solved 
in Fourier space.

\par
For example, suppose we feel some data values are bad and we
would like to throw out the regression equations involving the bad data points. At 
Vesuvius we might consider the strength of the radar return (which we have
previously ignored) and use it as a weighting function ${\bf W}$.
Now our regression~\label{eqn:analyticsolution} becomes 
\begin{equation}
{\bf 0} \quad \approx\quad
{\bf W} \ (\nabla \phi -{\bf d})
\eq ({\bf W} \nabla) \phi \ -\  {\bf Wd}
\end{equation}
This is a problem we know how to solve,
a regression with an operator ${\bf W \nabla}$  and 
data ${\bf Wd}$. The weighted problem is not solvable in the Fourier domain because 
the operator $({\bf W} \nabla )\T {\bf W} \nabla$ has no simple expression in the Fourier domain. Thus we would use the analytic solution to the unweighted problem as a starting guess for 
the iterative solution to the real problem. 
\par
With the Vesuvius data we might we construct the weight ${\bf W}$ from the signal 
strength. We also have available the curl, which should vanish. Its non-vanishing 
is an indicator of questionable data which could be weighted down relative to other 
data.

%\begin{notforlecture}
\section{OPERATOR SCALING (BINORMALIZATION)}
%
We can accept model $\bold m$ and data $\bold d$ as they arise from the geometry
or geophysics of an application,
or we can transform them to forms that are computationally convenient,
work with them, and finally transform back.
Say we transform them to new variables, $\bold u$ and $\bold v$.
\begin{eqnarray}
\bold u &=& \bold M \bold m
\\
\bold v &=& \bold D \bold d
\end{eqnarray}
These transformations change the $\bold F$ operator to $ \bold D \bold F \bold M^{-1} $ because
\begin{eqnarray}
\bold d &=& \bold F \bold m
\\
\bold D \bold d &=& \bold D \bold F \bold M^{-1} \bold M \bold m
\\
\bold v &=& \bold D \bold F \bold M^{-1} \bold u
\end{eqnarray}
The game is looking for the best
$\bold M$ and
$\bold D$.
\par
If I were able and willing to handle linear algebra in a modern way,
I would show you this matrix iteration
\begin{equation}
\lambda\ 
\left[
	\begin{array}{c}
	\bold d   \\
	\bold m 
	\end{array}
\right]_{i+1}
\eq
\left[
	\begin{array}{cc}
	\bold 0 & \bold F  \\
	\bold F\T & \bold 0
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold d   \\
	\bold m 
	\end{array}
\right]_i
\end{equation}
What is $\lambda$?  
It is a scale factor that you use to keep the vectors normalized
as the iteration proceeds.
You may recognize a path leading to eigenvectors, eigenvalues, Hermitian matrices,
and singular-value decomposition.
You'll need to find other sources to go further on that path because
it has not led me to a solution to the problem at hand
which is how to choose the best $\bold M$ and $\bold D$.
\par
If you have an operator that you are using millions of times 
it is worth seeking good choices.
Good choices are those
that make the adjoint of your new operator
$ \bold D \bold F \bold M^{-1}$
a good approximation to its inverse.
These are the two conditions we seek:
\begin{eqnarray}
\bold I &\approx &
(\bold D \bold F \bold M^{-1})\T \ 
(\bold D \bold F \bold M^{-1})
\\
\bold I &\approx &
(\bold D \bold F \bold M^{-1}) \ 
(\bold D \bold F \bold M^{-1})\T
\end{eqnarray}
If these were true, we could probe with any test vectors we wished
\begin{eqnarray}
\hat {\bold t}_m
&=&
(\bold M^{-1})\T \ (\rm something)\ \bold M^{-1}\ \bold t_m
\\
\hat {\bold t}_d
&=&
\bold D \ {(\rm something\ else)} \
\bold D\T \ \bold t_d
\end{eqnarray}
and find both $\hat {\bold t}_m\approx \bold t_m$
and  $\hat {\bold t}_d\approx \bold t_d$.
So, the game is to play with
$\bold M$ and $\bold D$
to try to get this to happen.
\par
About the only trick I know is to try $\bold M$ and $\bold D$ as diagonal matrices.
For test functions $\bold t$, I generally use a pattern of moderately spaced impulses.
In physical space we may see places where
$\hat {\bold t}$ is smaller than $\bold t$.
Those are the places to boost the corresponding diagonal.
\par
There are many test functions you could use.
You could use all ones.
You could use random numbers.
You could use a pile of random old data,
though I'm not sure what you would use for old models.
Take the output.
Take its absolute value.
Maybe smooth it.
Take the square root since the half you put in
$\bold F$ appears a second time in $\bold F\T$.
\par
I know one more trick.
In seismology many operators appear as integrals.
One of many such operators is called ``Kirchhoff migration''.
Because these operators and their adjoints contain integrations they boost low frequencies.
We can attenuate them back to their original size
by having $\bold M$ or $\bold D$ apply $\sqrt{-i\omega}$
(known in the time domain as the ``Hankel tail'').
%\par
%Sometimes Jacobians or determinents
%give guidance to finding the inverse and thereby deducing the diagonal matrices.
%That's a nice start, but practicalities often arise
%(such as data truncation, depth variable velocity)
%that keep the guessing game alive.
%
\par
What, may we ask is the interpretation of the $(\bold u, \bold v)$ variables?
They feel like ``energy conservation'' variables, though it makes no sense
to say the physical energy in $\bold m$ or $\bold d$ should be conserved
in the way of Parseval's theorem of Fourier transforms.
I imagined the $(\bold u,\bold v)$ variables might be especially suitable for display
(like preconditioned variables) but now I am less certain.

%\end{notforlecture}

\section{THE WORLD OF CONJUGATE GRADIENTS}

Nonlinearity arises in two ways:
First, theoretical data might be a nonlinear function of the model parameters.
Second, observed data could contain imperfections that force us to use
\bx{nonlinear methods} of statistical estimation.

\subsection{Physical nonlinearity}
When standard methods of physics
relate theoretical data $\bold d_{\rm theor}$ to model parameters $\bold m$,
they often use a nonlinear relation,
say $\bold d_{\rm theor} =\bold f(\bold m)$.
The power-series approach then leads to
representing theoretical data as
\begin{equation}
\bold d_{\rm theor} \eq
  \bold f(\bold m_0 + \Delta \bold m)
  \quad\approx\quad
  \bold f\bold (\bold m_0) + \bold F\Delta \bold m
\end{equation}
where $\bold F$ is the matrix of partial derivatives
of data values by model parameters,
say $\partial d_i/\partial m_j$,
evaluated at $\bold m_0$.
The theoretical data  $\bold d_{\rm theor}$ minus
the observed data $\bold d_{\rm obs}$ is the residual we minimize.
\begin{eqnarray}
\bold 0 \quad\approx\quad
 \bold d_{\rm theor} - \bold d_{\rm obs}
 &=& \bold F\bold \Delta\bold  m +[\bold f(\bold m_0) - \bold d_{\rm obs}] \\
\bold r_{\rm new}
 &=& \bold F\bold \Delta\bold  m + \bold r_{\rm old}
\label{eqn:resupdatenl}
\end{eqnarray}
It is worth noticing that the residual updating
(\ref{eqn:resupdatenl})
in a nonlinear application is the same
as that in a linear application (\ref{eqn:resupdatelin}).
If you make a large step $\Delta \bold m$, however,
the new residual
will be different from that expected by
(\ref{eqn:resupdatenl}).
Thus you should always re-evaluate the residual vector at the new location,
and if you are reasonably cautious,
you should be sure the residual norm has actually decreased
before you accept a large step.

\par
The pathway of inversion with physical nonlinearity
is well developed in the academic literature
and Bill \bx{Symes} at Rice University has a particularly active group.

\subsection{Statistical nonlinearity}
The data itself often has \bx{noise bursts} or \bx{gaps}, and we will
see later in Chapter \ref{paper:noiz} that this leads us to
readjusting the \bx{weighting function}.  In principle, we should fix
the weighting function and solve the problem.  Then we should revise
the weighting function and solve the problem again.  In practice we
find it convenient to change the weighting function during the
optimization descent.  Failure is possible when the weighting function
is changed too rapidly or drastically.  (The proper way to solve this
problem is with robust estimators.  Unfortunately, I do not yet have
an all-purpose robust solver.  Thus we are (temporarily, I hope)
reduced to using crude reweighted least-squares methods.  Sometimes
they work and sometimes they don't.)

\subsection{Coding nonlinear fitting problems}
We can solve nonlinear least-squares problems
in about the same way as we do iteratively reweighted ones.
A simple adaptation of a linear method gives us a \bx{nonlinear solver} if
the residual is recomputed at each iteration.
Omitting the weighting function (for simplicity) the \bx{template} is:
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> {\rm iterate \{ }                                                    \\
\>      \> $\bold r \padarrow \bold f( \bold m) - \bold d$       \\
\>      \> Define $\bold F=\partial \bold d/\partial \bold m$.       \\
\>      \>  $\Delta\bold m  \padarrow \bold F\T\         \bold r$ \\
\>      \>  $\Delta\bold r\ \padarrow \bold F \ \Delta \bold m$  \\
\>      \>  $(\bold m,\bold r) \padarrow {\rm step}
             (\bold m,\bold r, \Delta\bold m,\Delta\bold r )$ \\
\>      \> \}
\end{tabbing}

\par
A formal theory for the optimization exists,
but we are not using it here.
The assumption we make is that the step size will be small,
so that familiar line-search and plane-search approximations
will succeed in reducing the residual.
Unfortunately this assumption is not reliable.
What we should do is test that the residual really does decrease,
and if it does not we should revert
to steepest descent with a smaller step size.
Perhaps we should test an incremental variation on the status quo:
where inside \texttt{solver} \vpageref{lst:tinysolver},
we check to see if the residual
diminished in the {\it previous} step, and if it did not,
restart the iteration (choose the {\it current} step to be steepest descent instead of CD).
I am planning to work with some mathematicians
to gain experience with other solvers.

\par
Experience shows that nonlinear applications have many pitfalls.
Start with a linear problem,
add a minor physical improvement or unnormal noise,
and the problem becomes nonlinear and probably has another solution
far from anything reasonable.
When solving such a nonlinear problem,
we cannot arbitrarily begin from zero as we do with linear problems.
We must choose a reasonable starting guess,
and then move in a stable and controlled manner.
A simple solution is to begin with several steps of steepest descent
and then switch over to do some more steps of CD.
Avoiding CD in earlier iterations can avoid instability.
Strong linear ``regularization'' discussed later
can also reduce the effect of nonlinearity.

\subsection{Standard methods}
The conjugate-direction method is really a family of methods.
Mathematically, where there are $n$ unknowns, these algorithms all
converge to the answer in $n$ (or fewer) steps.  The various methods
differ in numerical accuracy, treatment of underdetermined systems,
accuracy in treating ill-conditioned systems, space requirements, and
numbers of dot products.  Technically, the method of CD used in the
\texttt{cgstep} module \vpageref{lst:cgstep} is not the
conjugate-gradient method itself, but is equivalent to it.  This
method is more properly called the \bx{conjugate-direction method}
with a memory of one step.  I chose this method for its clarity and
flexibility.  If you would like a free introduction and summary of
conjugate-gradient methods, I particularly recommend {\it An
  Introduction to Conjugate Gradient Method Without Agonizing Pain }
by Jonathon Shewchuk, which you can \htmladdnormallinkfoot{download}{%
http://www.cs.cmu.edu/afs/cs/project/quake/public/papers/painless-conjugate-gradient.ps%
}.

\par
I suggest you skip over the remainder of this section and return
after you have seen many examples and have developed some expertise,
and have some technical problems.
\par
The \bx{conjugate-gradient method} was introduced
by \bx{Hestenes} and \bx{Stiefel} in 1952.
To read the standard literature and relate it to this book,
you should first realize that when I write fitting goals like
\begin{eqnarray}
 0  &\approx&  \bold W( \bold F \bold m - \bold d ) \\
 0  &\approx&  \bold A \bold m,
\end{eqnarray}
they are equivalent to minimizing the quadratic form:
\begin{equation}
\bold  m:  \quad\quad
\min_{\bold m}  Q(\bold m) \eq
( \bold m\T\bold F\T - \bold d\T)\bold W\T\bold W
( \bold F \bold m  - \bold d)
\ +\ \bold m\T\bold A\T\bold A\bold m
\label{eqn:geoinvtheory}
\end{equation}
The optimization theory (OT) literature starts from a minimization of
\begin{equation}
 \bold x:  \quad\quad
 \min_{\bold x} Q(\bold x) \eq \bold x\T\bold H \bold x - \bold b\T \bold x
\label{eqn:optimtheory}
\end{equation}
To relate equation (\ref{eqn:geoinvtheory}) to (\ref{eqn:optimtheory})
we expand the parentheses in (\ref{eqn:geoinvtheory}) 
and abandon the constant term $\bold d\T\bold d$.
Then gather the quadratic term in $\bold m$ and the linear term in $\bold m$.
There are two terms linear in $\bold m$
that are transposes of each other.
They are scalars so they are equal.
Thus, to invoke ``standard methods,'' you take
your problem-formulation operators $\bold F$, $\bold W$, $\bold A$
and create two subroutines that apply:
\begin{eqnarray}
 \bold H   &=&  \bold F\T\bold W\T\bold W\bold F + \bold A\T\bold A  \\
 \bold b\T  &=&  2(\bold F\T\bold W\T\bold W\bold d)\T
\end{eqnarray}
The operators $\bold H$ and $\bold b\T$ operate on model space.
Standard procedures do not require their adjoints
because $\bold H$ is its own adjoint and $\bold b\T$
reduces model space to a scalar.
You can see that computing $\bold H$ and $\bold b\T$ requires
one temporary space the size of data space
(whereas \texttt{cgstep} requires two).
\par
When people have trouble with conjugate gradients or conjugate
directions, I always refer them to the \bx{Paige and Saunders
  algorithm} {\tt LSQR}.  Methods that form $\bold H$ explicitly or
implicitly (including both the standard literature and the book3
method) square the condition number, that is, they are twice as
susceptible to rounding error as is {\tt LSQR}. The Paige and
Saunders method is reviewed by Nolet in a geophysical context. 
%I include module \texttt{lsqr}
%\vpageref{/prog:lsqr}
%without explaining
%why it works. The interface is similar to \texttt{solver}
%\vpageref{/prog:smallsolver}.
%Note that the residual vector does not appear
%explicitly in the program and that we cannot start from a nonzero
%initial model.  \moddex{lsqr}{LSQR solver}

\begin{comment}
\subsection{Understanding CG magic and advanced methods}
This section includes Sergey Fomel's explanation on the ``magic''
convergence properties of the conjugate-direction methods. It also
presents a classic version of conjugate gradients, which can be found
in numerous books on least-square optimization.

The key idea for constructing an optimal iteration is to update the
solution at each step in the direction, composed by a linear
combination of the current direction and all previous solution steps.
To see why this is a helpful idea, let us consider first the method of
random directions. Substituting expression (\ref{eqn:alfa}) into
formula (\ref{eqn:mindot}), we see that the residual power
decreases at each step by
\begin{equation}
  \label{eqn:resdecr}
  \bold r \cdot \bold r -
  \bold r_{\rm new} \cdot \bold r_{\rm new} \eq
  \frac{(\bold r \cdot \Delta \bold r )^2}
  {( \Delta \bold r \cdot \Delta \bold r )}\;.
\end{equation}
To achieve a better convergence, we need to maximize the right hand
side of (\ref{eqn:resdecr}). Let us define a new solution step $\bold
s_{\rm new}$ as a combination of the current direction $\Delta \bold
x$ and the previous step $\bold s$, as follows:
\begin{equation}
  \label{eqn:snew}
  \bold s_{\rm new} \eq \Delta \bold x + \beta \bold s\;.
\end{equation}
The solution update is then defined as
\begin{equation}
\bold x_{\rm new} \eq \bold x+\alpha \bold s_{\rm new}\;.
\label{eqn:newx}
\end{equation}
The formula for $\alpha$ (\ref{eqn:alfa}) still holds, because we have
preserved in (\ref{eqn:newx}) the form of equation (\ref{eqn:oldx})
and just replaced $\Delta \bold x$ with $\bold s_{\rm new}$. In fact,
formula (\ref{eqn:alfa}) can be simplified a little bit. From
(\ref{eqn:newresperp}), we know that $\bold r_{\rm new}$ is orthogonal
to $\Delta \bold r = \bold F \bold s_{\rm new}$. Likewise, $\bold r$
should be orthogonal to $\bold F \bold s$ (recall that $\bold r$ was
$\bold r_{\rm new}$ and $\bold s$ was $\bold s_{\rm new}$ at the
previous iteration). We can conclude that
\begin{equation}
  \label{eqn:rdr}
  (\bold r \cdot \Delta \bold r ) \eq 
  (\bold r \cdot \bold F \bold s_{\rm new}) \eq
  (\bold r \cdot \bold F \Delta \bold x) + 
  \beta (\bold r \cdot \bold F \bold s) \eq
  (\bold r \cdot \bold F \Delta \bold x)\;.
\end{equation}
Comparing (\ref{eqn:rdr}) with (\ref{eqn:resdecr}), we can see that
adding a portion of the previous step to the current direction does
not change the value of the numerator in expression
(\ref{eqn:resdecr}). However, the value of the denominator can be
changed. Minimizing the denominator maximizes the residual increase at
each step and leads to a faster convergence. This is the denominator
minimization that constrains the value of the adjustable coefficient
$\beta$ in (\ref{eqn:snew}).
\par
The procedure for finding $\beta$ is completely analogous to the
derivation of formula (\ref{eqn:alfa}). We start with expanding the
dot product $(\Delta \bold r \cdot \Delta \bold r)$:
\begin{equation}
  \label{eqn:dotrexp}
  (\bold F \bold s_{\rm new} \cdot \bold F \bold s_{\rm new}) \eq
  \bold F \Delta \bold x \cdot \bold F \Delta \bold x +
  2 \beta (\bold F \Delta \bold x \cdot \bold F \bold s) +
  \beta^2\,\bold F \bold s \cdot \bold F \bold s\;.
\end{equation}
Differentiating with respect to $\beta$ and setting the derivative to
zero,
we find that
\begin{equation}
  \label{eqn:beta0}
  0 \eq 2 (\bold F \Delta \bold x + \beta \bold F \bold s) 
  \cdot \bold F \bold s\;.
\end{equation}
Equation (\ref{eqn:beta0}) states that the \emph{conjugate direction}
$\bold F \bold s_{\rm new}$ is orthogonal (perpendicular) to the
previous conjugate direction $\bold F \bold s$. It also defines the
value of $\beta$ as
\begin{equation}
  \label{eqn:beta}
  \beta \eq - \frac{ (\bold F \Delta \bold x \cdot \bold F \bold s )}
  {(\bold F \bold s \cdot \bold F \bold s )}\;.
\end{equation}
\par
Can we do even better? The positive quantity that we minimized in
(\ref{eqn:dotrexp}) decreased by
\begin{equation}
  \label{eqn:sdecr}
  \bold F \Delta \bold x \cdot \bold F \Delta \bold x -
  \bold F \bold s_{\rm new} \cdot \bold F \bold s_{\rm new} \eq
  \frac{ (\bold F \Delta \bold x \cdot \bold F \bold s )^2}
  {(\bold F \bold s \cdot \bold F \bold s )}
\end{equation}
Can we decrease it further by adding another previous step? In
general, the answer is positive, and it defines the method of
conjugate directions. I will state this result without a formal proof
(which uses the method of mathematical induction). 
\begin{itemize}
\item If the new step is
composed of the current direction and a combination of all the
previous steps:
\begin{equation}
  \label{eqn:sn}
  \bold s_n \eq \Delta \bold x_n + \sum_{i < n} \beta_i \bold s_i\;, 
\end{equation}
then the optimal convergence is achieved when
\begin{equation}
  \label{eqn:betai}
  \beta_i \eq - \frac{ (\bold F \Delta \bold x_n \cdot \bold F \bold s_i )}
  {(\bold F \bold s_i \cdot \bold F \bold s_i )}\;.
\end{equation}
\item The new conjugate direction is orthogonal to the previous ones:
  \begin{equation}
    \label{eqn:cdortho}
    (\bold F \bold s_n \cdot \bold F \bold s_i) \eq 0 
    \quad \mbox{for all} \quad i < n
  \end{equation}
\end{itemize}
\par
To see why this is an optimally convergent method, it is sufficient to
notice that vectors $\bold F \bold s_i$ form an orthogonal basis in
the data space. The vector from the current residual to the smallest
residual also belongs to that space. If the data size is $n$, then $n$
basis components (at most) are required to represent this vector, hence
no more then $n$ conjugate-direction steps are required to find the
solution.
\par
The computation template for the method of conjugate directions is
\label{'cdtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow {\rm random\ numbers}$          \\
\>      \>  $\bold s   \padarrow \Delta \bold x + 
\sum_{i < n} \beta_i \bold s_i \quad \mbox{\rm where} \quad 
\beta_i = - \frac{(\bold F \Delta \bold x \cdot \bold F \bold s_i )}
  {(\bold F \bold s_i \cdot \bold F \bold s_i )}$                       \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \bold  s$               \\
\>      \> $\alpha \padarrow
                -(       \bold r \cdot \Delta\bold r )/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\bold s$              \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}
\par
Let us derive this, as well as some useful related formulas.
\par
According to formula (\ref{eqn:newresperp}), the new residual $\bold
r_{\rm new}$ is orthogonal to the conjugate direction $\Delta \bold r
= \bold F \bold s_{\rm new}$. According to the orthogonality condition
(\ref{eqn:cdortho}), it is also orthogonal to all the previous
conjugate directions. Defining $\Delta \bold x$ equal to the gradient
$\bold F\T \bold r$ and applying the definition of the adjoint
operator, it is convenient to rewrite the orthogonality condition in
the form
  \begin{equation}
    \label{eqn:rnortho}
    0 \eq (\bold r_n \cdot \bold F \bold s_i) \eq 
    (\bold F\T \bold r_n \cdot \bold s_i) \eq
    (\Delta \bold x_{n+1} \cdot \bold s_i) 
    \quad \mbox{for all} \quad i \leq n
  \end{equation}
  According to formula (\ref{eqn:sn}), each solution step $\bold s_i$
  is just a linear combination of the gradient $\Delta \bold x_i$ and
  the previous solution steps. We deduce from formula
  (\ref{eqn:rnortho}) that
  \begin{equation}
    \label{eqn:cgortho}
    0 \eq (\Delta \bold x_n \cdot \bold s_i) \eq 
    (\Delta \bold x_n \cdot \Delta \bold x_i)
    \quad \mbox{for all} \quad i < n
  \end{equation}
  In other words, in the method of conjugate gradients, the current
  gradient direction is always orthogonal to all the previous
  directions. The iteration process constructs not only an orthogonal
  basis in the data space but also an orthogonal basis in the model
  space, composed of the gradient directions.
  
  Now let us take a closer look at formula (\ref{eqn:betai}). Note
  that $\bold F \bold s_i$ is simply related to the residual step at
  $i$-th iteration: 
  \begin{equation}
  \label{eqn:simple}
\bold F \bold s_i = \frac{\bold r_i - \bold
    r_{i-1}}{\alpha_i}\;.
  \end{equation}
  Substituting relationship (\ref{eqn:simple}) into formula
  (\ref{eqn:betai}) and applying again the definition of the adjoint
  operator, we obtain
\begin{equation}
  \label{eqn:betan}  
   \beta_i = 
  - \frac{ \bold F \Delta \bold x_n \cdot (\bold r_i - \bold r_{i-1})}
  {\alpha_i (\bold F \bold s_i \cdot \bold F \bold s_i )} =
  - \frac{\Delta \bold x_n \cdot \bold F\T (\bold r_i - \bold r_{i-1})}
  {\alpha_i (\bold F \bold s_i \cdot \bold F \bold s_i )} =
  - \frac{ \Delta \bold x_n \cdot (\Delta \bold x_{i+1} - \Delta \bold x_i)}
  {\alpha_i (\bold F \bold s_i \cdot \bold F \bold s_i )} 
\end{equation}
Since the gradients $\Delta \bold x_i$ are orthogonal to each other,
the dot product in the numerator is equal to zero unless $i = n-1$. It
means that only the immediately preceding step $\bold s_{n-1}$
contributes to the definition of the new solution direction $\bold
s_n$ in (\ref{eqn:sn}). This is precisely the property of the
conjugate gradient method we wanted to prove.
\par
To simplify formula (\ref{eqn:betan}), rewrite formula (\ref{eqn:alfa}) as
\begin{equation}
  \label{eqn:cgalfa}
  \alpha_i \eq - \frac 
  { (\bold r_{i-1} \cdot \bold F \Delta \bold x_i )}
  {( \bold F \bold s_i \cdot \bold F \bold s_i ) } \eq - \frac
  { (\bold F\T \bold r_{i-1} \cdot \Delta \bold x_i )}
  {( \bold F \bold s_i \cdot \bold F \bold s_i ) } \eq - \frac
  { (\Delta \bold x_i \cdot \Delta \bold x_i )}
  {( \bold F \bold s_i \cdot \bold F \bold s_i ) }
\end{equation}
Substituting (\ref{eqn:cgalfa}) into (\ref{eqn:betan}), we obtain 
\begin{equation}
  \label{eqn:cgbeta}  
   \beta = 
  - \frac{( \Delta \bold x_n \cdot \Delta \bold x_n)}
  {\alpha_{n-1} (\bold F \bold s_{n-1} \cdot \bold F \bold s_{n-1} )} =
  \frac{(\Delta \bold x_n \cdot \Delta \bold x_n)}
  {(\Delta \bold x_{n-1} \cdot \Delta \bold x_{n-1})}\;.
\end{equation}
\par
The computation template for the method of conjugate gradients is then
\label{'cgtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> $\beta \padarrow 0$ \\
\> {\rm iterate \{ }                                               \\
\>      \>  $\Delta \bold x   \padarrow \bold F\T \bold r$          \\
\>      \>  {\rm if not the first iteration} 
$\beta \padarrow \frac{ (\Delta \bold x \cdot \Delta \bold x )}
                            { \gamma}$                              \\
\>      \>  $\gamma \padarrow (\Delta \bold x \cdot \Delta \bold x )$ \\
\>      \>  $\bold s   \padarrow \Delta \bold x + \beta \bold s$      \\
\>      \>  $\Delta \bold r  \padarrow \bold F \bold  s$               \\
\>      \> $\alpha \padarrow
                - \gamma/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\bold s$              \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}
\end{comment}

%Module \texttt{conjgrad} \vpageref{lst:conjgrad} provides an
%implementation of this method. The interface is exactly similar to
%that of \texttt{cgstep} \vpageref{lst:cgstep}, therefore you can
%use \texttt{conjgrad} as an argument to \texttt{solver}
%\vpageref{lst:smallsolver}. 
%
%When the orthogonality of the gradients, (implied by the classical
%conjugate-gradient method) is not numerically assured, the
%\texttt{conjgrad} algorithm may loose its convergence properties. This
%problem does not exist in the algebraic derivations, but appears in
%practice because of numerical errors. A proper remedy is to
%orthogonalize each new gradient against previous ones. Naturally, this
%increases the cost and memory requirements of the method.
%
%\moddex{conjgrad}{one step of CG}

\section{REFERENCES}

%\reference{Gill, P.E., Murray, W., and Wright, M.H., 1981,
%        Practical optimization:  Academic Press.
%        }
\reference{Hestenes, M.R., and Stiefel, E., 1952,
        Methods of
        conjugate gradients for solving linear systems:
        J. Res. Natl. Bur. Stand., {\bf 49}, 409-436.
        }
%\reference{Luenberger, D.G., 1973,
%        Introduction to linear and nonlinear programming:
%        Addison-Wesley.
%        }
%\reference{Nolet, G., 1985,
%        Solving or resolving inadequate and noisy
%        tomographic systems:
%        J. Comp. Phys., {\bf 61}, 463-482.
%        }
\reference{Paige, C.C., and Saunders, M.A., 1982a,
        LSQR: an algorithm for sparse linear equations
        and sparse least squares:
        Assn. Comp. Mach. Trans. Mathematical Software,
        {\bf 8,} 43-71.
        }
\reference{Paige, C.C., and Saunders, M.A., 1982b,
        Algorithm 583, LSQR:
        sparse linear equations and least squares problems:
        Assn. Comp. Mach. Trans. Mathematical Software,
        {\bf 8,}  195-209.
        }
%\reference{Strang, G., 1986,
%        Introduction to applied mathematics:
%        Wellesley-Cambridge Press.
%        }

%\end{notforlecture}

\clearpage

