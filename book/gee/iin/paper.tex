% copyright (c) 1997 Jon Claerbout

\title{Empty bins and inverse interpolation}
\author{Jon Claerbout}
\maketitle

\label{paper:iin}
	
\begin{comment}
\par
Let us review the big picture.
In Chapter \ref{paper:ajt} we developed adjoints and
in Chapter \ref{paper:lsq} we developed inverse operators.
Logically, correct solutions come only through inversion.
Real life, however, seems nearly the opposite.
This is puzzling but intriguing.

\par
Every time you fill your car with gasoline,
it derives much more from the adjoint than from inversion.
I refer to the fact that ``practical seismic data processing''
relates much more to the use of adjoints than of inverses.
It has been widely known for about the last 15 years
that medical imaging and all basic image creation methods are like this.
It might seem that an easy path to fame and profit would
be to introduce the notion of inversion,
but it is not that easy.
Both cost and result quality enter the picture.

\par
First consider cost.
For simplicity, consider a data space with $N$ values
and a model (or image) space of the same size.
The computational cost of applying a dense adjoint
operator increases in direct proportion to the number
of elements in the matrix, in this case $N^2$.
To achieve the minimum discrepancy between theoretical data
and observed data (inversion) theoretically requires $N$ iterations
raising the cost to $N^3$.

\par
Consider an image of size $m\times m=N$.
Continuing, for simplicity, to assume a dense matrix of relations between
model and data,
the cost for the adjoint is $m^4$ whereas the cost for inversion is $m^6$.
We'll consider computational costs for the year 2000, but
noticing that costs go as the sixth power of the mesh size,
the overall situation will not change much in the foreseeable future.
Suppose you give a stiff workout to a powerful machine;
you take an hour to invert a $4096\times 4096$ matrix.
The solution, a vector of $4096$ components could
be laid into an image of size $64\times 64= 2^6\times 2^6 = 4096$.
Here is what we are looking at for costs:

\par
\begin{center}
\begin{tabular}{||r|r|r|r|r||}                                          \hline
adjoint cost &$(m\times m )^2$ & $(512\times 512)^2$ & $(2^9 2^9)^2$ & $2^{36}$
\\ \hline
inverse cost &$(m\times m )^3$ & $  (64\times 64)^3$ & $(2^6 2^6)^3$ & $2^{36}$
\\ \hline
\end{tabular}

\end{center}

\par\noindent
These numbers tell us that for applications with dense operators,
the biggest images that we are likely to see coming from inversion methods
are $64\times 64$ whereas those from adjoint methods are $512\times 512$.
For comparison, the retina of your eye is comparable to your computer
screen at $1000\times 1000$.
We might summarize by saying that while adjoint methods are less than perfect,
inverse methods are ``legally blind'' :-)
\par\noindent
\url{http://sepwww.stanford.edu/sep/jon/family/jos/gifmovie.html} holds a movie
blinking between Figures \ref{fig:512x512} and \ref{fig:64x64}.

\inputdir{.}
\sideplot{512x512}{height=3.5in,width=3.5in}{
  Jos greets Andrew, ``Welcome back Andrew''
  from the Peace Corps.
  At a resolution of $512\times 512$, this picture
  is about the same as the resolution
  as the paper it is printed on,
  or the same as your viewing screen,
  if you have scaled it to 50\%\ of screen size.
}
%\newslide

\sideplot{64x64}{height=3.5in,width=3.5in}{
  Jos greets Andrew, ``Welcome back Andrew'' again.
  At a resolution of $64\times 64$
  the pixels are clearly visible.
  From far the pictures are the same.
  From near, examine their glasses.
}
%\newslide

\par
This cost analysis is oversimplified in that most applications
do not require dense operators.
With sparse operators, the cost advantage of adjoints is even more
pronounced since for adjoints,
the cost savings of operator sparseness translate directly to
real cost savings.
The situation is less favorable and much more muddy for inversion.
The reason that Chapter 2 covers iterative methods
and neglects exact methods is that in practice
iterative methods are not run to their theoretical completion
but they run until we run out of patience.

\par
Cost is a big part of the story, but the story has many other parts.
Inversion, while being the only logical path to the best answer,
is a path littered with pitfalls.
The first pitfall is that the data is rarely able to 
determine a complete solution reliably.
Generally there are aspects of the image that are not learnable
from the data.
\end{comment}

\par
In this chapter we study the simplest, most transparent example
of data insufficiency.
Data exists at irregularly spaced positions in a plane.
We set up a cartesian mesh and we discover that some
of the bins contain no data points.
What then?

\section{MISSING DATA IN ONE DIMENSION}
\inputdir{miss1figs}
\par
A method for restoring \bx{missing data} is to ensure that the restored data,
after specified filtering, has minimum energy.
Specifying the filter chooses the interpolation philosophy.
Generally the filter is a \bx{roughening} filter.
\sx{filter ! roughening}
When a roughening filter goes off the end of smooth data,
it typically produces a big end transient.
Minimizing energy implies a choice for unknown data values
at the end, to minimize the transient.
We will examine five cases and then make some generalizations.
\par
\boxit{A method for restoring missing data
        is to ensure that the restored data,
        after specified filtering, 
        has \protect\bx{minimum energy}.}
\par
Let $u$ denote an unknown (missing) value.
The dataset on which the examples are based is
$(\cdots, u, u,$ 
$1, u, 2, 1, 2, u, u, \cdots )$.
Theoretically we could adjust the missing $u$ values (each different)
to minimize the energy in the unfiltered data.
Those adjusted values would obviously turn out to be all zeros.
The unfiltered data is data that has been filtered by
an impulse function.
To find the missing values
that minimize energy out of other filters,
we can use subroutine \texttt{mis1()} \vpageref{lst:mis1}.
Figure~\ref{fig:mlines}
shows interpolation of the dataset with $(1,-1)$ as a roughening filter.
The interpolated data matches the given data where they overlap.%
\sideplot{mlines}{width=2.2in}{
  Top is given data.
  Middle is given data with interpolated values.
  Missing values seem to be interpolated by straight lines.
  Bottom shows the filter $(1,-1)$,
  whose output has minimum energy.
}
\sideplot{mparab}{width=2.2in}{
  Top is the same input data as in Figure~\protect\ref{fig:mlines}.
  Middle is interpolated.
  Bottom shows the filter $(-1,2,-1)$.
  The missing data seems to be interpolated by parabolas.
}
\sideplot{mseis}{width=2.2in}{
  Top is the same input.
  Middle is interpolated.
  Bottom shows the filter $(1,-3,3,-1)$.
  The missing data is very smooth.
  It shoots upward high off the right end of the observations,
  apparently to match the data slope there.
}
%\ACTIVESIDEPLOT{msmo}{width=2.2in}{miss1figs}{
%       The filter \protect\linebreak[2]
%       $(-1,$\protect\linebreak[2]$-1,$\protect\linebreak[2]$4,$
%       \protect\linebreak[2]$-1,$\protect\linebreak[2]$-1)$ gives
%%linebreaks are kludgy way to fix an line breaking bug 
%       interpolations with stiff lines.  They resemble
%       the straight lines of Figure~\protect\FIG{mlines},
%       but they project through a cluster of given values
%       instead of projecting to the nearest given value.
%       Thus, this interpolation tolerates noise in the given data
%       better than the interpolation shown in
%       Figure~\protect\FIG{mseis}.
%       }
\sideplot{moscil}{width=2.2in}{
  Bottom shows the filter $(1,1)$.
  The interpolation is rough.
  Like the given data itself, the interpolation
  has much energy at the Nyquist frequency.
  But unlike the given data, it has little zero-frequency energy.
}
\par
Figures~\ref{fig:mlines}--\ref{fig:moscil}
illustrate that the rougher the filter,
the smoother the interpolated data,
and vice versa.
Let us switch our attention from the residual spectrum
to the residual itself.
The residual for Figure~\ref{fig:mlines}
is the {\it slope} of the signal
(because the filter $(1,-1)$ is a {\it first derivative}),
and the slope is constant (uniformly distributed) along the straight lines
where the least-squares procedure is choosing signal values.
So these examples confirm the idea
that the \bx{least-squares method} abhors large values
(because they are squared).
Thus, least squares tends to distribute residuals uniformly
in both time and frequency to the extent
allowed by the \bx{constraint}s.
\par
This idea helps us answer the question,
what is the best filter to use?
It suggests choosing
the filter to have an amplitude spectrum
that is inverse to the spectrum we want for the interpolated data.
A systematic approach is given in chapter \ref{paper:mda},
but I offer a simple subjective analysis here:
Looking at the data, we see that all points are positive.
It seems, therefore, that
the data is rich in low frequencies;
thus the filter should contain something like $(1,-1)$,
which vanishes at zero frequency.
Likewise, the data seems to contain Nyquist frequency,
so the filter should contain $(1,1)$.
The result of using the filter $(1,-1)\ast (1,1)=(1,0,-1)$
is shown in Figure~\ref{fig:mbest}.
This is my best subjective interpolation
based on the idea that the missing data should look like the given data.
The \bx{interpolation} and \bx{extrapolation}s are so good that
you can hardly guess which data values are given
and which are interpolated.
\sideplot{mbest}{width=2.2in}{
  Top is the same as in
  Figures~\protect\ref{fig:mlines} to \protect\ref{fig:moscil}.
  Middle is interpolated.
  Bottom shows the filter $(1,0,-1)$, which comes from
  the coefficients of $(1,-1)\ast (1,1)$.
  Both the given data and the interpolated data
  have significant energy at
  both zero and Nyquist frequencies.
}

\subsection{Missing-data program}
\sx{missing data}
Now let us see how Figures \ref{fig:mlines}-\ref{fig:mbest}
could have been calculated and how they were calculated.
They could have been calculated with matrices,
in which matrices were pulled apart according to subscripts of known
or missing data;
instead I computed them with operators,
and applied only operators and their adjoints.
First we inspect the matrix approach
because it is more conventional.

\subsubsection{Matrix approach to missing data}
\sx{missing data}
\par
Customarily, we have referred to data by the symbol $\bold d$.
Now that we are dividing the data space into two parts,
known and unknown (or missing),
we will refer to this complete space
as the model (or map) space $\bold m$.
\par
There are 15 data points in Figures \ref{fig:mlines}-\ref{fig:mbest}.
Of the 15, 4 are known and 11 are missing.
Denote the known by $k$ and the missing by $u$.
Then the sequence of missing and known
is $(u,u,u,u,k,u,k,k,k,u,u,u,u,u,u)$.
Because I cannot print $15\times 15$ matrices,
please allow me to describe instead a data space of 6 values
$(m_1, m_2, m_3, m_4, m_5, m_6)$ with known values only $m_2$ and $m_3$,
that is arranged like $(u,k,k,u,u,u)$.
\par
Our approach is to minimize the energy in the residual,
which is the filtered map (model) space.
We state the fitting goals
$\bold 0\approx \bold F\bold m$ as
\begin{equation} 
\left[ 
\begin{array}{c}
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0
  \end{array} \right] 
\quad \approx \quad
\bold r
\quad =\quad
\left[ 
\begin{array}{cccccc}
  a_1 & 0   & 0    & 0   & 0   & 0   \\
  a_2 & a_1 & 0    & 0   & 0   & 0   \\
  a_3 & a_2 & a_1  & 0   & 0   & 0   \\
  0   & a_3 & a_2  & a_1 & 0   & 0   \\
  0   & 0   & a_3  & a_2 & a_1 & 0   \\
  0   & 0   & 0    & a_3 & a_2 & a_1 \\
  0   & 0   & 0    & 0   & a_3 & a_2 \\
  0   & 0   & 0    & 0   & 0   & a_3 
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_1 \\ 
  m_2 \\ 
  m_3 \\ 
  m_4 \\ 
  m_5 \\ 
  m_6
  \end{array} \right]
\label{eqn:headache}
\end{equation}

We rearrange the above fitting goals,
bringing the columns multiplying known data values
($m_2$ and $m_3$) to the left,
getting $\bold y =-\bold F_k \bold m_k \approx \bold F_u \bold m_u$.
\begin{equation} 
\left[ 
\begin{array}{c}
  y_1 \\ 
  y_2 \\ 
  y_3 \\ 
  y_4 \\ 
  y_5 \\ 
  y_6 \\ 
  y_7 \\ 
  y_8
  \end{array} \right] 
\quad = \quad  -
\left[ 
\begin{array}{cc}
   0   & 0    \\
   a_1 & 0    \\
   a_2 & a_1  \\
   a_3 & a_2  \\
   0   & a_3  \\
   0   & 0    \\
   0   & 0    \\
   0   & 0    
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_2 \\ 
  m_3 
  \end{array} \right]
\quad \approx \quad
\left[ 
\begin{array}{cccc}
  a_1  & 0   & 0   & 0   \\
  a_2  & 0   & 0   & 0   \\
  a_3  & 0   & 0   & 0   \\
  0    & a_1 & 0   & 0   \\
  0    & a_2 & a_1 & 0   \\
  0    & a_3 & a_2 & a_1 \\
  0    & 0   & a_3 & a_2 \\
  0    & 0   & 0   & a_3 
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_1 \\ 
  m_4 \\ 
  m_5 \\ 
  m_6
  \end{array} \right]
\label{eqn:wristpain}
\end{equation}
This is the familiar form of an overdetermined system of equations
$\bold y \approx \bold F_u \bold m_u$
which we could solve
for $\bold m_u$
as illustrated earlier by conjugate directions,
or by a wide variety of well-known methods.

\par
The trouble with this matrix approach is that it is awkward to program
the partitioning of the operator into the known and missing parts,
particularly if the application of the operator uses arcane techniques,
such as those used by the fast--Fourier-transform operator
or various numerical approximations to differential or partial
differential operators that depend on regular data sampling.
Even for the modest convolution operator,
we already have a library of convolution programs
that handle a variety of end effects,
and it would be much nicer to use the library as it is
rather than recode it for all possible geometrical arrangements
of missing data values.
\par
Note:
Here I take the main goal to be the clarity of the code,
not the efficiency or accuracy of the solution.
So, if your application consumes too many resources,
and if you have many more known points than missing ones,
maybe you should fit
$\bold y \approx \bold F_u \bold m_u$
and ignore the suggestions below.

\subsubsection{Operator approach to missing data}
\sx{missing data}
\par
For the operator approach to the fitting goal
$ -\bold F_k \bold m_k \approx \bold F_u \bold m_u$
we rewrite it as
$ -\bold F_k \bold m_k \approx \bold F \bold J \bold m $ where

\begin{equation} 
-\bold F_k \bold m_k
\quad \approx \quad
\left[ 
\begin{array}{cccccc}
  a_1 & 0   & 0    & 0   & 0   & 0   \\
  a_2 & a_1 & 0    & 0   & 0   & 0   \\
  a_3 & a_2 & a_1  & 0   & 0   & 0   \\
  0   & a_3 & a_2  & a_1 & 0   & 0   \\
  0   & 0   & a_3  & a_2 & a_1 & 0   \\
  0   & 0   & 0    & a_3 & a_2 & a_1 \\
  0   & 0   & 0    & 0   & a_3 & a_2 \\
  0   & 0   & 0    & 0   & 0   & a_3 
  \end{array} \right] 
\;
\left[ 
\begin{array}{cccccc}
  1   & .   & .    & .   & .   & .   \\
  .   & 0   & .    & .   & .   & .   \\
  .   & .   & 0    & .   & .   & .   \\
  .   & .   & .    & 1   & .   & .   \\
  .   & .   & .    & .   & 1   & .   \\
  .   & .   & .    & .   & .   & 1  
  \end{array} \right] 
\;
\left[ 
\begin{array}{c}
  m_1 \\ 
  m_2 \\ 
  m_3 \\ 
  m_4 \\ 
  m_5 \\ 
  m_6
  \end{array} \right]
\quad =\quad \bold F \bold J \bold m
\label{eqn:migraine}
\end{equation}
Notice the introduction of the new diagonal matrix $\bold J$,
called a \bx{mask}ing matrix or a \bx{constraint-mask} matrix
because it multiplies
constrained variables by zero
leaving freely adjustable variables untouched.
Experience shows that a better name than ``mask matrix'' is
``\bx{selector} matrix''
because what comes out of it,
that which is selected,
is a less-confusing name for it than which is rejected.
With a selector matrix the whole data space seems freely adjustable,
both the missing data values and known values.
We see that the CD method does not change the known (constrained) values.
In general, we derive the fitting goal (\ref{eqn:migraine}) by
\begin{eqnarray}
 \label{eqn:misfitgoal}
 \bold 0 &\approx& \bold F \bold m \\
 \bold 0 &\approx& \bold F( \bold J + (\bold I-\bold J) ) \bold m \\
 \bold 0 &\approx& \bold F\bold J\bold m + \bold F(\bold I-\bold J)\bold m \\
 \label{eqn:misfitgoal2}
 \bold 0 &\approx& \bold F\bold J\bold m + \bold F\bold m_{\rm known} \\
                                         \bold 0 \quad\approx\quad
 \bold r &=&       \bold F\bold J\bold m + \bold r_0
\end{eqnarray}
As usual, we find a direction to go $\Delta \bold m$
by the gradient of the residual energy.
\begin{equation}
\Delta\bold m
\eq \frac{\partial }{\partial \bold m\T}\ \bold r\T\bold r
\eq \left( \frac{\partial }{\partial \bold m\T} \ \bold r\T \right) \bold r
\eq \left( \frac{\partial }{\partial \bold m\T} \
(\bold m\T\bold J\T\bold F\T + \bold r\T_0)                 \right) \bold r
\eq  \bold J\T \bold F\T \bold r
\label{eqn:fooo}
\end{equation}

\par
We begin the calculation with
the known data values where missing data values are replaced by zeros, namely
$(\bold I-\bold J)\bold m$.
Filter this data,
getting $\bold F(\bold I-\bold J)\bold m$,
and load it into the residual $\bold r_0$.
With this initialization completed,
we begin an iteration loop.
First we compute $\Delta m$ from equation (\ref{eqn:fooo}).
\begin{equation}
\Delta\bold m \quad\longleftarrow\quad \bold J\T \bold F\T \bold r
\end{equation}
$\bold F\T$ applies a {\it crosscorrelation} of the filter to the residual
and then $\bold J\T$ sets to zero any changes proposed to known data values.
Next, compute the change in residual $\Delta\bold r$
from the proposed change in the data $\Delta\bold m$.
\begin{equation}
\Delta\bold r \quad \longleftarrow \quad \bold F \bold J \Delta \bold m
\label{eqn:lastmistheory}
\end{equation}
This applies the filtering again.
Then use the method of steepest descent (or conjugate direction)
to choose the appropriate scaling (or inclusion of previous step)
of $\Delta \bold m$ and $\Delta \bold r$,
and update $\bold m$ and $\bold r$ accordingly
and iterate.

\begin{comment}
\par
I could have passed a new operator $\bold F \bold J$
into the old solver,
but found it worthwhile to write a new,
more powerful solver having built-in constraints.
To introduce the masking operator $\bold J$ into the \texttt{solver-smp}
subroutine \vpageref{lst:solver-tiny},
I introduce an optional operator \texttt{Jop},
which is initialized with a logical array of the model size.
 %The values of \texttt{known} are \texttt{.true.}~for the known data locations,
 %and \texttt{.false.}~for the unknown data.
Two lines in the \texttt{solver-tiny} module \vpageref{lst:solver-tiny}
\par\indent
\footnotesize
\begin{verbatim}
stat = Fop( T, F, g, rd)                          #  g = F' Rd
stat = Fop( F, F, g, gd)                          #  G = F  g
\end{verbatim}
\normalsize
\par\noindent
become three lines in the standard library module \texttt{solver\_smp}.
(We use a temporary array \texttt{tm} of the size of model space.)
$\Delta \bold m$ is {\tt g} and
$\Delta \bold r$ is {\tt gg}.
\par\noindent
\footnotesize
\begin{verbatim}
stat = Fop( T, F, g, rd)                                  # g = F' Rd
if ( present( Jop)) { tm=g;  stat= Jop( F, F, tm, g)      # g = J g
stat = Fop( F, F, g, gg)                                  # G = F g
\end{verbatim}
\normalsize

\par
The full code includes all the definitions we had earlier
in \texttt{solver-tiny} module \vpageref{lst:solver-tiny}.
Merging it with the above bits of code we have
the simple solver \texttt{solver-smp}.


%In Fortran77, I simply multiplied by $\bold J$.
%In Fortran90, subroutines can have optional arguments.
%The expression
%\texttt{if( present( known))} says, ``if the argument
%\texttt{known}, a logical array, is an argument of the call.''
%The expression,
%\texttt{where( known) dm = 0} means that each component 
%of the logical array \texttt{known} is examined;
%if the value of that component is \texttt{.true.}~then
%the corresponding component of $\Delta \bold m$ is set to zero.
%In other words, we are not going to change the given, known data values.


\moddex{solver-smp}{simple solver}

\par
There are two methods of invoking the solver.
Comment cards in the code indicate the slightly more verbose
method of solution which matches the theory presented in the book.
\end{comment}

\par
The subroutine to find missing data is \texttt{mis1()}.
It assumes that zero values in the input data
correspond to missing data locations.
It uses our convolution operator
\texttt{tcai1()} \vpageref{lst:tcai1}.
You can also check the Index for other
\bx{operators} and \bx{modules}.
%\progdex{mis1}{1-D missing data}
\moddex{mis1}{1-D missing data}{54}{76}{user/gee}
\noindent

%Here \texttt{known} is declared in \texttt{solver\_mod} to be a logical vector.
%The call argument \texttt{ known=(xx/=0.)} sets the mask vector components
%to \texttt{.true.} where components of $\bold x$ are nonzero and sets it
%to \texttt{.false.} where the components are zero.

\par
I sought reference material on conjugate gradients with constraints
and didn't find anything,
leaving me to fear that this chapter was in error
and that I had lost the magic property of convergence
in a finite number of iterations.
I tested the code and it did converge in a finite number of iterations.
The explanation is that these constraints are almost trivial.
We pretended we had extra variables,
and computed a $\Delta \bold m =\bold g$ for each of them.
Then we set the $\Delta \bold m =\bold g$ to zero,
hence making no changes to anything,
like as if we had never calculated the extra  $\Delta \bold m$'s.

\begin{exer}
\item
        Figures~\ref{fig:mlines}--\ref{fig:moscil}
        seem to extrapolate to vanishing signals at the side boundaries.
        Why is that so, and what could be done to leave the sides
        unconstrained in that way?
\item
        Show that the interpolation curve in Figure~\ref{fig:mparab} is not
        parabolic as it appears, but cubic.
        ({\sc hint}:  First show that $(\nabla^2)\T\nabla^2 u = \bold 0$.)
        \item
        Verify by a program example that the number of iterations
        required with simple constraints is the number of free parameters.
        \todo{ idoc to reference printout}
\item
        A signal on a uniform mesh has missing values.
        How should we estimate the mean?
\end{exer}

\section{WELLS NOT MATCHING THE SEISMIC MAP}
\inputdir{chevron}
Accurate knowledge comes from a \bx{well},
but wells are expensive and far apart.
Less accurate knowledge comes from surface seismology,
but this knowledge is available densely in space
and can indicate significant \bx{trend}s between the wells.
For example,
a prospective area may contain 15 wells
but 600 or more seismic stations.
To choose future well locations,
it is helpful to match the known well data with the seismic data.
Although the seismic data is delightfully dense in space,
it often mismatches the wells
because there are systematic differences in the nature of the measurements.
These discrepancies
are sometimes attributed to velocity \bx{anisotropy}.
To work with such measurements,
we do not need to track down the physical model,
we need only to merge the information somehow
so we can appropriately \bx{map} the trends between wells
and make a proposal for the next drill site.
Here we consider only a scalar value at each location.
Take $\bold w$ to be a vector of 15 components,
each component being the seismic travel time to some fixed depth in a well.
Likewise let $\bold s$ be a 600-component vector
each with the seismic travel time to that fixed depth
as estimated wholly from surface seismology.
Such empirical corrections are often called ``\bx{fudge factor}s''.
An example is the Chevron oil field in Figure \ref{fig:wellseis}.
%\activesideplot{wellseis90}{width=3in,height=2in}{ER}{
%\activeplot{wellseis90}{width=6in,height=4in}{ER}{
\plot{wellseis}{width=4.5in,height=3in}{
  Binning by data push.
  Left is seismic data.
  Right is well locations.
  Values in bins are divided by numbers in bins.
  (Toldi)
}
The binning of the seismic data in Figure \ref{fig:wellseis}
is not really satisfactory when we have available
the techniques of missing data estimation
to fill the empty bins.
Using the ideas of subroutine \texttt{mis1()} \vpageref{lst:mis1},
we can extend the seismic data into the empty part of the plane.
We use the same principle that we minimize the energy in 
the filtered map where the map must match the data where it is known.
I chose the filter $\bold A = \nabla'\nabla=-\nabla^2$
to be the Laplacian operator (actually, its negative)
to obtain the result in Figure \ref{fig:misseis}.
%\activesideplot{misseis90}{width=3in,height=2in}{ER}{
%\activeplot{misseis90}{width=6in,height=4in}{ER}{
\plot{misseis}{width=4.5in,height=3in}{
  Seismic binned (left) and extended (right)
  by minimizing energy in $\nabla^2 \bold s$.
}

%\begin{notforlecture}
\par
Figure \ref{fig:misseis} also involves a \bx{boundary condition calculation}.
Many differential equations have a solution that becomes infinite
at infinite distance, and in practice this means that the largest
solutions may often be found on the boundaries of the plot,
exactly where there is the least information.
To obtain a more pleasing result,
I placed artificial ``average'' data
along the outer boundary.
Each boundary point was given the value of an average
of the interior data values.
The average was weighted,
each weight being an inverse power of the separation distance
of the boundary point from the interior point.

\par
Parenthetically, we notice that all the unknown interior points
could be guessed by the same method we used on the outer boundary.
After some experience guessing what inverse power would
be best for the weighting functions, I do not recommend this method.
Like gravity, the forces of interpolation from the weighted sums
are not blocked by intervening objects.
But the temperature in a house is not a function of temperature
in its neighbor's house.
To further isolate the more remote points,
I chose weights to be the inverse fourth power of distance.

\par
The first job is to fill the gaps in the seismic data.
We just finished doing a job like this in one dimension.
I'll give you more computational details later.
Let us call the extended seismic data  $\bold s$.

\par
Think of a map of a model space $\bold m$
of infinitely many hypothetical wells that must match the real wells,
where we have real wells.
We must find a map that matches the wells exactly
and somehow matches the seismic information elsewhere.
Let us define the vector $\bold w$ as shown in Figure \ref{fig:wellseis}
so $\bold w$ is
observed values at wells and zeros elsewhere.

\par
Where the seismic data contains sharp bumps or streaks,
we want our final earth model to have those features.
The wells cannot provide the rough features because the wells
are too far apart to provide high spatial frequencies.
The well information generally conflicts with the seismic data 
at low spatial frequencies because of systematic discrepancies between
the two types of measurements.
Thus we must accept that $\bold m$ and $\bold s$ may differ
at low spatial frequencies (where gradient and Laplacian are small).

\par
Our final map $\bold m$ would be very unconvincing if
it simply jumped from a well value at one point
to a seismic value at a neighboring point.
The map would contain discontinuities around each well.
Our philosophy of finding an earth model $\bold m$
is that our earth map should contain no obvious 
``footprint'' of the data acquisition (well locations).
We adopt the philosophy that the difference
between the final map (extended wells)
and the seismic information $\bold x=\bold m-\bold s$ should be smooth.
Thus,
we seek the minimum residual $\bold r$
which is the roughened difference between the seismic data $\bold s$
and the map $\bold m$ of hypothetical omnipresent wells.
With roughening operator $\bold A$ we fit
\begin{equation}
\bold 0\quad\approx\quad \bold r \eq \bold A ( \bold m - \bold s )
	\eq \bold A \bold x
\label{eqn:unconmap}
\end{equation}
along with the constraint
that the map should match the wells at the wells.
We could write this as
$\bold 0 = (\bold I-\bold J) ( \bold m - \bold w )$.
We honor this constraint by initializing the map $\bold m = \bold w$
to the wells (where we have wells, and zero elsewhere).
After we find the gradient direction to suggest some changes
to $\bold m$, we simply will not allow those changes at well locations.
We do this with a mask.
We apply a "missing data selector" to the gradient.
It zeros out possible changes at well locations.
Like with the goal (\ref{eqn:misfitgoal2}),
we have
\begin{equation}
\bold 0\quad\approx\quad \bold r \eq
\bold A \bold J \bold x + \bold A \bold x_{\rm known}
\end{equation}
After minimizing $\bold r$ by adjusting $\bold x$,
we have our solution $ \bold m =  \bold x + \bold s $.

\par
Now we prepare some roughening operators $\bold A$.
We have already coded a 2-D gradient operator
\texttt{igrad2} \vpageref{lst:igrad2}.
Let us combine it with its adjoint to get the 2-D laplacian operator.
(You might notice that the laplacian operator is ``self-adjoint'' meaning
that the operator does the same calculation that its adjoint does.
Any operator of the form $\bold A\T\bold A$ is self-adjoint because
$(\bold A\T\bold A)\T=\bold A\T (\bold A\T)\T=\bold A\T\bold A$. )
\par
\opdex{laplac2}{Laplacian in 2-D}{34}{78}{system/generic} 
Subroutine \texttt{lapfill()}
\vpageref{lst:lapfill} is the same idea as \texttt{mis1()}
\vpageref{lst:mis1} except that 
the filter $\bold A$ has been specialized to the 
laplacian
implemented by module \texttt{laplac2} \vpageref{lst:laplac2}. 


\moddex{lapfill}{Find 2-D missing data}{64}{79}{system/generic}

\par
Subroutine \texttt{lapfill()}
can be used for each of our two applications,
(1) extending the seismic data to fill space, and
(2) fitting the map exactly to the wells and approximately to the seismic data.
When extending the seismic data,
the initially non-zero components $\bold s \ne \bold 0$ are fixed
and cannot be changed.
\begin{comment}
That is done by calling
\texttt{lapfill()} with \texttt{mfixed=(s/=0.)}.
When extending wells,
the initially non-zero components $\bold w \ne \bold 0$ are fixed
and cannot be changed.
That is done by calling
\texttt{lapfill()} with \texttt{mfixed=(w/=0.)}.
\end{comment}

\par
The final map is shown in Figure \ref{fig:finalmap}.
\plot{finalmap}{width=4.5in,height=3in}{
  Final map based on Laplacian roughening.
}

\par
Results can be computed with various filters.
I tried both $\nabla^2$ and $\nabla$.
There are disadvantages of each,
$\nabla$ being too cautious and
$\nabla^2$ perhaps being too aggressive.
Figure \ref{fig:diffdiff} shows the difference $\bold x$ between
the extended seismic data and the extended wells.
Notice that for $\nabla$ the difference shows
a localized ``tent pole'' disturbance about each well.
For $\nabla^2$ there could be large overshoot between wells,
especially if two nearby wells have significantly different values.
I don't see that problem here.
\par
My overall opinion is that the Laplacian does the better job in this case.
I have that opinion because in viewing the extended gradient
I can clearly see where the wells are.
The wells are where we have acquired data.
We'd like our map of the world to not show where we acquired data.
Perhaps our estimated map of the world cannot help but show where
we have and have not acquired data, but we'd like to minimize that aspect.
\par
\boxit{ A good image of the earth hides our data \bx{acquisition footprint}.  }


\plot{diffdiff}{width=5in,height=2.25in}{
  Difference between wells (the final map)
  and the extended seismic data.
  Left is plotted at the wells (with gray background for zero).
  Center is based on gradient roughening and shows
  tent-pole-like residuals at wells.
  Right is based on Laplacian roughening.
}

\par
To understand the behavior theoretically,
recall that in one dimension 
the filter $\nabla$ interpolates with straight lines
and $\nabla^2$ interpolates with cubics.
This is because the fitting goal
$\bold 0 \approx \nabla \bold m$,
leads to
$\frac{\partial }{\partial \bold m\T} \bold m\T\nabla'\nabla \bold m = \bold 0$
or $\nabla\T\nabla \bold m = \bold 0$, whereas the fitting goal 
        $\bold 0 \approx \nabla^2 \bold m$
leads to
        $\nabla^4 \bold m = \bold 0$
which is satisfied by cubics.
In two dimensions, minimizing the output of $\nabla$
gives us solutions of Laplace's equation with sources at the known data.
It is as if $\nabla$ stretches a rubber sheet over poles at each well,
whereas $\nabla^2$ bends a stiff plate.
\par
Just because $\nabla^2$ gives smoother maps than  $\nabla$
does not mean those maps are closer to reality.
This is a deeper topic, addressed in Chapter~\ref{paper:mda}.
It is the same issue we noticed when comparing
figures \ref{fig:mlines}-\ref{fig:mbest}.

\section{SEARCHING THE SEA OF GALILEE}
\sx{Sea of Galilee}
\sx{Galilee, Sea of}
\inputdir{galilee}
Figure \ref{fig:locfil} shows a bottom-sounding survey
of the Sea of Galilee\footnote{
        Data collected by Zvi \bx{ben Avraham}, TelAviv University.
        Please communicate with him {\tt zvi@jupiter1.tau.ac.il}
        for more details or if you make something
        publishable with his data.
        }
at various stages of processing.
The ultimate goal is not only a good map of
the depth to bottom,
but images useful for the purpose
of identifying \bx{archaeological}, geological, or
geophysical details of the sea bottom.
The Sea of Galilee is unique
because it is a {\it fresh}-water lake {\it below} sea-level.
It seems to be connected to the great rift (pull-apart)
valley crossing east Africa.
We might delineate the Jordan River delta.
We might find springs on the water bottom.
We might find archaeological objects.
\par
\plot{locfil}{width=6in,height=8.5in}{
  Views of the bottom of the Sea of Galilee.
}
The raw data is 132,044 triples, $(x_i,y_i,z_i)$,
where $x_i$ ranges over about 12 km and
where $y_i$ ranges over about 20 km.
The lines you see in Figure \ref{fig:locfil}
are sequences of data points, i.e., the track of the survey vessel.
The depths $z_i$ are recorded to an accuracy of about 10 cm.
\par
The first frame in Figure~\ref{fig:locfil} shows simple binning.
A coarser mesh would avoid the empty bins but lose resolution.
As we refine the mesh for more detail,
the number of empty bins grows
as does the care needed in devising a technique
for filling them.
This first frame uses the simple idea from Chapter \ref{paper:ajt} of
spraying all the data values to the nearest bin
with \texttt{bin2()} \vpageref{lst:bin2}
and dividing by the number in the bin.
Bins with no data obviously need to be filled in some other way.
I used a missing data program like that in the recent section
on ``wells not matching the seismic map.''
Instead of roughening with a Laplacian, however,
I used the gradient operator \texttt{igrad2} \vpageref{lst:igrad2}
% I used the \bx{roughening operator} \texttt{rufftri2()} \vpageref{lst:rufftri2},
% which is an impulse minus a broad smoothing operator.
The solver is \texttt{grad2fill()}.
\moddex{igrad2}{finite-difference gradient}{48}{60}{api/c}
\moddex{grad2fill}{low cut missing data}{55}{63}{api/c}

%\par
%Having a roughener with an adjustable width in the smoothing operator
%allows us to choose a cutoff frequency.
%This is interesting for two reasons:
%(1) The output of the roughening operator
%is interesting to look at; and
%(2) Larger filters allow missing data iterations to converge more quickly.

\par
The output of the roughening operator is an image,
a filtered version of the depth,
a filtered version of something real.
Such filtering can enhance the appearance of interesting features.
For example,
scanning the shoreline of the roughened image
(after missing data was filled),
we see several ancient shorelines, now submerged.
\par
\boxit{The adjoint is the easiest image to build.
The roughened map is often more informative than the map itself.}
\par
The views expose several defects
of the data acquisition and of our data processing.
The impulsive glitches (St.~Peter's fish?)
need to be removed but we must be careful not to throw
out the sunken ships along with the bad data points.
Even our best image shows clear evidence of the recording vessel's tracks.
Strangely, some tracks are deeper than others.
Perhaps the survey is assembled from work done in different seasons
and the water level varied by season.
Perhaps some days the vessel was more heavily loaded
and the depth sounder was on a deeper keel.
As for the navigation equipment,
we can see that some data values are reported outside the lake!
\par
%\boxit{ A good image of the earth hides our data \bx{acquisition footprint}.  }
\par
We want the sharpest possible view
of this classical site.
A treasure hunt is never easy
and no one guarantees we will
find anything of great value
but at least the exercise is a good warm-up
for submarine petroleum exploration.


\section{INVERSE LINEAR INTERPOLATION}
\inputdir{invint}
In Chapter \ref{paper:ajt} we defined \bx{linear interpolation}
\sx{interpolation}
as the extraction of values from between mesh points.
In a typical setup (occasionally the role of data and model are swapped),
a model is given on a uniform mesh
and we solve the easy problem of extracting values
between the mesh points with subroutine \texttt{lint1()} % \vpageref{lst:lint1}.
The genuine problem is the inverse problem, which we attack here.
Data values are sprinkled all around,
and we wish to find a function on a uniform mesh
from which we can extract that data by \bx{linear interpolation}.
The adjoint operator for subroutine {\tt lint1()}
simply piles data back into its proper location in model space
without regard to how many data values land in each region.
Thus some model values may have many data points added
to them while other model values get none.
We could interpolate by minimizing the energy in the model gradient,
or that in the second derivative of the model,
or that in the output of any other roughening filter
applied to the model.
\par
Formalizing now our wish
that data $\bold d$ be extractable by \bx{linear interpolation} $\bold F$,
from a model $\bold m$,
and our wish that application of a roughening filter
with an operator $\bold A$ have minimum energy, we write the fitting goals:
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F \bold m - \bold d \\
        \bold 0 &\approx & \bold A \bold m
        \label{eqn:invintwish}
        \end{array}
\end{equation}
Suppose we take the roughening filter to be the second difference operator
$(1,-2,1)$
scaled by a constant $\epsilon$,
and suppose we have a data point near each end of the model
and a third data point exactly in the middle.
Then,
for a model space 6 points long,
the fitting goal could look like
\def\E{\epsilon}
\begin{equation} { 
\left[ 
\begin{array}{rrrrrr}
   .8 & .2 & .  & .  & .  & .  \\
   .  & .  & 1  & .  & .  & .  \\
   .  & .  & .  & .  & .5 & .5 \\
   \hline
   \E & .  & .  & .  & .  & .  \\
  -2\E& \E & .  & .  & .  & .  \\
   \E &-2\E&  \E& .  & .  & .  \\
   .  & \E &-2\E&  \E& .  & .  \\
   .  & .  &  \E&-2\E&  \E& .  \\
   .  & .  & .  &  \E&-2\E&  \E\\
   .  & .  & .  & .  &  \E&-2\E\\
   .  & .  & .  & .  & .  &  \E
  \end{array} \right] 
\left[ 
        \begin{array}{c}
          m_0 \\ 
          m_1 \\ 
          m_2 \\ 
          m_3 \\ 
          m_4 \\ 
          m_5 
        \end{array}
\right] 
\ -\ 
\left[ 
\begin{array}{c}
  d_0 \\ 
  d_1 \\ 
  d_2 \\ 
  \hline
  0   \\
  0   \\
  0   \\
  0   \\
  0   \\
  0   \\
  0   \\
  0
  \end{array} \right] 
\eq
\left[ 
\begin{array}{c}
  \bold r_d \\ 
  \bold r_m 
  \end{array} \right] 
\quad \approx \ \bold 0
\label{eqn:tworegexam}
} \end{equation}
\par
The residual vector has two parts,
a data part $\bold r_d$ on top
and a model part $\bold r_m$ on the bottom.
The data residual
should vanish except where contradictory data values
happen to lie in the same place.
The model residual is the roughened model.

\begin{comment}
\par
Two fitting goals (\ref{eqn:invintwish}) are so common in practice
that it is convenient to adopt our least-square fitting
subroutine \texttt{solver-smp} \vpageref{lst:solver-smp} accordingly.
The modification
is shown in module \texttt{solver-reg} \vpageref{lst:solver-reg}.
In addition to specifying the ``data fitting'' operator $\bold F$
(parameter \texttt{Fop}),
we need to pass the ``model regularization'' operator $\bold A$
(parameter \texttt{Aop}) and the
size of its output (parameter \texttt{nAop}) for proper memory allocation.

%\begin{notforlecture}
\par
(When I first looked at module \texttt{solver-reg} I was appalled
by the many lines of code, especially all the declarations.
Then I realized how much much worse was Fortran 77 where
I needed to write a new solver for every pair of operators.
This one solver module works for all operator pairs
and for many optimization descent strategies
because these ``objects'' are arguments.
These more powerful objects require declarations that are more complicated
than the simple objects of Fortran 77.
As an author I have a dilemma:
To make algorithms compact (and seem simple) requires many careful definitions.
When these definitions put in the code, they are careful,
but the code becomes annoyingly verbose.
Otherwise, the definitions must go in the surrounding natural language
where they are not easily made precise.)


\moddex{solver-reg}{generic solver with regularization}
\end{comment}

\par
% The subroutine for this job does not initialize the original model.
% The caller might want to initialize the model with $\bold m=\bold 0$
% or with some other model $\bold m_0$.
After all the definitions,
we load the negative of the data into the residual.
If a starting model $\bold m_0$ is present,
then we update the data part of the residual
$\bold r_d=\bold F \bold m_0 - \bold d$
and we load
the model part of the residual
$ \bold r_m = \bold A \bold m_0$.
Otherwise we begin from a zero model $\bold m_0=\bold 0$ and thus
the model part of the residual $ \bold r_m$ is also zero.
After this initialization, subroutine 
\texttt{solver\_reg()} % \vpageref{lst:solver_reg}
begins an iteration loop by first computing
the proposed model perturbation $\Delta \bold m$
(called \texttt{g} in the program)
with the adjoint operator:
\begin{equation}
 \Delta \bold m
 \quad\longleftarrow\quad
 \left[
 \begin{array}{cc}
   \bold F\T &   \bold A\T
 \end{array}
 \right]
 \
 \left[
 \begin{array}{c}
   \bold r_d \\
   \bold r_m
 \end{array}
 \right]
\end{equation}
\begin{comment}
I chose to implement the model roughening operator $\bold A$
with the convolution subroutine \texttt{tcai1()} % \vpageref{lst:tcai1},
which has transient end effects
(and an output length equal to the input length plus the filter length).
The adjoint of subroutine {\tt tcai1()} suggests perturbations
in the convolution input (not the filter).
\end{comment}
Using this value of $\Delta \bold m$,
% {\tt invint1()} 
we can
find the implied change in residual $\Delta \bold r$ as
\begin{equation}
        \Delta
        \left[
        \begin{array}{c}
        \bold r_d \\
        \bold r_m
        \end{array}
        \right]
\quad\longleftarrow\quad
        \left[
        \begin{array}{c}
        \bold F \\
        \bold A
        \end{array}
        \right]
        \
        \Delta \bold m
\end{equation}
and the last thing in the loop is to use
the optimization step function \texttt{stepper()} 
% conjugate-direction subroutine \texttt{cgplus()} \vpageref{lst:cgplus}
to choose the length of the step size
and to choose how much of the previous step to include.
\par
An example of using the new solver is subroutine \texttt{invint1}.
%\vpageref{lst:invint1}.
I chose to implement the model roughening operator $\bold A$
with the convolution subroutine \texttt{tcai1()} \vpageref{lst:tcai1},
which has transient end effects
(and an output length equal to the input length plus the filter length).
The adjoint of subroutine {\tt tcai1()} suggests perturbations
in the convolution input (not the filter).
\moddex{invint1}{invers linear interp.}{24}{40}{user/gee}
%\end{notforlecture}
\par
Figure \ref{fig:im1-2+1} shows an example for a $(1,-2,1)$ filter with $\epsilon = 1$.
The continuous curve representing the model $\bold m$
passes through the data points.
Because the models are computed with transient convolution end-effects,
the models tend to damp linearly to zero outside the region where
signal samples are given.
\sideplot{im1-2+1}{width=3in,height=1.5in}{
  Sample points and estimation of a continuous function through them.
}
\par
To show an example where the result is clearly a theoretical answer,
I prepared another figure with the simpler filter $(1,-1)$.
When we minimize energy in the first derivative of the waveform,
the residual distributes itself uniformly
between data points
so the solution there is a straight line.
Theoretically it should be a straight line
because a straight line has a vanishing second derivative,
and that condition arises by differentiating by $\bold x\T$,
the minimized quadratic form
$\bold x\T \bold A\T\bold A \bold x$, and getting
$         \bold A\T\bold A \bold x=\bold 0$.
(By this logic, the curves between data points in Figure \ref{fig:im1-2+1}
must be cubics.)
The $(1,-1)$ result is shown in Figure \ref{fig:im1-1a}.
\sideplot{im1-1a}{width=3in,height=1.5in}{
  The same data samples
  and a function through them that minimizes
  the energy in the first derivative.
}
\par
The example of
Figure \ref{fig:im1-1a}
has been a useful test case for me.
You'll see it again in later chapters.
What I would like to show you here is a movie showing the convergence
to Figure \ref{fig:im1-1a}.
Convergence occurs rapidly where data points are close together.
The large gaps, however, fill at a rate of one point per iteration.

\subsection{Abandoned theory for matching wells and seismograms}
\par
Let us consider theory to
construct a map $\bold m$ that fits dense seismic data
$\bold s$ and the well data $\bold w$.
The first goal
$\bold 0 \approx  \bold L \bold m - \bold w$
says that when we linearly interpolate from the map,
we should get the well data.
The second goal
$\bold 0 \approx \bold A (\bold m - \bold s)$
(where $\bold A$ is a roughening operator like $\nabla$ or $\nabla^2$)
says that the map $\bold m$ should match the seismic data $\bold s$
at high frequencies but need not do so at low frequencies.
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold L \bold m - \bold w \\
        \bold 0 &\approx & \bold A (\bold m - \bold s)
        \label{eqn:toldisregression}
        \end{array}
\end{equation}
\par
Although (\ref{eqn:toldisregression}) is the way I originally formulated
the well-fitting application, I abandoned it for several reasons:
First, the map had ample pixel resolution compared to other sources of error,
so I switched from linear interpolation to binning.
Once I was using binning,
I had available the simpler empty-bin approaches.
These have the further advantage that it is not necessary
to experiment with the relative weighting between
the two goals in (\ref{eqn:toldisregression}).
A formulation like (\ref{eqn:toldisregression}) is more likely
to be helpful where we need to handle rapidly changing functions
where binning is inferior to linear interpolation,
perhaps in reflection seismology where high resolution is meaningful.
\begin{exer}
\item
        It is desired to find a compromise between
        the Laplacian roughener
        and the gradient roughener.
        What is the size of the residual space?
        \sx{roughener ! Laplacian}
        \sx{roughener ! gradient}
\item
        Like the seismic prospecting industry,
        you have solved a huge problem using binning.
        You have computer power left over
        to do a few iterations with linear interpolation.
        How much does the cost per iteration increase?
        Should you refine your model mesh,
        or can you use the same model mesh
        that you used when binning?
\end{exer}
%\end{notforlecture}


\section{PREJUDICE, BULLHEADEDNESS, AND CROSS VALIDATION}

First we first look at data $\bold d$.
Then we think about a model $\bold m$,
and an operator $\bold L$ to link the model and the data.
Sometimes the operator is merely the first term in a series expansion
about $(\bold m_0,\bold d_0)$.
Then we fit
$\bold d-\bold d_0 \approx \bold L ( \bold m-\bold m_0)$.
To fit the model, we must reduce the fitting residuals.
Realizing that the importance of a data residual
is not always simply the size of the residual
but is generally a function of it,
we conjure up (topic for later chapters)
a weighting function (which could be a filter) operator $\bold W$.
This defines our data residual:
\begin{equation}
\bold r_d \eq \bold W
[ \bold L
        ( \bold m-\bold m_0)
\ -\ 
        ( \bold d-\bold d_0)
]
\end{equation}

\par
Next we realize that the data might not be adequate to determine the model,
perhaps because our comfortable dense sampling of the model
ill fits our economical sparse sampling of data.
Thus we adopt a fitting goal that mathematicians call ``regularization''
and we might call a ``model style'' goal
or more simply,
a quantification of our prejudice about models.
We express this by choosing an operator $\bold A$,
often simply a roughener like a gradient
(the choice again a topic in this and later chapters).
It defines our model residual by
$\bold A \bold m$ or
$\bold A ( \bold m-\bold m_0)$, say we choose
\begin{equation}
\bold r_m \eq \bold A \bold m 
\end{equation}

\par
In an ideal world, our model prejudice would not conflict
with measured data, however,
life is not so simple.
Since conflicts between data and preconceived notions invariably arise
(and they are why we go to the expense of acquiring data)
we need an adjustable parameter
that measures our ``bullheadedness'', how much we intend
to stick to our preconceived notions in spite of contradicting data.
This parameter is generally called epsilon $\epsilon$
because we like to imagine that our bullheadedness is small.
(In mathematics, $\epsilon$ is often taken to be
an infinitesimally small quantity.)
Although any bullheadedness seems like a bad thing,
it must be admitted that measurements are imperfect too.
Thus as a practical matter we often find ourselves minimizing
\begin{equation}
\min \quad := \quad
\bold r_d \cdot \bold r_d \ +\  \epsilon^2\ \bold r_m \cdot \bold r_m 
\end{equation}
and wondering what to choose for $\epsilon$.
I have two suggestions:
My simplest suggestion is to choose $\epsilon$
so that the residual of data fitting matches that of model styling.
Thus
\begin{equation}
\epsilon \eq \sqrt{\frac{ \bold r_d \cdot \bold r_d }{ \bold r_m \cdot \bold r_m }}
\end{equation}
My second suggestion is to think of the force on our final solution.
In physics, force is associated with a gradient.
We have a gradient for the data fitting
and another for the model styling:
\begin{eqnarray}
\bold g_d &=& \bold L\T \bold W\T \bold r_d  \\
\bold g_m &=& \bold A\T \bold r_m
\end{eqnarray}
We could balance these forces by the choice
\begin{equation}
\epsilon \eq \sqrt{
\bold g_d \cdot \bold g_d
\over
\bold g_m \cdot \bold g_m 
}
\end{equation}

Although we often ignore $\epsilon$ in discussing the formulation
of an application, when time comes to solve the problem, reality intercedes.
Generally, $\bold r_d$ has different physical units than $\bold r_m$
(likewise $\bold g_d$ and $\bold g_m$)
and we cannot allow our solution
to depend on the accidental choice of units
in which we express the problem.
I have had much experience choosing $\epsilon$, but it is
only recently that I boiled it down to the above two suggestions.
Normally I also try other values, like double or half those
of the above choices,
and I examine the solutions for subjective appearance.
If you find any insightful examples, please tell me about them.

\par
Computationally, we could choose a new $\epsilon$ with each iteration,
but it is more expeditious
to freeze $\epsilon$, solve the problem,
recompute $\epsilon$, and solve the problem again.
I have never seen a case where more than one iteration was necessary.

\par
People who work with small applications
(less than about $10^3$ vector components)
have access to an attractive theoretical approach
called cross-validation.
Simply speaking,
we could solve the problem many times,
each time omitting a different data value.
Each solution would provide a model
that could be used to predict
the omitted data value.
The quality of these predictions
is a function of $\epsilon$
and this provides a guide to finding it.
My objections to cross validation are two-fold:
First, I don't know how to apply it in the large applications
like we solve in this book
(I should think more about it);
and second,
people who worry much about $\epsilon$,
perhaps first should think 
more carefully about
their choice of the filters $\bold W$ and $\bold A$,
which is the focus of this book.
Notice that both $\bold W$ and $\bold A$
can be defined with a scaling factor which is like scaling $\epsilon$.
Often more important in practice,
with $\bold W$ and $\bold A$
we have a scaling factor that need not be constant but
can be a function of space or spatial frequency
within the data space and/or model space.


\begin{comment}

\section{References}

\reference{Gill, P.E., Murray, W., and Wright, M.H., 1981,
        Practical optimization:  Academic Press.
        }
\reference{Hestenes, M.R., and Stiefel, E., 1952,
        Methods of
        conjugate gradients for solving linear systems:
        J. Res. Natl. Bur. Stand., {\bf 49}, 409-436.
        }
\reference{Luenberger, D.G., 1973,
        Introduction to linear and nonlinear programming:
        Addison-Wesley.
        }
\reference{Nolet, G., 1985,
        Solving or resolving inadequate and noisy
        tomographic systems:
        J. Comp. Phys., {\bf 61}, 463-482.
        }
\reference{Paige, C.C., and Saunders, M.A., 1982a,
        LSQR: an algorithm for sparse linear equations
        and sparse least squares:
        Assn. Comp. Mach. Trans. Mathematical Software,
        {\bf 8,} 43-71.
        }
\reference{Paige, C.C., and Saunders, M.A., 1982b,
        Algorithm 583, LSQR:
        sparse linear equations and least squares problems:
        Assn. Comp. Mach. Trans. Mathematical Software,
        {\bf 8,}  195-209.
        }
%\reference{Press, W.H. et al., 1989,
%        Numerical recipes: the art of scientific computing:
%        Cambridge University Press.
%        }
\reference{Strang, G., 1986,
        Introduction to applied mathematics:
        Wellesley-Cambridge Press.
        }
\end{comment}

\clearpage

